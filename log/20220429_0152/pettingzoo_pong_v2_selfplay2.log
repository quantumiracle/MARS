pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [544, 480, 809, 666, 142]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 100000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 10, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220429_0152/pettingzoo_pong_v2_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220429_0152/pettingzoo_pong_v2_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 4.2237s / 4.2237 s
env0_first_0:                 episode reward: 8.0000,                 loss: 0.1524
env0_second_0:                 episode reward: -8.0000,                 loss: nan
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 64.7403s / 68.9639 s
env0_first_0:                 episode reward: 8.0000,                 loss: 0.0746
env0_second_0:                 episode reward: -8.0000,                 loss: nan
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Score delta: 16.0, update the opponent.
Episode: 41/10000 (0.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 76.9271s / 145.8910 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0603
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
env2_first_0:                 episode reward: 2.8000,                 loss: nan
env2_second_0:                 episode reward: -2.8000,                 loss: nan
env3_first_0:                 episode reward: 1.4500,                 loss: nan
env3_second_0:                 episode reward: -1.4500,                 loss: nan
env4_first_0:                 episode reward: 2.5500,                 loss: nan
env4_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 81.3128s / 227.2039 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0433
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
env2_first_0:                 episode reward: 1.9500,                 loss: nan
env2_second_0:                 episode reward: -1.9500,                 loss: nan
env3_first_0:                 episode reward: 3.5000,                 loss: nan
env3_second_0:                 episode reward: -3.5000,                 loss: nan
env4_first_0:                 episode reward: 4.3500,                 loss: nan
env4_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 85.6806s / 312.8845 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0207
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
env2_first_0:                 episode reward: 3.7500,                 loss: nan
env2_second_0:                 episode reward: -3.7500,                 loss: nan
env3_first_0:                 episode reward: 4.1000,                 loss: nan
env3_second_0:                 episode reward: -4.1000,                 loss: nan
env4_first_0:                 episode reward: 5.1500,                 loss: nan
env4_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 98.3963s / 411.2808 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0275
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
env2_first_0:                 episode reward: 1.3500,                 loss: nan
env2_second_0:                 episode reward: -1.3500,                 loss: nan
env3_first_0:                 episode reward: 3.5000,                 loss: nan
env3_second_0:                 episode reward: -3.5000,                 loss: nan
env4_first_0:                 episode reward: 3.9000,                 loss: nan
env4_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 114.0416s / 525.3224 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0175
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
env2_first_0:                 episode reward: 1.9500,                 loss: nan
env2_second_0:                 episode reward: -1.9500,                 loss: nan
env3_first_0:                 episode reward: 3.1000,                 loss: nan
env3_second_0:                 episode reward: -3.1000,                 loss: nan
env4_first_0:                 episode reward: 3.0500,                 loss: nan
env4_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 119.2407s / 644.5631 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0144
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
env2_first_0:                 episode reward: 3.9000,                 loss: nan
env2_second_0:                 episode reward: -3.9000,                 loss: nan
env3_first_0:                 episode reward: 4.7500,                 loss: nan
env3_second_0:                 episode reward: -4.7500,                 loss: nan
env4_first_0:                 episode reward: 1.8000,                 loss: nan
env4_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 124.7561s / 769.3192 s
env0_first_0:                 episode reward: 1.9500,                 loss: nan
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0121
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
env2_first_0:                 episode reward: 3.1000,                 loss: nan
env2_second_0:                 episode reward: -3.1000,                 loss: nan
env3_first_0:                 episode reward: 2.8500,                 loss: nan
env3_second_0:                 episode reward: -2.8500,                 loss: nan
env4_first_0:                 episode reward: 1.8000,                 loss: nan
env4_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 131.5337s / 900.8529 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0117
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
env2_first_0:                 episode reward: 3.3000,                 loss: nan
env2_second_0:                 episode reward: -3.3000,                 loss: nan
env3_first_0:                 episode reward: 2.9000,                 loss: nan
env3_second_0:                 episode reward: -2.9000,                 loss: nan
env4_first_0:                 episode reward: 2.1500,                 loss: nan
env4_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 137.0218s / 1037.8748 s
env0_first_0:                 episode reward: 1.2500,                 loss: nan
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0130
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
env2_first_0:                 episode reward: 3.0000,                 loss: nan
env2_second_0:                 episode reward: -3.0000,                 loss: nan
env3_first_0:                 episode reward: 1.6000,                 loss: nan
env3_second_0:                 episode reward: -1.6000,                 loss: nan
env4_first_0:                 episode reward: 0.3000,                 loss: nan
env4_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 142.7616s / 1180.6363 s
env0_first_0:                 episode reward: 1.9500,                 loss: nan
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0146
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
env2_first_0:                 episode reward: 2.8000,                 loss: nan
env2_second_0:                 episode reward: -2.8000,                 loss: nan
env3_first_0:                 episode reward: 3.2000,                 loss: nan
env3_second_0:                 episode reward: -3.2000,                 loss: nan
env4_first_0:                 episode reward: 2.3000,                 loss: nan
env4_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 147.5803s / 1328.2167 s
env0_first_0:                 episode reward: 4.4000,                 loss: nan
env0_second_0:                 episode reward: -4.4000,                 loss: 0.0149
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
env2_first_0:                 episode reward: 2.5500,                 loss: nan
env2_second_0:                 episode reward: -2.5500,                 loss: nan
env3_first_0:                 episode reward: 2.4500,                 loss: nan
env3_second_0:                 episode reward: -2.4500,                 loss: nan
env4_first_0:                 episode reward: 3.8000,                 loss: nan
env4_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 155.1683s / 1483.3850 s
env0_first_0:                 episode reward: 1.5000,                 loss: nan
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0105
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
env2_first_0:                 episode reward: -1.6500,                 loss: nan
env2_second_0:                 episode reward: 1.6500,                 loss: nan
env3_first_0:                 episode reward: 2.9500,                 loss: nan
env3_second_0:                 episode reward: -2.9500,                 loss: nan
env4_first_0:                 episode reward: 2.5500,                 loss: nan
env4_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.8832s / 1646.2682 s
env0_first_0:                 episode reward: 2.1500,                 loss: nan
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0092
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
env2_first_0:                 episode reward: 1.6500,                 loss: nan
env2_second_0:                 episode reward: -1.6500,                 loss: nan
env3_first_0:                 episode reward: 2.7000,                 loss: nan
env3_second_0:                 episode reward: -2.7000,                 loss: nan
env4_first_0:                 episode reward: 1.3500,                 loss: nan
env4_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 170.1898s / 1816.4580 s
env0_first_0:                 episode reward: 4.2000,                 loss: nan
env0_second_0:                 episode reward: -4.2000,                 loss: 0.0113
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
env2_first_0:                 episode reward: 5.8000,                 loss: nan
env2_second_0:                 episode reward: -5.8000,                 loss: nan
env3_first_0:                 episode reward: 2.8500,                 loss: nan
env3_second_0:                 episode reward: -2.8500,                 loss: nan
env4_first_0:                 episode reward: 3.1500,                 loss: nan
env4_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 178.5808s / 1995.0388 s
env0_first_0:                 episode reward: 2.3000,                 loss: nan
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0128
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
env2_first_0:                 episode reward: 5.1500,                 loss: nan
env2_second_0:                 episode reward: -5.1500,                 loss: nan
env3_first_0:                 episode reward: 2.5000,                 loss: nan
env3_second_0:                 episode reward: -2.5000,                 loss: nan
env4_first_0:                 episode reward: 1.5000,                 loss: nan
env4_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 186.7165s / 2181.7553 s
env0_first_0:                 episode reward: 3.6500,                 loss: nan
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0129
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
env2_first_0:                 episode reward: 4.6500,                 loss: nan
env2_second_0:                 episode reward: -4.6500,                 loss: nan
env3_first_0:                 episode reward: 5.4000,                 loss: nan
env3_second_0:                 episode reward: -5.4000,                 loss: nan
env4_first_0:                 episode reward: 4.8000,                 loss: nan
env4_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.2388s / 2373.9941 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0115
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
env2_first_0:                 episode reward: 3.9000,                 loss: nan
env2_second_0:                 episode reward: -3.9000,                 loss: nan
env3_first_0:                 episode reward: 1.6000,                 loss: nan
env3_second_0:                 episode reward: -1.6000,                 loss: nan
env4_first_0:                 episode reward: 0.6000,                 loss: nan
env4_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.5801s / 2567.5742 s
env0_first_0:                 episode reward: 0.4000,                 loss: nan
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0110
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
env2_first_0:                 episode reward: 2.7500,                 loss: nan
env2_second_0:                 episode reward: -2.7500,                 loss: nan
env3_first_0:                 episode reward: 4.7000,                 loss: nan
env3_second_0:                 episode reward: -4.7000,                 loss: nan
env4_first_0:                 episode reward: 1.2000,                 loss: nan
env4_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.5169s / 2759.0911 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0104
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
env2_first_0:                 episode reward: 0.4500,                 loss: nan
env2_second_0:                 episode reward: -0.4500,                 loss: nan
env3_first_0:                 episode reward: 3.7500,                 loss: nan
env3_second_0:                 episode reward: -3.7500,                 loss: nan
env4_first_0:                 episode reward: 2.2500,                 loss: nan
env4_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.3333s / 2950.4245 s
env0_first_0:                 episode reward: 1.8500,                 loss: nan
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0099
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
env2_first_0:                 episode reward: 1.1500,                 loss: nan
env2_second_0:                 episode reward: -1.1500,                 loss: nan
env3_first_0:                 episode reward: 2.9500,                 loss: nan
env3_second_0:                 episode reward: -2.9500,                 loss: nan
env4_first_0:                 episode reward: 1.8000,                 loss: nan
env4_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.5865s / 3142.0110 s
env0_first_0:                 episode reward: 4.3500,                 loss: nan
env0_second_0:                 episode reward: -4.3500,                 loss: 0.0100
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
env2_first_0:                 episode reward: 3.7500,                 loss: nan
env2_second_0:                 episode reward: -3.7500,                 loss: nan
env3_first_0:                 episode reward: 4.4500,                 loss: nan
env3_second_0:                 episode reward: -4.4500,                 loss: nan
env4_first_0:                 episode reward: 4.7500,                 loss: nan
env4_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.5093s / 3334.5203 s
env0_first_0:                 episode reward: 3.6500,                 loss: nan
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0102
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
env2_first_0:                 episode reward: 2.9500,                 loss: nan
env2_second_0:                 episode reward: -2.9500,                 loss: nan
env3_first_0:                 episode reward: 4.7500,                 loss: nan
env3_second_0:                 episode reward: -4.7500,                 loss: nan
env4_first_0:                 episode reward: 3.8500,                 loss: nan
env4_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.0611s / 3526.5814 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0100
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
env2_first_0:                 episode reward: 0.9500,                 loss: nan
env2_second_0:                 episode reward: -0.9500,                 loss: nan
env3_first_0:                 episode reward: 3.4000,                 loss: nan
env3_second_0:                 episode reward: -3.4000,                 loss: nan
env4_first_0:                 episode reward: 2.0500,                 loss: nan
env4_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.8592s / 3718.4406 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0095
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
env2_first_0:                 episode reward: 3.6500,                 loss: nan
env2_second_0:                 episode reward: -3.6500,                 loss: nan
env3_first_0:                 episode reward: 4.5000,                 loss: nan
env3_second_0:                 episode reward: -4.5000,                 loss: nan
env4_first_0:                 episode reward: 3.8000,                 loss: nan
env4_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.9226s / 3910.3632 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0091
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
env2_first_0:                 episode reward: 0.9500,                 loss: nan
env2_second_0:                 episode reward: -0.9500,                 loss: nan
env3_first_0:                 episode reward: 1.4000,                 loss: nan
env3_second_0:                 episode reward: -1.4000,                 loss: nan
env4_first_0:                 episode reward: 0.8000,                 loss: nan
env4_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.4498s / 4101.8130 s
env0_first_0:                 episode reward: -0.4000,                 loss: nan
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0091
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
env2_first_0:                 episode reward: -0.8000,                 loss: nan
env2_second_0:                 episode reward: 0.8000,                 loss: nan
env3_first_0:                 episode reward: 0.7000,                 loss: nan
env3_second_0:                 episode reward: -0.7000,                 loss: nan
env4_first_0:                 episode reward: 0.8500,                 loss: nan
env4_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.9684s / 4293.7814 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0090
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
env2_first_0:                 episode reward: -0.7500,                 loss: nan
env2_second_0:                 episode reward: 0.7500,                 loss: nan
env3_first_0:                 episode reward: -1.2500,                 loss: nan
env3_second_0:                 episode reward: 1.2500,                 loss: nan
env4_first_0:                 episode reward: -0.2500,                 loss: nan
env4_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.5024s / 4485.2838 s
env0_first_0:                 episode reward: -3.3000,                 loss: nan
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0089
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
env2_first_0:                 episode reward: -3.0000,                 loss: nan
env2_second_0:                 episode reward: 3.0000,                 loss: nan
env3_first_0:                 episode reward: -3.0000,                 loss: nan
env3_second_0:                 episode reward: 3.0000,                 loss: nan
env4_first_0:                 episode reward: -3.1500,                 loss: nan
env4_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 155.7856s / 4641.0694 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0579
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0084
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
env2_first_0:                 episode reward: -4.6000,                 loss: nan
env2_second_0:                 episode reward: 4.6000,                 loss: nan
env3_first_0:                 episode reward: -4.3000,                 loss: nan
env3_second_0:                 episode reward: 4.3000,                 loss: nan
env4_first_0:                 episode reward: -4.5000,                 loss: nan
env4_second_0:                 episode reward: 4.5000,                 loss: nan
Score delta: 12.2, update the opponent.
Episode: 621/10000 (6.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 93.0595s / 4734.1288 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0128
env0_second_0:                 episode reward: 3.2500,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
env2_first_0:                 episode reward: -2.6500,                 loss: nan
env2_second_0:                 episode reward: 2.6500,                 loss: nan
env3_first_0:                 episode reward: -2.0000,                 loss: nan
env3_second_0:                 episode reward: 2.0000,                 loss: nan
env4_first_0:                 episode reward: -3.1000,                 loss: nan
env4_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 98.3405s / 4832.4693 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0099
env0_second_0:                 episode reward: 5.2500,                 loss: nan
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
env2_first_0:                 episode reward: -4.4500,                 loss: nan
env2_second_0:                 episode reward: 4.4500,                 loss: nan
env3_first_0:                 episode reward: -6.0000,                 loss: nan
env3_second_0:                 episode reward: 6.0000,                 loss: nan
env4_first_0:                 episode reward: -5.0500,                 loss: nan
env4_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 106.2901s / 4938.7594 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0101
env0_second_0:                 episode reward: 5.0500,                 loss: nan
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
env2_first_0:                 episode reward: -6.3000,                 loss: nan
env2_second_0:                 episode reward: 6.3000,                 loss: nan
env3_first_0:                 episode reward: -5.6500,                 loss: nan
env3_second_0:                 episode reward: 5.6500,                 loss: nan
env4_first_0:                 episode reward: -6.8000,                 loss: nan
env4_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 110.1846s / 5048.9440 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0123
env0_second_0:                 episode reward: 5.6500,                 loss: nan
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
env2_first_0:                 episode reward: -5.1500,                 loss: nan
env2_second_0:                 episode reward: 5.1500,                 loss: nan
env3_first_0:                 episode reward: -6.4000,                 loss: nan
env3_second_0:                 episode reward: 6.4000,                 loss: nan
env4_first_0:                 episode reward: -5.9000,                 loss: nan
env4_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 116.8168s / 5165.7609 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0154
env0_second_0:                 episode reward: 7.0000,                 loss: nan
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
env2_first_0:                 episode reward: -6.3500,                 loss: nan
env2_second_0:                 episode reward: 6.3500,                 loss: nan
env3_first_0:                 episode reward: -6.1000,                 loss: nan
env3_second_0:                 episode reward: 6.1000,                 loss: nan
env4_first_0:                 episode reward: -6.3000,                 loss: nan
env4_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 123.6454s / 5289.4062 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0167
env0_second_0:                 episode reward: 6.7500,                 loss: nan
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
env2_first_0:                 episode reward: -6.8500,                 loss: nan
env2_second_0:                 episode reward: 6.8500,                 loss: nan
env3_first_0:                 episode reward: -6.7000,                 loss: nan
env3_second_0:                 episode reward: 6.7000,                 loss: nan
env4_first_0:                 episode reward: -6.9000,                 loss: nan
env4_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 129.8790s / 5419.2852 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0248
env0_second_0:                 episode reward: 5.9000,                 loss: nan
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
env2_first_0:                 episode reward: -5.9500,                 loss: nan
env2_second_0:                 episode reward: 5.9500,                 loss: nan
env3_first_0:                 episode reward: -6.2000,                 loss: nan
env3_second_0:                 episode reward: 6.2000,                 loss: nan
env4_first_0:                 episode reward: -6.2000,                 loss: nan
env4_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 136.6089s / 5555.8940 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0173
env0_second_0:                 episode reward: 3.9500,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
env2_first_0:                 episode reward: -3.8500,                 loss: nan
env2_second_0:                 episode reward: 3.8500,                 loss: nan
env3_first_0:                 episode reward: -3.5000,                 loss: nan
env3_second_0:                 episode reward: 3.5000,                 loss: nan
env4_first_0:                 episode reward: -3.3500,                 loss: nan
env4_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 142.5074s / 5698.4014 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0126
env0_second_0:                 episode reward: -1.2000,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
env2_first_0:                 episode reward: 1.1500,                 loss: nan
env2_second_0:                 episode reward: -1.1500,                 loss: nan
env3_first_0:                 episode reward: 0.1500,                 loss: nan
env3_second_0:                 episode reward: -0.1500,                 loss: nan
env4_first_0:                 episode reward: -0.4000,                 loss: nan
env4_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 147.7415s / 5846.1429 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0138
env0_second_0:                 episode reward: -1.8000,                 loss: nan
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
env2_first_0:                 episode reward: 1.6000,                 loss: nan
env2_second_0:                 episode reward: -1.6000,                 loss: nan
env3_first_0:                 episode reward: 2.9000,                 loss: nan
env3_second_0:                 episode reward: -2.9000,                 loss: nan
env4_first_0:                 episode reward: 1.2000,                 loss: nan
env4_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 155.1572s / 6001.3001 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0122
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
env2_first_0:                 episode reward: -2.5000,                 loss: nan
env2_second_0:                 episode reward: 2.5000,                 loss: nan
env3_first_0:                 episode reward: -3.4000,                 loss: nan
env3_second_0:                 episode reward: 3.4000,                 loss: nan
env4_first_0:                 episode reward: -1.7000,                 loss: nan
env4_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 169.3866s / 6170.6867 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0092
env0_second_0:                 episode reward: -4.4000,                 loss: 0.0367
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
env2_first_0:                 episode reward: 4.5000,                 loss: nan
env2_second_0:                 episode reward: -4.5000,                 loss: nan
env3_first_0:                 episode reward: 4.1500,                 loss: nan
env3_second_0:                 episode reward: -4.1500,                 loss: nan
env4_first_0:                 episode reward: 4.4000,                 loss: nan
env4_second_0:                 episode reward: -4.4000,                 loss: nan
Score delta: 10.8, update the opponent.
Episode: 861/10000 (8.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 190.6057s / 6361.2924 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0129
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 7.6500,                 loss: nan
env3_second_0:                 episode reward: -7.6500,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.3814s / 6552.6738 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0122
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.1077s / 6744.7816 s
env0_first_0:                 episode reward: 6.6000,                 loss: nan
env0_second_0:                 episode reward: -6.6000,                 loss: 0.0113
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
env2_first_0:                 episode reward: 5.7000,                 loss: nan
env2_second_0:                 episode reward: -5.7000,                 loss: nan
env3_first_0:                 episode reward: 6.7500,                 loss: nan
env3_second_0:                 episode reward: -6.7500,                 loss: nan
env4_first_0:                 episode reward: 6.3000,                 loss: nan
env4_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.0732s / 6936.8548 s
env0_first_0:                 episode reward: 4.2000,                 loss: nan
env0_second_0:                 episode reward: -4.2000,                 loss: 0.0140
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
env2_first_0:                 episode reward: 5.3500,                 loss: nan
env2_second_0:                 episode reward: -5.3500,                 loss: nan
env3_first_0:                 episode reward: 4.0000,                 loss: nan
env3_second_0:                 episode reward: -4.0000,                 loss: nan
env4_first_0:                 episode reward: 5.6000,                 loss: nan
env4_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.9614s / 7128.8162 s
env0_first_0:                 episode reward: 6.0000,                 loss: nan
env0_second_0:                 episode reward: -6.0000,                 loss: 0.0161
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
env2_first_0:                 episode reward: 6.4000,                 loss: nan
env2_second_0:                 episode reward: -6.4000,                 loss: nan
env3_first_0:                 episode reward: 5.4000,                 loss: nan
env3_second_0:                 episode reward: -5.4000,                 loss: nan
env4_first_0:                 episode reward: 5.5500,                 loss: nan
env4_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.4779s / 7320.2942 s
env0_first_0:                 episode reward: 5.7500,                 loss: nan
env0_second_0:                 episode reward: -5.7500,                 loss: 0.0158
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
env2_first_0:                 episode reward: 5.8000,                 loss: nan
env2_second_0:                 episode reward: -5.8000,                 loss: nan
env3_first_0:                 episode reward: 5.7000,                 loss: nan
env3_second_0:                 episode reward: -5.7000,                 loss: nan
env4_first_0:                 episode reward: 5.9000,                 loss: nan
env4_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 190.6506s / 7510.9448 s
env0_first_0:                 episode reward: 7.3000,                 loss: nan
env0_second_0:                 episode reward: -7.3000,                 loss: 0.0145
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
env2_first_0:                 episode reward: 6.8500,                 loss: nan
env2_second_0:                 episode reward: -6.8500,                 loss: nan
env3_first_0:                 episode reward: 7.2000,                 loss: nan
env3_second_0:                 episode reward: -7.2000,                 loss: nan
env4_first_0:                 episode reward: 7.6000,                 loss: nan
env4_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.6268s / 7702.5716 s
env0_first_0:                 episode reward: 7.0500,                 loss: nan
env0_second_0:                 episode reward: -7.0500,                 loss: 0.0128
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
env2_first_0:                 episode reward: 5.7500,                 loss: nan
env2_second_0:                 episode reward: -5.7500,                 loss: nan
env3_first_0:                 episode reward: 7.5500,                 loss: nan
env3_second_0:                 episode reward: -7.5500,                 loss: nan
env4_first_0:                 episode reward: 6.9000,                 loss: nan
env4_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.8333s / 7895.4049 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0110
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
env2_first_0:                 episode reward: 2.9000,                 loss: nan
env2_second_0:                 episode reward: -2.9000,                 loss: nan
env3_first_0:                 episode reward: 1.9500,                 loss: nan
env3_second_0:                 episode reward: -1.9500,                 loss: nan
env4_first_0:                 episode reward: 2.4500,                 loss: nan
env4_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.2756s / 8086.6805 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0094
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
env2_first_0:                 episode reward: 4.7000,                 loss: nan
env2_second_0:                 episode reward: -4.7000,                 loss: nan
env3_first_0:                 episode reward: 3.0500,                 loss: nan
env3_second_0:                 episode reward: -3.0500,                 loss: nan
env4_first_0:                 episode reward: 4.9000,                 loss: nan
env4_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 190.8859s / 8277.5664 s
env0_first_0:                 episode reward: 3.9500,                 loss: nan
env0_second_0:                 episode reward: -3.9500,                 loss: 0.0098
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
env2_first_0:                 episode reward: 3.7000,                 loss: nan
env2_second_0:                 episode reward: -3.7000,                 loss: nan
env3_first_0:                 episode reward: 3.7500,                 loss: nan
env3_second_0:                 episode reward: -3.7500,                 loss: nan
env4_first_0:                 episode reward: 3.9000,                 loss: nan
env4_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.8407s / 8469.4071 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0116
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
env2_first_0:                 episode reward: 3.9000,                 loss: nan
env2_second_0:                 episode reward: -3.9000,                 loss: nan
env3_first_0:                 episode reward: 3.0500,                 loss: nan
env3_second_0:                 episode reward: -3.0500,                 loss: nan
env4_first_0:                 episode reward: 3.8500,                 loss: nan
env4_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.8483s / 8662.2553 s
env0_first_0:                 episode reward: 0.9500,                 loss: nan
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0120
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
env2_first_0:                 episode reward: 1.7000,                 loss: nan
env2_second_0:                 episode reward: -1.7000,                 loss: nan
env3_first_0:                 episode reward: 1.4500,                 loss: nan
env3_second_0:                 episode reward: -1.4500,                 loss: nan
env4_first_0:                 episode reward: 1.1000,                 loss: nan
env4_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.8271s / 8854.0824 s
env0_first_0:                 episode reward: -0.3000,                 loss: nan
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0118
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
env2_first_0:                 episode reward: -0.6500,                 loss: nan
env2_second_0:                 episode reward: 0.6500,                 loss: nan
env3_first_0:                 episode reward: -1.0000,                 loss: nan
env3_second_0:                 episode reward: 1.0000,                 loss: nan
env4_first_0:                 episode reward: -1.1500,                 loss: nan
env4_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.2091s / 9045.2916 s
env0_first_0:                 episode reward: 7.4000,                 loss: nan
env0_second_0:                 episode reward: -7.4000,                 loss: 0.0112
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
env2_first_0:                 episode reward: 6.5000,                 loss: nan
env2_second_0:                 episode reward: -6.5000,                 loss: nan
env3_first_0:                 episode reward: 7.4000,                 loss: nan
env3_second_0:                 episode reward: -7.4000,                 loss: nan
env4_first_0:                 episode reward: 7.4000,                 loss: nan
env4_second_0:                 episode reward: -7.4000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 190.6441s / 9235.9356 s
env0_first_0:                 episode reward: 4.4000,                 loss: nan
env0_second_0:                 episode reward: -4.4000,                 loss: 0.0114
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
env2_first_0:                 episode reward: 4.2500,                 loss: nan
env2_second_0:                 episode reward: -4.2500,                 loss: nan
env3_first_0:                 episode reward: 3.7000,                 loss: nan
env3_second_0:                 episode reward: -3.7000,                 loss: nan
env4_first_0:                 episode reward: 4.8000,                 loss: nan
env4_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.4409s / 9427.3765 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0116
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
env2_first_0:                 episode reward: 2.0500,                 loss: nan
env2_second_0:                 episode reward: -2.0500,                 loss: nan
env3_first_0:                 episode reward: 1.3500,                 loss: nan
env3_second_0:                 episode reward: -1.3500,                 loss: nan
env4_first_0:                 episode reward: 1.0000,                 loss: nan
env4_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.9322s / 9619.3088 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0122
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
env2_first_0:                 episode reward: 3.6500,                 loss: nan
env2_second_0:                 episode reward: -3.6500,                 loss: nan
env3_first_0:                 episode reward: 3.6000,                 loss: nan
env3_second_0:                 episode reward: -3.6000,                 loss: nan
env4_first_0:                 episode reward: 2.3000,                 loss: nan
env4_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.3255s / 9811.6343 s
env0_first_0:                 episode reward: 1.6500,                 loss: nan
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0119
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
env2_first_0:                 episode reward: 2.2500,                 loss: nan
env2_second_0:                 episode reward: -2.2500,                 loss: nan
env3_first_0:                 episode reward: 2.8500,                 loss: nan
env3_second_0:                 episode reward: -2.8500,                 loss: nan
env4_first_0:                 episode reward: 2.7000,                 loss: nan
env4_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.3418s / 10003.9761 s
env0_first_0:                 episode reward: 1.9500,                 loss: nan
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0114
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
env2_first_0:                 episode reward: 0.6500,                 loss: nan
env2_second_0:                 episode reward: -0.6500,                 loss: nan
env3_first_0:                 episode reward: 1.7500,                 loss: nan
env3_second_0:                 episode reward: -1.7500,                 loss: nan
env4_first_0:                 episode reward: 1.9500,                 loss: nan
env4_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.8222s / 10195.7982 s
env0_first_0:                 episode reward: -3.8000,                 loss: nan
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0110
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
env2_first_0:                 episode reward: -2.7500,                 loss: nan
env2_second_0:                 episode reward: 2.7500,                 loss: nan
env3_first_0:                 episode reward: -2.8000,                 loss: nan
env3_second_0:                 episode reward: 2.8000,                 loss: nan
env4_first_0:                 episode reward: -3.3500,                 loss: nan
env4_second_0:                 episode reward: 3.3500,                 loss: nan
Score delta: 10.4, update the opponent.
Episode: 1281/10000 (12.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 170.8643s / 10366.6626 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0229
env0_second_0:                 episode reward: 3.3500,                 loss: nan
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
env2_first_0:                 episode reward: -2.9000,                 loss: nan
env2_second_0:                 episode reward: 2.9000,                 loss: nan
env3_first_0:                 episode reward: -3.2000,                 loss: nan
env3_second_0:                 episode reward: 3.2000,                 loss: nan
env4_first_0:                 episode reward: -3.4500,                 loss: nan
env4_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 178.1429s / 10544.8055 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0096
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
env2_first_0:                 episode reward: -0.4000,                 loss: nan
env2_second_0:                 episode reward: 0.4000,                 loss: nan
env3_first_0:                 episode reward: -0.6500,                 loss: nan
env3_second_0:                 episode reward: 0.6500,                 loss: nan
env4_first_0:                 episode reward: 0.2000,                 loss: nan
env4_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 190.5395s / 10735.3450 s
env0_first_0:                 episode reward: 6.6000,                 loss: 0.0092
env0_second_0:                 episode reward: -6.6000,                 loss: 0.0360
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
env2_first_0:                 episode reward: 5.8500,                 loss: nan
env2_second_0:                 episode reward: -5.8500,                 loss: nan
env3_first_0:                 episode reward: 6.4500,                 loss: nan
env3_second_0:                 episode reward: -6.4500,                 loss: nan
env4_first_0:                 episode reward: 7.2500,                 loss: nan
env4_second_0:                 episode reward: -7.2500,                 loss: nan
Score delta: 11.6, update the opponent.
Episode: 1341/10000 (13.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.5488s / 10927.8939 s
env0_first_0:                 episode reward: 4.1500,                 loss: nan
env0_second_0:                 episode reward: -4.1500,                 loss: 0.0173
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
env2_first_0:                 episode reward: 5.1000,                 loss: nan
env2_second_0:                 episode reward: -5.1000,                 loss: nan
env3_first_0:                 episode reward: 5.1500,                 loss: nan
env3_second_0:                 episode reward: -5.1500,                 loss: nan
env4_first_0:                 episode reward: 4.1500,                 loss: nan
env4_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.7042s / 11119.5981 s
env0_first_0:                 episode reward: 4.9500,                 loss: nan
env0_second_0:                 episode reward: -4.9500,                 loss: 0.0145
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
env2_first_0:                 episode reward: 5.4000,                 loss: nan
env2_second_0:                 episode reward: -5.4000,                 loss: nan
env3_first_0:                 episode reward: 5.6500,                 loss: nan
env3_second_0:                 episode reward: -5.6500,                 loss: nan
env4_first_0:                 episode reward: 5.1500,                 loss: nan
env4_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.0780s / 11311.6761 s
env0_first_0:                 episode reward: 1.4000,                 loss: nan
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0132
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
env2_first_0:                 episode reward: 1.8000,                 loss: nan
env2_second_0:                 episode reward: -1.8000,                 loss: nan
env3_first_0:                 episode reward: 1.5000,                 loss: nan
env3_second_0:                 episode reward: -1.5000,                 loss: nan
env4_first_0:                 episode reward: 1.8000,                 loss: nan
env4_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.9479s / 11503.6240 s
env0_first_0:                 episode reward: 2.1000,                 loss: nan
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0106
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
env2_first_0:                 episode reward: 2.5000,                 loss: nan
env2_second_0:                 episode reward: -2.5000,                 loss: nan
env3_first_0:                 episode reward: 2.7500,                 loss: nan
env3_second_0:                 episode reward: -2.7500,                 loss: nan
env4_first_0:                 episode reward: 2.4000,                 loss: nan
env4_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.9096s / 11695.5336 s
env0_first_0:                 episode reward: 1.6000,                 loss: nan
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0098
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
env2_first_0:                 episode reward: 3.0000,                 loss: nan
env2_second_0:                 episode reward: -3.0000,                 loss: nan
env3_first_0:                 episode reward: 2.8500,                 loss: nan
env3_second_0:                 episode reward: -2.8500,                 loss: nan
env4_first_0:                 episode reward: 1.8000,                 loss: nan
env4_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.5887s / 11887.1223 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0105
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
env2_first_0:                 episode reward: 4.2500,                 loss: nan
env2_second_0:                 episode reward: -4.2500,                 loss: nan
env3_first_0:                 episode reward: 4.2000,                 loss: nan
env3_second_0:                 episode reward: -4.2000,                 loss: nan
env4_first_0:                 episode reward: 4.2500,                 loss: nan
env4_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.1371s / 12080.2595 s
env0_first_0:                 episode reward: -0.9000,                 loss: nan
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0107
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
env2_first_0:                 episode reward: -0.9000,                 loss: nan
env2_second_0:                 episode reward: 0.9000,                 loss: nan
env3_first_0:                 episode reward: -0.5000,                 loss: nan
env3_second_0:                 episode reward: 0.5000,                 loss: nan
env4_first_0:                 episode reward: -1.0500,                 loss: nan
env4_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 187.0493s / 12267.3087 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0246
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0106
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
env2_first_0:                 episode reward: -3.4500,                 loss: nan
env2_second_0:                 episode reward: 3.4500,                 loss: nan
env3_first_0:                 episode reward: -3.4500,                 loss: nan
env3_second_0:                 episode reward: 3.4500,                 loss: nan
env4_first_0:                 episode reward: -3.4500,                 loss: nan
env4_second_0:                 episode reward: 3.4500,                 loss: nan
Score delta: 11.6, update the opponent.
Episode: 1501/10000 (15.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.3100s / 12458.6187 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0118
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
env2_first_0:                 episode reward: -2.2500,                 loss: nan
env2_second_0:                 episode reward: 2.2500,                 loss: nan
env3_first_0:                 episode reward: -2.0500,                 loss: nan
env3_second_0:                 episode reward: 2.0500,                 loss: nan
env4_first_0:                 episode reward: -2.3000,                 loss: nan
env4_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 190.7273s / 12649.3461 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0104
env0_second_0:                 episode reward: 3.4000,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
env2_first_0:                 episode reward: -3.4000,                 loss: nan
env2_second_0:                 episode reward: 3.4000,                 loss: nan
env3_first_0:                 episode reward: -2.9000,                 loss: nan
env3_second_0:                 episode reward: 2.9000,                 loss: nan
env4_first_0:                 episode reward: -2.9000,                 loss: nan
env4_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.8994s / 12841.2454 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0126
env0_second_0:                 episode reward: -5.0500,                 loss: 0.0416
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
env2_first_0:                 episode reward: 3.8000,                 loss: nan
env2_second_0:                 episode reward: -3.8000,                 loss: nan
env3_first_0:                 episode reward: 3.2500,                 loss: nan
env3_second_0:                 episode reward: -3.2500,                 loss: nan
env4_first_0:                 episode reward: 4.1000,                 loss: nan
env4_second_0:                 episode reward: -4.1000,                 loss: nan
Score delta: 10.4, update the opponent.
Episode: 1561/10000 (15.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.3707s / 13032.6162 s
env0_first_0:                 episode reward: 5.2000,                 loss: nan
env0_second_0:                 episode reward: -5.2000,                 loss: 0.0238
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
env2_first_0:                 episode reward: 5.3000,                 loss: nan
env2_second_0:                 episode reward: -5.3000,                 loss: nan
env3_first_0:                 episode reward: 5.1000,                 loss: nan
env3_second_0:                 episode reward: -5.1000,                 loss: nan
env4_first_0:                 episode reward: 4.8500,                 loss: nan
env4_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.2135s / 13224.8297 s
env0_first_0:                 episode reward: 5.5000,                 loss: nan
env0_second_0:                 episode reward: -5.5000,                 loss: 0.0179
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
env2_first_0:                 episode reward: 5.4500,                 loss: nan
env2_second_0:                 episode reward: -5.4500,                 loss: nan
env3_first_0:                 episode reward: 5.7000,                 loss: nan
env3_second_0:                 episode reward: -5.7000,                 loss: nan
env4_first_0:                 episode reward: 5.9000,                 loss: nan
env4_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.5513s / 13417.3810 s
env0_first_0:                 episode reward: -3.5500,                 loss: nan
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0147
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
env2_first_0:                 episode reward: -3.4500,                 loss: nan
env2_second_0:                 episode reward: 3.4500,                 loss: nan
env3_first_0:                 episode reward: -3.6500,                 loss: nan
env3_second_0:                 episode reward: 3.6500,                 loss: nan
env4_first_0:                 episode reward: -3.6500,                 loss: nan
env4_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.4011s / 13608.7821 s
env0_first_0:                 episode reward: -3.9000,                 loss: nan
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0116
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
env2_first_0:                 episode reward: -3.7500,                 loss: nan
env2_second_0:                 episode reward: 3.7500,                 loss: nan
env3_first_0:                 episode reward: -3.6500,                 loss: nan
env3_second_0:                 episode reward: 3.6500,                 loss: nan
env4_first_0:                 episode reward: -3.7000,                 loss: nan
env4_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.0961s / 13799.8782 s
env0_first_0:                 episode reward: -4.1500,                 loss: nan
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0098
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
env2_first_0:                 episode reward: -4.2000,                 loss: nan
env2_second_0:                 episode reward: 4.2000,                 loss: nan
env3_first_0:                 episode reward: -4.3000,                 loss: nan
env3_second_0:                 episode reward: 4.3000,                 loss: nan
env4_first_0:                 episode reward: -3.9000,                 loss: nan
env4_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.8804s / 13991.7586 s
env0_first_0:                 episode reward: 0.4000,                 loss: nan
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0103
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
env2_first_0:                 episode reward: 0.1500,                 loss: nan
env2_second_0:                 episode reward: -0.1500,                 loss: nan
env3_first_0:                 episode reward: 0.3000,                 loss: nan
env3_second_0:                 episode reward: -0.3000,                 loss: nan
env4_first_0:                 episode reward: 0.0500,                 loss: nan
env4_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.2959s / 14185.0545 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0327
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0114
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
env2_first_0:                 episode reward: -2.7500,                 loss: nan
env2_second_0:                 episode reward: 2.7500,                 loss: nan
env3_first_0:                 episode reward: -2.7500,                 loss: nan
env3_second_0:                 episode reward: 2.7500,                 loss: nan
env4_first_0:                 episode reward: -2.7500,                 loss: nan
env4_second_0:                 episode reward: 2.7500,                 loss: nan
Score delta: 10.8, update the opponent.
Episode: 1701/10000 (17.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.0457s / 14376.1002 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0159
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
env2_first_0:                 episode reward: 3.9000,                 loss: nan
env2_second_0:                 episode reward: -3.9000,                 loss: nan
env3_first_0:                 episode reward: 3.8000,                 loss: nan
env3_second_0:                 episode reward: -3.8000,                 loss: nan
env4_first_0:                 episode reward: 4.0500,                 loss: nan
env4_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.8587s / 14568.9589 s
env0_first_0:                 episode reward: 3.8000,                 loss: 0.0115
env0_second_0:                 episode reward: -3.8000,                 loss: 0.0228
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
env2_first_0:                 episode reward: 3.8000,                 loss: nan
env2_second_0:                 episode reward: -3.8000,                 loss: nan
env3_first_0:                 episode reward: 3.9500,                 loss: nan
env3_second_0:                 episode reward: -3.9500,                 loss: nan
env4_first_0:                 episode reward: 4.0500,                 loss: nan
env4_second_0:                 episode reward: -4.0500,                 loss: nan
Score delta: 10.4, update the opponent.
Episode: 1741/10000 (17.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.6046s / 14761.5635 s
env0_first_0:                 episode reward: -1.7500,                 loss: nan
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0142
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
env2_first_0:                 episode reward: -1.4500,                 loss: nan
env2_second_0:                 episode reward: 1.4500,                 loss: nan
env3_first_0:                 episode reward: -1.9000,                 loss: nan
env3_second_0:                 episode reward: 1.9000,                 loss: nan
env4_first_0:                 episode reward: -1.9000,                 loss: nan
env4_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.7512s / 14954.3148 s
env0_first_0:                 episode reward: -2.1000,                 loss: nan
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0146
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
env2_first_0:                 episode reward: -2.3000,                 loss: nan
env2_second_0:                 episode reward: 2.3000,                 loss: nan
env3_first_0:                 episode reward: -1.8500,                 loss: nan
env3_second_0:                 episode reward: 1.8500,                 loss: nan
env4_first_0:                 episode reward: -1.9500,                 loss: nan
env4_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.7016s / 15148.0163 s
env0_first_0:                 episode reward: 1.1500,                 loss: nan
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0126
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
env2_first_0:                 episode reward: 1.2500,                 loss: nan
env2_second_0:                 episode reward: -1.2500,                 loss: nan
env3_first_0:                 episode reward: 1.2500,                 loss: nan
env3_second_0:                 episode reward: -1.2500,                 loss: nan
env4_first_0:                 episode reward: 1.2000,                 loss: nan
env4_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.3032s / 15340.3195 s
env0_first_0:                 episode reward: 0.8500,                 loss: nan
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0117
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
env2_first_0:                 episode reward: 0.9000,                 loss: nan
env2_second_0:                 episode reward: -0.9000,                 loss: nan
env3_first_0:                 episode reward: 0.9000,                 loss: nan
env3_second_0:                 episode reward: -0.9000,                 loss: nan
env4_first_0:                 episode reward: 0.8500,                 loss: nan
env4_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.4595s / 15532.7791 s
env0_first_0:                 episode reward: -0.9500,                 loss: nan
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0118
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
env2_first_0:                 episode reward: -0.9500,                 loss: nan
env2_second_0:                 episode reward: 0.9500,                 loss: nan
env3_first_0:                 episode reward: -0.9500,                 loss: nan
env3_second_0:                 episode reward: 0.9500,                 loss: nan
env4_first_0:                 episode reward: -1.3500,                 loss: nan
env4_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.7616s / 15725.5407 s
env0_first_0:                 episode reward: -3.6500,                 loss: nan
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0120
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
env2_first_0:                 episode reward: -3.7500,                 loss: nan
env2_second_0:                 episode reward: 3.7500,                 loss: nan
env3_first_0:                 episode reward: -3.7500,                 loss: nan
env3_second_0:                 episode reward: 3.7500,                 loss: nan
env4_first_0:                 episode reward: -3.3500,                 loss: nan
env4_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.7996s / 15919.3403 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0370
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0124
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
env2_first_0:                 episode reward: -3.3000,                 loss: nan
env2_second_0:                 episode reward: 3.3000,                 loss: nan
env3_first_0:                 episode reward: -3.1000,                 loss: nan
env3_second_0:                 episode reward: 3.1000,                 loss: nan
env4_first_0:                 episode reward: -3.3000,                 loss: nan
env4_second_0:                 episode reward: 3.3000,                 loss: nan
Score delta: 10.2, update the opponent.
Episode: 1881/10000 (18.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.2906s / 16112.6310 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0170
env0_second_0:                 episode reward: 5.6500,                 loss: nan
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
env2_first_0:                 episode reward: -5.6500,                 loss: nan
env2_second_0:                 episode reward: 5.6500,                 loss: nan
env3_first_0:                 episode reward: -5.6500,                 loss: nan
env3_second_0:                 episode reward: 5.6500,                 loss: nan
env4_first_0:                 episode reward: -5.6500,                 loss: nan
env4_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.7599s / 16306.3909 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0108
env0_second_0:                 episode reward: 4.6000,                 loss: nan
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
env2_first_0:                 episode reward: -4.6000,                 loss: nan
env2_second_0:                 episode reward: 4.6000,                 loss: nan
env3_first_0:                 episode reward: -4.6000,                 loss: nan
env3_second_0:                 episode reward: 4.6000,                 loss: nan
env4_first_0:                 episode reward: -4.6000,                 loss: nan
env4_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.5414s / 16499.9323 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0123
env0_second_0:                 episode reward: 6.1500,                 loss: nan
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
env2_first_0:                 episode reward: -6.1500,                 loss: nan
env2_second_0:                 episode reward: 6.1500,                 loss: nan
env3_first_0:                 episode reward: -6.1500,                 loss: nan
env3_second_0:                 episode reward: 6.1500,                 loss: nan
env4_first_0:                 episode reward: -6.1500,                 loss: nan
env4_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.3981s / 16692.3304 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0160
env0_second_0:                 episode reward: 6.0000,                 loss: nan
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
env2_first_0:                 episode reward: -6.1500,                 loss: nan
env2_second_0:                 episode reward: 6.1500,                 loss: nan
env3_first_0:                 episode reward: -6.0000,                 loss: nan
env3_second_0:                 episode reward: 6.0000,                 loss: nan
env4_first_0:                 episode reward: -6.0000,                 loss: nan
env4_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.4509s / 16884.7813 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0199
env0_second_0:                 episode reward: 6.0500,                 loss: nan
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
env2_first_0:                 episode reward: -6.1000,                 loss: nan
env2_second_0:                 episode reward: 6.1000,                 loss: nan
env3_first_0:                 episode reward: -6.1000,                 loss: nan
env3_second_0:                 episode reward: 6.1000,                 loss: nan
env4_first_0:                 episode reward: -6.2000,                 loss: nan
env4_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.5868s / 17076.3681 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0227
env0_second_0:                 episode reward: 7.0000,                 loss: nan
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
env2_first_0:                 episode reward: -7.0000,                 loss: nan
env2_second_0:                 episode reward: 7.0000,                 loss: nan
env3_first_0:                 episode reward: -7.0000,                 loss: nan
env3_second_0:                 episode reward: 7.0000,                 loss: nan
env4_first_0:                 episode reward: -7.0000,                 loss: nan
env4_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.3228s / 17269.6909 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0301
env0_second_0:                 episode reward: 6.5000,                 loss: nan
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
env2_first_0:                 episode reward: -6.4000,                 loss: nan
env2_second_0:                 episode reward: 6.4000,                 loss: nan
env3_first_0:                 episode reward: -6.4000,                 loss: nan
env3_second_0:                 episode reward: 6.4000,                 loss: nan
env4_first_0:                 episode reward: -6.5000,                 loss: nan
env4_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.8129s / 17461.5039 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0297
env0_second_0:                 episode reward: 4.5500,                 loss: nan
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
env2_first_0:                 episode reward: -4.4500,                 loss: nan
env2_second_0:                 episode reward: 4.4500,                 loss: nan
env3_first_0:                 episode reward: -4.6000,                 loss: nan
env3_second_0:                 episode reward: 4.6000,                 loss: nan
env4_first_0:                 episode reward: -4.5000,                 loss: nan
env4_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.4459s / 17653.9497 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0178
env0_second_0:                 episode reward: 3.8500,                 loss: nan
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
env2_first_0:                 episode reward: -3.8000,                 loss: nan
env2_second_0:                 episode reward: 3.8000,                 loss: nan
env3_first_0:                 episode reward: -4.0500,                 loss: nan
env3_second_0:                 episode reward: 4.0500,                 loss: nan
env4_first_0:                 episode reward: -4.1500,                 loss: nan
env4_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.1833s / 17846.1331 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0161
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
env2_first_0:                 episode reward: -0.7500,                 loss: nan
env2_second_0:                 episode reward: 0.7500,                 loss: nan
env3_first_0:                 episode reward: -0.8000,                 loss: nan
env3_second_0:                 episode reward: 0.8000,                 loss: nan
env4_first_0:                 episode reward: -0.6000,                 loss: nan
env4_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.3421s / 18039.4752 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0158
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
env2_first_0:                 episode reward: 0.0500,                 loss: nan
env2_second_0:                 episode reward: -0.0500,                 loss: nan
env3_first_0:                 episode reward: -0.2500,                 loss: nan
env3_second_0:                 episode reward: 0.2500,                 loss: nan
env4_first_0:                 episode reward: -0.0500,                 loss: nan
env4_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.0544s / 18232.5296 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0127
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
env2_first_0:                 episode reward: -0.2500,                 loss: nan
env2_second_0:                 episode reward: 0.2500,                 loss: nan
env3_first_0:                 episode reward: -0.2500,                 loss: nan
env3_second_0:                 episode reward: 0.2500,                 loss: nan
env4_first_0:                 episode reward: -0.3500,                 loss: nan
env4_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.2547s / 18424.7842 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0153
env0_second_0:                 episode reward: 3.8000,                 loss: nan
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
env2_first_0:                 episode reward: -3.2000,                 loss: nan
env2_second_0:                 episode reward: 3.2000,                 loss: nan
env3_first_0:                 episode reward: -3.8000,                 loss: nan
env3_second_0:                 episode reward: 3.8000,                 loss: nan
env4_first_0:                 episode reward: -3.5000,                 loss: nan
env4_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.6781s / 18617.4624 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0207
env0_second_0:                 episode reward: 3.8500,                 loss: nan
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
env2_first_0:                 episode reward: -4.2000,                 loss: nan
env2_second_0:                 episode reward: 4.2000,                 loss: nan
env3_first_0:                 episode reward: -3.8000,                 loss: nan
env3_second_0:                 episode reward: 3.8000,                 loss: nan
env4_first_0:                 episode reward: -4.2000,                 loss: nan
env4_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.2249s / 18809.6873 s
env0_first_0:                 episode reward: 2.0000,                 loss: 0.0201
env0_second_0:                 episode reward: -2.0000,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
env2_first_0:                 episode reward: 1.6000,                 loss: nan
env2_second_0:                 episode reward: -1.6000,                 loss: nan
env3_first_0:                 episode reward: 1.9500,                 loss: nan
env3_second_0:                 episode reward: -1.9500,                 loss: nan
env4_first_0:                 episode reward: 2.0500,                 loss: nan
env4_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.8432s / 19003.5305 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0183
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0326
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
env2_first_0:                 episode reward: -5.7500,                 loss: nan
env2_second_0:                 episode reward: 5.7500,                 loss: nan
env3_first_0:                 episode reward: -5.7500,                 loss: nan
env3_second_0:                 episode reward: 5.7500,                 loss: nan
env4_first_0:                 episode reward: -5.7500,                 loss: nan
env4_second_0:                 episode reward: 5.7500,                 loss: nan
Score delta: 11.0, update the opponent.
Episode: 2201/10000 (22.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.8049s / 19197.3354 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0194
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0236
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
env2_first_0:                 episode reward: -2.7000,                 loss: nan
env2_second_0:                 episode reward: 2.7000,                 loss: nan
env3_first_0:                 episode reward: -2.9500,                 loss: nan
env3_second_0:                 episode reward: 2.9500,                 loss: nan
env4_first_0:                 episode reward: -2.7500,                 loss: nan
env4_second_0:                 episode reward: 2.7500,                 loss: nan
Score delta: 14.0, update the opponent.
Episode: 2221/10000 (22.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.7045s / 19391.0399 s
env0_first_0:                 episode reward: 7.3000,                 loss: 0.0159
env0_second_0:                 episode reward: -7.3000,                 loss: 0.0249
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
env2_first_0:                 episode reward: 7.2000,                 loss: nan
env2_second_0:                 episode reward: -7.2000,                 loss: nan
env3_first_0:                 episode reward: 7.2000,                 loss: nan
env3_second_0:                 episode reward: -7.2000,                 loss: nan
env4_first_0:                 episode reward: 7.1000,                 loss: nan
env4_second_0:                 episode reward: -7.1000,                 loss: nan
Score delta: 10.4, update the opponent.
Episode: 2241/10000 (22.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.1284s / 19584.1683 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0146
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.7284s / 19777.8967 s
env0_first_0:                 episode reward: 7.6000,                 loss: nan
env0_second_0:                 episode reward: -7.6000,                 loss: 0.0137
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
env2_first_0:                 episode reward: 7.6000,                 loss: nan
env2_second_0:                 episode reward: -7.6000,                 loss: nan
env3_first_0:                 episode reward: 7.7500,                 loss: nan
env3_second_0:                 episode reward: -7.7500,                 loss: nan
env4_first_0:                 episode reward: 7.6000,                 loss: nan
env4_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.7566s / 19972.6533 s
env0_first_0:                 episode reward: 6.7500,                 loss: nan
env0_second_0:                 episode reward: -6.7500,                 loss: 0.0161
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
env2_first_0:                 episode reward: 6.9000,                 loss: nan
env2_second_0:                 episode reward: -6.9000,                 loss: nan
env3_first_0:                 episode reward: 6.9000,                 loss: nan
env3_second_0:                 episode reward: -6.9000,                 loss: nan
env4_first_0:                 episode reward: 6.7500,                 loss: nan
env4_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.0987s / 20165.7520 s
env0_first_0:                 episode reward: 6.3000,                 loss: nan
env0_second_0:                 episode reward: -6.3000,                 loss: 0.0185
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
env2_first_0:                 episode reward: 5.9500,                 loss: nan
env2_second_0:                 episode reward: -5.9500,                 loss: nan
env3_first_0:                 episode reward: 6.0500,                 loss: nan
env3_second_0:                 episode reward: -6.0500,                 loss: nan
env4_first_0:                 episode reward: 6.1000,                 loss: nan
env4_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.3774s / 20359.1293 s
env0_first_0:                 episode reward: 7.5500,                 loss: nan
env0_second_0:                 episode reward: -7.5500,                 loss: 0.0191
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
env2_first_0:                 episode reward: 7.5500,                 loss: nan
env2_second_0:                 episode reward: -7.5500,                 loss: nan
env3_first_0:                 episode reward: 7.5000,                 loss: nan
env3_second_0:                 episode reward: -7.5000,                 loss: nan
env4_first_0:                 episode reward: 7.6000,                 loss: nan
env4_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.0327s / 20553.1621 s
env0_first_0:                 episode reward: 5.2000,                 loss: nan
env0_second_0:                 episode reward: -5.2000,                 loss: 0.0187
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
env2_first_0:                 episode reward: 5.1500,                 loss: nan
env2_second_0:                 episode reward: -5.1500,                 loss: nan
env3_first_0:                 episode reward: 5.2000,                 loss: nan
env3_second_0:                 episode reward: -5.2000,                 loss: nan
env4_first_0:                 episode reward: 5.2000,                 loss: nan
env4_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.4160s / 20747.5780 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0189
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
env2_first_0:                 episode reward: 3.8000,                 loss: nan
env2_second_0:                 episode reward: -3.8000,                 loss: nan
env3_first_0:                 episode reward: 4.1000,                 loss: nan
env3_second_0:                 episode reward: -4.1000,                 loss: nan
env4_first_0:                 episode reward: 3.7500,                 loss: nan
env4_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.2681s / 20941.8462 s
env0_first_0:                 episode reward: 5.7500,                 loss: nan
env0_second_0:                 episode reward: -5.7500,                 loss: 0.0158
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
env2_first_0:                 episode reward: 5.7500,                 loss: nan
env2_second_0:                 episode reward: -5.7500,                 loss: nan
env3_first_0:                 episode reward: 5.7500,                 loss: nan
env3_second_0:                 episode reward: -5.7500,                 loss: nan
env4_first_0:                 episode reward: 5.3000,                 loss: nan
env4_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.4328s / 21135.2790 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0132
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
env2_first_0:                 episode reward: 3.1000,                 loss: nan
env2_second_0:                 episode reward: -3.1000,                 loss: nan
env3_first_0:                 episode reward: 3.1000,                 loss: nan
env3_second_0:                 episode reward: -3.1000,                 loss: nan
env4_first_0:                 episode reward: 3.1000,                 loss: nan
env4_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.0150s / 21329.2940 s
env0_first_0:                 episode reward: 4.9500,                 loss: nan
env0_second_0:                 episode reward: -4.9500,                 loss: 0.0132
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
env2_first_0:                 episode reward: 4.8500,                 loss: nan
env2_second_0:                 episode reward: -4.8500,                 loss: nan
env3_first_0:                 episode reward: 4.6500,                 loss: nan
env3_second_0:                 episode reward: -4.6500,                 loss: nan
env4_first_0:                 episode reward: 4.7500,                 loss: nan
env4_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.5074s / 21522.8014 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0135
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
env2_first_0:                 episode reward: 2.9500,                 loss: nan
env2_second_0:                 episode reward: -2.9500,                 loss: nan
env3_first_0:                 episode reward: 2.8500,                 loss: nan
env3_second_0:                 episode reward: -2.8500,                 loss: nan
env4_first_0:                 episode reward: 3.0000,                 loss: nan
env4_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.4241s / 21717.2255 s
env0_first_0:                 episode reward: 2.1000,                 loss: nan
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0146
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
env2_first_0:                 episode reward: 2.2500,                 loss: nan
env2_second_0:                 episode reward: -2.2500,                 loss: nan
env3_first_0:                 episode reward: 1.8500,                 loss: nan
env3_second_0:                 episode reward: -1.8500,                 loss: nan
env4_first_0:                 episode reward: 2.2500,                 loss: nan
env4_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.7866s / 21911.0122 s
env0_first_0:                 episode reward: 1.5000,                 loss: nan
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0150
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
env2_first_0:                 episode reward: 1.5000,                 loss: nan
env2_second_0:                 episode reward: -1.5000,                 loss: nan
env3_first_0:                 episode reward: 1.5000,                 loss: nan
env3_second_0:                 episode reward: -1.5000,                 loss: nan
env4_first_0:                 episode reward: 1.5000,                 loss: nan
env4_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.2998s / 22105.3119 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0151
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
env2_first_0:                 episode reward: 3.5500,                 loss: nan
env2_second_0:                 episode reward: -3.5500,                 loss: nan
env3_first_0:                 episode reward: 3.6000,                 loss: nan
env3_second_0:                 episode reward: -3.6000,                 loss: nan
env4_first_0:                 episode reward: 3.6000,                 loss: nan
env4_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.7181s / 22300.0300 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0151
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
env2_first_0:                 episode reward: 2.7500,                 loss: nan
env2_second_0:                 episode reward: -2.7500,                 loss: nan
env3_first_0:                 episode reward: 2.7500,                 loss: nan
env3_second_0:                 episode reward: -2.7500,                 loss: nan
env4_first_0:                 episode reward: 2.7500,                 loss: nan
env4_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.4421s / 22494.4720 s
env0_first_0:                 episode reward: -2.6000,                 loss: nan
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0141
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
env2_first_0:                 episode reward: -2.6000,                 loss: nan
env2_second_0:                 episode reward: 2.6000,                 loss: nan
env3_first_0:                 episode reward: -2.5500,                 loss: nan
env3_second_0:                 episode reward: 2.5500,                 loss: nan
env4_first_0:                 episode reward: -2.5500,                 loss: nan
env4_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.6963s / 22689.1684 s
env0_first_0:                 episode reward: -4.3500,                 loss: nan
env0_second_0:                 episode reward: 4.3500,                 loss: 0.0126
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
env2_first_0:                 episode reward: -4.3500,                 loss: nan
env2_second_0:                 episode reward: 4.3500,                 loss: nan
env3_first_0:                 episode reward: -4.3500,                 loss: nan
env3_second_0:                 episode reward: 4.3500,                 loss: nan
env4_first_0:                 episode reward: -4.1000,                 loss: nan
env4_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.8596s / 22883.0280 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0376
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0124
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
env2_first_0:                 episode reward: -1.4000,                 loss: nan
env2_second_0:                 episode reward: 1.4000,                 loss: nan
env3_first_0:                 episode reward: -1.2500,                 loss: nan
env3_second_0:                 episode reward: 1.2500,                 loss: nan
env4_first_0:                 episode reward: -1.2500,                 loss: nan
env4_second_0:                 episode reward: 1.2500,                 loss: nan
Score delta: 10.4, update the opponent.
Episode: 2601/10000 (26.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.6668s / 23077.6948 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0166
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0291
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
env2_first_0:                 episode reward: 0.8000,                 loss: nan
env2_second_0:                 episode reward: -0.8000,                 loss: nan
env3_first_0:                 episode reward: 0.8000,                 loss: nan
env3_second_0:                 episode reward: -0.8000,                 loss: nan
env4_first_0:                 episode reward: 0.6500,                 loss: nan
env4_second_0:                 episode reward: -0.6500,                 loss: nan
Score delta: 10.2, update the opponent.
Episode: 2621/10000 (26.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.4160s / 23271.1108 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0269
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0182
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
env2_first_0:                 episode reward: -2.8500,                 loss: nan
env2_second_0:                 episode reward: 2.8500,                 loss: nan
env3_first_0:                 episode reward: -2.9000,                 loss: nan
env3_second_0:                 episode reward: 2.9000,                 loss: nan
env4_first_0:                 episode reward: -2.9000,                 loss: nan
env4_second_0:                 episode reward: 2.9000,                 loss: nan
Score delta: 10.2, update the opponent.
Episode: 2641/10000 (26.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.1229s / 23464.2337 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0159
env0_second_0:                 episode reward: -1.9500,                 loss: nan
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
env2_first_0:                 episode reward: 1.9500,                 loss: nan
env2_second_0:                 episode reward: -1.9500,                 loss: nan
env3_first_0:                 episode reward: 1.9500,                 loss: nan
env3_second_0:                 episode reward: -1.9500,                 loss: nan
env4_first_0:                 episode reward: 1.9500,                 loss: nan
env4_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.4942s / 23658.7279 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0150
env0_second_0:                 episode reward: -5.3500,                 loss: 0.0352
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
env2_first_0:                 episode reward: 5.3500,                 loss: nan
env2_second_0:                 episode reward: -5.3500,                 loss: nan
env3_first_0:                 episode reward: 5.3500,                 loss: nan
env3_second_0:                 episode reward: -5.3500,                 loss: nan
env4_first_0:                 episode reward: 5.3500,                 loss: nan
env4_second_0:                 episode reward: -5.3500,                 loss: nan
Score delta: 11.0, update the opponent.
Episode: 2681/10000 (26.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.6755s / 23854.4034 s
env0_first_0:                 episode reward: -1.0500,                 loss: nan
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0201
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
env2_first_0:                 episode reward: -1.2500,                 loss: nan
env2_second_0:                 episode reward: 1.2500,                 loss: nan
env3_first_0:                 episode reward: -1.2500,                 loss: nan
env3_second_0:                 episode reward: 1.2500,                 loss: nan
env4_first_0:                 episode reward: -1.2500,                 loss: nan
env4_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.4326s / 24048.8360 s
env0_first_0:                 episode reward: 4.7000,                 loss: nan
env0_second_0:                 episode reward: -4.7000,                 loss: 0.0194
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
env2_first_0:                 episode reward: 4.5500,                 loss: nan
env2_second_0:                 episode reward: -4.5500,                 loss: nan
env3_first_0:                 episode reward: 4.7000,                 loss: nan
env3_second_0:                 episode reward: -4.7000,                 loss: nan
env4_first_0:                 episode reward: 4.5500,                 loss: nan
env4_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.4657s / 24244.3017 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0166
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
env2_first_0:                 episode reward: 3.8000,                 loss: nan
env2_second_0:                 episode reward: -3.8000,                 loss: nan
env3_first_0:                 episode reward: 3.8000,                 loss: nan
env3_second_0:                 episode reward: -3.8000,                 loss: nan
env4_first_0:                 episode reward: 3.7000,                 loss: nan
env4_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 196.6659s / 24440.9677 s
env0_first_0:                 episode reward: -4.8500,                 loss: nan
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0144
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
env2_first_0:                 episode reward: -4.8500,                 loss: nan
env2_second_0:                 episode reward: 4.8500,                 loss: nan
env3_first_0:                 episode reward: -4.8500,                 loss: nan
env3_second_0:                 episode reward: 4.8500,                 loss: nan
env4_first_0:                 episode reward: -4.8500,                 loss: nan
env4_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.6952s / 24636.6629 s
env0_first_0:                 episode reward: -5.0000,                 loss: nan
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0131
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
env2_first_0:                 episode reward: -5.0000,                 loss: nan
env2_second_0:                 episode reward: 5.0000,                 loss: nan
env3_first_0:                 episode reward: -5.0000,                 loss: nan
env3_second_0:                 episode reward: 5.0000,                 loss: nan
env4_first_0:                 episode reward: -5.0000,                 loss: nan
env4_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.2472s / 24831.9101 s
env0_first_0:                 episode reward: -5.0000,                 loss: nan
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0132
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
env2_first_0:                 episode reward: -5.0000,                 loss: nan
env2_second_0:                 episode reward: 5.0000,                 loss: nan
env3_first_0:                 episode reward: -5.0000,                 loss: nan
env3_second_0:                 episode reward: 5.0000,                 loss: nan
env4_first_0:                 episode reward: -5.0000,                 loss: nan
env4_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.8345s / 25026.7446 s
env0_first_0:                 episode reward: -4.3000,                 loss: nan
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0133
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
env2_first_0:                 episode reward: -4.3000,                 loss: nan
env2_second_0:                 episode reward: 4.3000,                 loss: nan
env3_first_0:                 episode reward: -4.3000,                 loss: nan
env3_second_0:                 episode reward: 4.3000,                 loss: nan
env4_first_0:                 episode reward: -4.3000,                 loss: nan
env4_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.8440s / 25221.5885 s
env0_first_0:                 episode reward: -5.0000,                 loss: nan
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0126
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
env2_first_0:                 episode reward: -5.0000,                 loss: nan
env2_second_0:                 episode reward: 5.0000,                 loss: nan
env3_first_0:                 episode reward: -5.0000,                 loss: nan
env3_second_0:                 episode reward: 5.0000,                 loss: nan
env4_first_0:                 episode reward: -5.0000,                 loss: nan
env4_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.8085s / 25416.3970 s
env0_first_0:                 episode reward: -5.1000,                 loss: nan
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0108
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
env2_first_0:                 episode reward: -5.1000,                 loss: nan
env2_second_0:                 episode reward: 5.1000,                 loss: nan
env3_first_0:                 episode reward: -5.1000,                 loss: nan
env3_second_0:                 episode reward: 5.1000,                 loss: nan
env4_first_0:                 episode reward: -5.1000,                 loss: nan
env4_second_0:                 episode reward: 5.1000,                 loss: nan
Score delta: 10.4, update the opponent.
Episode: 2861/10000 (28.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.8913s / 25611.2884 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0277
env0_second_0:                 episode reward: 3.7500,                 loss: nan
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
env2_first_0:                 episode reward: -3.7500,                 loss: nan
env2_second_0:                 episode reward: 3.7500,                 loss: nan
env3_first_0:                 episode reward: -3.7500,                 loss: nan
env3_second_0:                 episode reward: 3.7500,                 loss: nan
env4_first_0:                 episode reward: -3.7500,                 loss: nan
env4_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.7593s / 25806.0477 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0136
env0_second_0:                 episode reward: 4.6000,                 loss: nan
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
env2_first_0:                 episode reward: -4.6000,                 loss: nan
env2_second_0:                 episode reward: 4.6000,                 loss: nan
env3_first_0:                 episode reward: -4.6000,                 loss: nan
env3_second_0:                 episode reward: 4.6000,                 loss: nan
env4_first_0:                 episode reward: -4.6000,                 loss: nan
env4_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.4164s / 26000.4640 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0148
env0_second_0:                 episode reward: 3.5500,                 loss: nan
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
env2_first_0:                 episode reward: -3.5000,                 loss: nan
env2_second_0:                 episode reward: 3.5000,                 loss: nan
env3_first_0:                 episode reward: -3.5000,                 loss: nan
env3_second_0:                 episode reward: 3.5000,                 loss: nan
env4_first_0:                 episode reward: -3.5500,                 loss: nan
env4_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.8038s / 26195.2678 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0184
env0_second_0:                 episode reward: 4.2500,                 loss: nan
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
env2_first_0:                 episode reward: -4.2500,                 loss: nan
env2_second_0:                 episode reward: 4.2500,                 loss: nan
env3_first_0:                 episode reward: -4.2500,                 loss: nan
env3_second_0:                 episode reward: 4.2500,                 loss: nan
env4_first_0:                 episode reward: -4.2500,                 loss: nan
env4_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.1570s / 26389.4248 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0188
env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
env2_first_0:                 episode reward: -2.6500,                 loss: nan
env2_second_0:                 episode reward: 2.6500,                 loss: nan
env3_first_0:                 episode reward: -2.6500,                 loss: nan
env3_second_0:                 episode reward: 2.6500,                 loss: nan
env4_first_0:                 episode reward: -2.7000,                 loss: nan
env4_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.4015s / 26584.8263 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0173
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0302
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
env2_first_0:                 episode reward: 2.3000,                 loss: nan
env2_second_0:                 episode reward: -2.3000,                 loss: nan
env3_first_0:                 episode reward: 2.1500,                 loss: nan
env3_second_0:                 episode reward: -2.1500,                 loss: nan
env4_first_0:                 episode reward: 2.2500,                 loss: nan
env4_second_0:                 episode reward: -2.2500,                 loss: nan
Score delta: 11.2, update the opponent.
Episode: 2981/10000 (29.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.1697s / 26779.9960 s
env0_first_0:                 episode reward: 5.8000,                 loss: nan
env0_second_0:                 episode reward: -5.8000,                 loss: 0.0151
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
env2_first_0:                 episode reward: 5.8000,                 loss: nan
env2_second_0:                 episode reward: -5.8000,                 loss: nan
env3_first_0:                 episode reward: 5.8000,                 loss: nan
env3_second_0:                 episode reward: -5.8000,                 loss: nan
env4_first_0:                 episode reward: 5.8000,                 loss: nan
env4_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.7734s / 26975.7694 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0181
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.5578s / 27171.3272 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0220
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
env2_first_0:                 episode reward: 2.8500,                 loss: nan
env2_second_0:                 episode reward: -2.8500,                 loss: nan
env3_first_0:                 episode reward: 2.8500,                 loss: nan
env3_second_0:                 episode reward: -2.8500,                 loss: nan
env4_first_0:                 episode reward: 2.8500,                 loss: nan
env4_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.4068s / 27365.7340 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0173
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.6043s / 27560.3383 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0131
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 196.1790s / 27756.5174 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0152
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.8058s / 27952.3231 s
env0_first_0:                 episode reward: 4.8000,                 loss: nan
env0_second_0:                 episode reward: -4.8000,                 loss: 0.0170
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
env2_first_0:                 episode reward: 4.8000,                 loss: nan
env2_second_0:                 episode reward: -4.8000,                 loss: nan
env3_first_0:                 episode reward: 4.8000,                 loss: nan
env3_second_0:                 episode reward: -4.8000,                 loss: nan
env4_first_0:                 episode reward: 4.8000,                 loss: nan
env4_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.7930s / 28148.1161 s
env0_first_0:                 episode reward: 7.0000,                 loss: nan
env0_second_0:                 episode reward: -7.0000,                 loss: 0.0136
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
env2_first_0:                 episode reward: 7.0000,                 loss: nan
env2_second_0:                 episode reward: -7.0000,                 loss: nan
env3_first_0:                 episode reward: 7.0000,                 loss: nan
env3_second_0:                 episode reward: -7.0000,                 loss: nan
env4_first_0:                 episode reward: 7.0000,                 loss: nan
env4_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 196.0520s / 28344.1680 s
env0_first_0:                 episode reward: 4.0000,                 loss: nan
env0_second_0:                 episode reward: -4.0000,                 loss: 0.0120
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
env2_first_0:                 episode reward: 4.0500,                 loss: nan
env2_second_0:                 episode reward: -4.0500,                 loss: nan
env3_first_0:                 episode reward: 4.3000,                 loss: nan
env3_second_0:                 episode reward: -4.3000,                 loss: nan
env4_first_0:                 episode reward: 4.2500,                 loss: nan
env4_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.4984s / 28537.6664 s
env0_first_0:                 episode reward: 5.7000,                 loss: nan
env0_second_0:                 episode reward: -5.7000,                 loss: 0.0101
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
env2_first_0:                 episode reward: 5.7000,                 loss: nan
env2_second_0:                 episode reward: -5.7000,                 loss: nan
env3_first_0:                 episode reward: 5.7000,                 loss: nan
env3_second_0:                 episode reward: -5.7000,                 loss: nan
env4_first_0:                 episode reward: 5.7000,                 loss: nan
env4_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.8818s / 28733.5482 s
env0_first_0:                 episode reward: 5.4500,                 loss: nan
env0_second_0:                 episode reward: -5.4500,                 loss: 0.0088
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
env2_first_0:                 episode reward: 5.4500,                 loss: nan
env2_second_0:                 episode reward: -5.4500,                 loss: nan
env3_first_0:                 episode reward: 5.4500,                 loss: nan
env3_second_0:                 episode reward: -5.4500,                 loss: nan
env4_first_0:                 episode reward: 5.4500,                 loss: nan
env4_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.6008s / 28929.1491 s
env0_first_0:                 episode reward: 4.8000,                 loss: nan
env0_second_0:                 episode reward: -4.8000,                 loss: 0.0088
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
env2_first_0:                 episode reward: 4.7500,                 loss: nan
env2_second_0:                 episode reward: -4.7500,                 loss: nan
env3_first_0:                 episode reward: 4.5000,                 loss: nan
env3_second_0:                 episode reward: -4.5000,                 loss: nan
env4_first_0:                 episode reward: 5.1000,                 loss: nan
env4_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.2988s / 29124.4479 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0090
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
env2_first_0:                 episode reward: 2.9500,                 loss: nan
env2_second_0:                 episode reward: -2.9500,                 loss: nan
env3_first_0:                 episode reward: 2.9500,                 loss: nan
env3_second_0:                 episode reward: -2.9500,                 loss: nan
env4_first_0:                 episode reward: 2.9500,                 loss: nan
env4_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.9203s / 29319.3682 s
env0_first_0:                 episode reward: 2.2500,                 loss: nan
env0_second_0:                 episode reward: -2.2500,                 loss: 0.0092
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
env2_first_0:                 episode reward: 2.2500,                 loss: nan
env2_second_0:                 episode reward: -2.2500,                 loss: nan
env3_first_0:                 episode reward: 2.1000,                 loss: nan
env3_second_0:                 episode reward: -2.1000,                 loss: nan
env4_first_0:                 episode reward: 1.6500,                 loss: nan
env4_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.5603s / 29514.9285 s
env0_first_0:                 episode reward: 0.1500,                 loss: nan
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0091
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
env2_first_0:                 episode reward: 0.1500,                 loss: nan
env2_second_0:                 episode reward: -0.1500,                 loss: nan
env3_first_0:                 episode reward: 0.1500,                 loss: nan
env3_second_0:                 episode reward: -0.1500,                 loss: nan
env4_first_0:                 episode reward: 0.1500,                 loss: nan
env4_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 196.5020s / 29711.4305 s
env0_first_0:                 episode reward: 1.4000,                 loss: nan
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0098
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
env2_first_0:                 episode reward: 0.7500,                 loss: nan
env2_second_0:                 episode reward: -0.7500,                 loss: nan
env3_first_0:                 episode reward: 0.7500,                 loss: nan
env3_second_0:                 episode reward: -0.7500,                 loss: nan
env4_first_0:                 episode reward: 1.4000,                 loss: nan
env4_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.0896s / 29906.5201 s
env0_first_0:                 episode reward: 1.0000,                 loss: nan
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0098
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
env2_first_0:                 episode reward: 1.0000,                 loss: nan
env2_second_0:                 episode reward: -1.0000,                 loss: nan
env3_first_0:                 episode reward: 1.0000,                 loss: nan
env3_second_0:                 episode reward: -1.0000,                 loss: nan
env4_first_0:                 episode reward: 1.0000,                 loss: nan
env4_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.0731s / 30101.5931 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0097
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
env2_first_0:                 episode reward: 3.4500,                 loss: nan
env2_second_0:                 episode reward: -3.4500,                 loss: nan
env3_first_0:                 episode reward: 3.4500,                 loss: nan
env3_second_0:                 episode reward: -3.4500,                 loss: nan
env4_first_0:                 episode reward: 3.4500,                 loss: nan
env4_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.6338s / 30296.2270 s
env0_first_0:                 episode reward: -1.3000,                 loss: nan
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0097
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
env2_first_0:                 episode reward: -0.7000,                 loss: nan
env2_second_0:                 episode reward: 0.7000,                 loss: nan
env3_first_0:                 episode reward: -0.6500,                 loss: nan
env3_second_0:                 episode reward: 0.6500,                 loss: nan
env4_first_0:                 episode reward: -0.6500,                 loss: nan
env4_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.7308s / 30489.9577 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0257
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0096
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
env2_first_0:                 episode reward: -3.3000,                 loss: nan
env2_second_0:                 episode reward: 3.3000,                 loss: nan
env3_first_0:                 episode reward: -3.3000,                 loss: nan
env3_second_0:                 episode reward: 3.3000,                 loss: nan
env4_first_0:                 episode reward: -3.3000,                 loss: nan
env4_second_0:                 episode reward: 3.3000,                 loss: nan
Score delta: 11.6, update the opponent.
Episode: 3381/10000 (33.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.4400s / 30684.3977 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0111
env0_second_0:                 episode reward: 6.0000,                 loss: nan
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
env2_first_0:                 episode reward: -6.0000,                 loss: nan
env2_second_0:                 episode reward: 6.0000,                 loss: nan
env3_first_0:                 episode reward: -6.0000,                 loss: nan
env3_second_0:                 episode reward: 6.0000,                 loss: nan
env4_first_0:                 episode reward: -6.0000,                 loss: nan
env4_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.9379s / 30879.3356 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0117
env0_second_0:                 episode reward: 5.3000,                 loss: nan
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
env2_first_0:                 episode reward: -5.3000,                 loss: nan
env2_second_0:                 episode reward: 5.3000,                 loss: nan
env3_first_0:                 episode reward: -5.3000,                 loss: nan
env3_second_0:                 episode reward: 5.3000,                 loss: nan
env4_first_0:                 episode reward: -5.3000,                 loss: nan
env4_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.9034s / 31073.2390 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0164
env0_second_0:                 episode reward: 6.5000,                 loss: nan
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
env2_first_0:                 episode reward: -6.5000,                 loss: nan
env2_second_0:                 episode reward: 6.5000,                 loss: nan
env3_first_0:                 episode reward: -6.5000,                 loss: nan
env3_second_0:                 episode reward: 6.5000,                 loss: nan
env4_first_0:                 episode reward: -6.5000,                 loss: nan
env4_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.0632s / 31267.3022 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0200
env0_second_0:                 episode reward: 6.2500,                 loss: nan
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
env2_first_0:                 episode reward: -6.2500,                 loss: nan
env2_second_0:                 episode reward: 6.2500,                 loss: nan
env3_first_0:                 episode reward: -6.2500,                 loss: nan
env3_second_0:                 episode reward: 6.2500,                 loss: nan
env4_first_0:                 episode reward: -6.2500,                 loss: nan
env4_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.9828s / 31462.2850 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0204
env0_second_0:                 episode reward: 6.2000,                 loss: nan
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
env2_first_0:                 episode reward: -5.7500,                 loss: nan
env2_second_0:                 episode reward: 5.7500,                 loss: nan
env3_first_0:                 episode reward: -5.7500,                 loss: nan
env3_second_0:                 episode reward: 5.7500,                 loss: nan
env4_first_0:                 episode reward: -6.2000,                 loss: nan
env4_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.2147s / 31656.4998 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0289
env0_second_0:                 episode reward: 5.6500,                 loss: nan
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
env2_first_0:                 episode reward: -5.6500,                 loss: nan
env2_second_0:                 episode reward: 5.6500,                 loss: nan
env3_first_0:                 episode reward: -5.5000,                 loss: nan
env3_second_0:                 episode reward: 5.5000,                 loss: nan
env4_first_0:                 episode reward: -5.6500,                 loss: nan
env4_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.3652s / 31850.8650 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0384
env0_second_0:                 episode reward: 6.5000,                 loss: nan
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
env2_first_0:                 episode reward: -6.5000,                 loss: nan
env2_second_0:                 episode reward: 6.5000,                 loss: nan
env3_first_0:                 episode reward: -6.5000,                 loss: nan
env3_second_0:                 episode reward: 6.5000,                 loss: nan
env4_first_0:                 episode reward: -6.5000,                 loss: nan
env4_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.3788s / 32045.2438 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0232
env0_second_0:                 episode reward: 6.0000,                 loss: nan
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
env2_first_0:                 episode reward: -6.0000,                 loss: nan
env2_second_0:                 episode reward: 6.0000,                 loss: nan
env3_first_0:                 episode reward: -6.0000,                 loss: nan
env3_second_0:                 episode reward: 6.0000,                 loss: nan
env4_first_0:                 episode reward: -6.0000,                 loss: nan
env4_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.8227s / 32239.0665 s
env0_first_0:                 episode reward: -6.9000,                 loss: 0.0159
env0_second_0:                 episode reward: 6.9000,                 loss: nan
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
env2_first_0:                 episode reward: -6.8500,                 loss: nan
env2_second_0:                 episode reward: 6.8500,                 loss: nan
env3_first_0:                 episode reward: -6.9000,                 loss: nan
env3_second_0:                 episode reward: 6.9000,                 loss: nan
env4_first_0:                 episode reward: -6.9000,                 loss: nan
env4_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.3023s / 32432.3688 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0134
env0_second_0:                 episode reward: 7.0000,                 loss: nan
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
env2_first_0:                 episode reward: -7.0000,                 loss: nan
env2_second_0:                 episode reward: 7.0000,                 loss: nan
env3_first_0:                 episode reward: -7.0000,                 loss: nan
env3_second_0:                 episode reward: 7.0000,                 loss: nan
env4_first_0:                 episode reward: -7.0000,                 loss: nan
env4_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.8392s / 32626.2079 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0121
env0_second_0:                 episode reward: 5.6000,                 loss: nan
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
env2_first_0:                 episode reward: -5.6000,                 loss: nan
env2_second_0:                 episode reward: 5.6000,                 loss: nan
env3_first_0:                 episode reward: -5.7000,                 loss: nan
env3_second_0:                 episode reward: 5.7000,                 loss: nan
env4_first_0:                 episode reward: -5.7000,                 loss: nan
env4_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.6701s / 32820.8780 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0123
env0_second_0:                 episode reward: 5.8000,                 loss: nan
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
env2_first_0:                 episode reward: -5.8000,                 loss: nan
env2_second_0:                 episode reward: 5.8000,                 loss: nan
env3_first_0:                 episode reward: -5.8000,                 loss: nan
env3_second_0:                 episode reward: 5.8000,                 loss: nan
env4_first_0:                 episode reward: -5.8000,                 loss: nan
env4_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.3234s / 33015.2014 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0134
env0_second_0:                 episode reward: 6.0000,                 loss: nan
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
env2_first_0:                 episode reward: -6.0000,                 loss: nan
env2_second_0:                 episode reward: 6.0000,                 loss: nan
env3_first_0:                 episode reward: -6.0000,                 loss: nan
env3_second_0:                 episode reward: 6.0000,                 loss: nan
env4_first_0:                 episode reward: -6.0500,                 loss: nan
env4_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.2922s / 33209.4936 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0128
env0_second_0:                 episode reward: 5.4000,                 loss: nan
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
env2_first_0:                 episode reward: -5.4000,                 loss: nan
env2_second_0:                 episode reward: 5.4000,                 loss: nan
env3_first_0:                 episode reward: -5.4000,                 loss: nan
env3_second_0:                 episode reward: 5.4000,                 loss: nan
env4_first_0:                 episode reward: -5.6000,                 loss: nan
env4_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.6262s / 33404.1198 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0127
env0_second_0:                 episode reward: 3.8500,                 loss: nan
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
env2_first_0:                 episode reward: -3.8500,                 loss: nan
env2_second_0:                 episode reward: 3.8500,                 loss: nan
env3_first_0:                 episode reward: -3.8500,                 loss: nan
env3_second_0:                 episode reward: 3.8500,                 loss: nan
env4_first_0:                 episode reward: -3.8500,                 loss: nan
env4_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.9495s / 33599.0693 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0126
env0_second_0:                 episode reward: 4.2500,                 loss: nan
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
env2_first_0:                 episode reward: -4.2500,                 loss: nan
env2_second_0:                 episode reward: 4.2500,                 loss: nan
env3_first_0:                 episode reward: -4.2500,                 loss: nan
env3_second_0:                 episode reward: 4.2500,                 loss: nan
env4_first_0:                 episode reward: -4.2500,                 loss: nan
env4_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.6586s / 33793.7279 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0135
env0_second_0:                 episode reward: 5.8000,                 loss: nan
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
env2_first_0:                 episode reward: -5.8000,                 loss: nan
env2_second_0:                 episode reward: 5.8000,                 loss: nan
env3_first_0:                 episode reward: -5.8000,                 loss: nan
env3_second_0:                 episode reward: 5.8000,                 loss: nan
env4_first_0:                 episode reward: -5.8000,                 loss: nan
env4_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.4180s / 33988.1459 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0139
env0_second_0:                 episode reward: 5.0500,                 loss: nan
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
env2_first_0:                 episode reward: -5.0500,                 loss: nan
env2_second_0:                 episode reward: 5.0500,                 loss: nan
env3_first_0:                 episode reward: -5.0500,                 loss: nan
env3_second_0:                 episode reward: 5.0500,                 loss: nan
env4_first_0:                 episode reward: -5.0500,                 loss: nan
env4_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.4805s / 34182.6264 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0141
env0_second_0:                 episode reward: 6.6000,                 loss: nan
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
env2_first_0:                 episode reward: -6.6000,                 loss: nan
env2_second_0:                 episode reward: 6.6000,                 loss: nan
env3_first_0:                 episode reward: -6.6000,                 loss: nan
env3_second_0:                 episode reward: 6.6000,                 loss: nan
env4_first_0:                 episode reward: -6.6000,                 loss: nan
env4_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.8045s / 34377.4308 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0143
env0_second_0:                 episode reward: 5.4500,                 loss: nan
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
env2_first_0:                 episode reward: -5.4500,                 loss: nan
env2_second_0:                 episode reward: 5.4500,                 loss: nan
env3_first_0:                 episode reward: -5.4500,                 loss: nan
env3_second_0:                 episode reward: 5.4500,                 loss: nan
env4_first_0:                 episode reward: -5.4500,                 loss: nan
env4_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.0053s / 34572.4361 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0136
env0_second_0:                 episode reward: 5.8500,                 loss: nan
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
env2_first_0:                 episode reward: -5.8500,                 loss: nan
env2_second_0:                 episode reward: 5.8500,                 loss: nan
env3_first_0:                 episode reward: -6.3000,                 loss: nan
env3_second_0:                 episode reward: 6.3000,                 loss: nan
env4_first_0:                 episode reward: -6.3000,                 loss: nan
env4_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.7707s / 34766.2068 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0131
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
env2_first_0:                 episode reward: -1.8500,                 loss: nan
env2_second_0:                 episode reward: 1.8500,                 loss: nan
env3_first_0:                 episode reward: -2.1500,                 loss: nan
env3_second_0:                 episode reward: 2.1500,                 loss: nan
env4_first_0:                 episode reward: -1.8500,                 loss: nan
env4_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.6654s / 34959.8722 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0119
env0_second_0:                 episode reward: -2.6500,                 loss: nan
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
env2_first_0:                 episode reward: 2.6500,                 loss: nan
env2_second_0:                 episode reward: -2.6500,                 loss: nan
env3_first_0:                 episode reward: 2.6500,                 loss: nan
env3_second_0:                 episode reward: -2.6500,                 loss: nan
env4_first_0:                 episode reward: 2.6500,                 loss: nan
env4_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.5054s / 35154.3776 s
env0_first_0:                 episode reward: 6.6000,                 loss: 0.0112
env0_second_0:                 episode reward: -6.6000,                 loss: 0.0270
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
env2_first_0:                 episode reward: 6.6000,                 loss: nan
env2_second_0:                 episode reward: -6.6000,                 loss: nan
env3_first_0:                 episode reward: 6.5000,                 loss: nan
env3_second_0:                 episode reward: -6.5000,                 loss: nan
env4_first_0:                 episode reward: 6.6000,                 loss: nan
env4_second_0:                 episode reward: -6.6000,                 loss: nan
Score delta: 13.0, update the opponent.
Episode: 3861/10000 (38.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.0821s / 35349.4596 s
env0_first_0:                 episode reward: 7.3500,                 loss: nan
env0_second_0:                 episode reward: -7.3500,                 loss: 0.0130
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
env2_first_0:                 episode reward: 7.3500,                 loss: nan
env2_second_0:                 episode reward: -7.3500,                 loss: nan
env3_first_0:                 episode reward: 7.3500,                 loss: nan
env3_second_0:                 episode reward: -7.3500,                 loss: nan
env4_first_0:                 episode reward: 7.3500,                 loss: nan
env4_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.7622s / 35544.2218 s
env0_first_0:                 episode reward: 6.6500,                 loss: nan
env0_second_0:                 episode reward: -6.6500,                 loss: 0.0103
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
env2_first_0:                 episode reward: 6.6500,                 loss: nan
env2_second_0:                 episode reward: -6.6500,                 loss: nan
env3_first_0:                 episode reward: 6.6500,                 loss: nan
env3_second_0:                 episode reward: -6.6500,                 loss: nan
env4_first_0:                 episode reward: 6.6500,                 loss: nan
env4_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.4050s / 35739.6268 s
env0_first_0:                 episode reward: -0.2500,                 loss: nan
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0100
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
env2_first_0:                 episode reward: -0.8000,                 loss: nan
env2_second_0:                 episode reward: 0.8000,                 loss: nan
env3_first_0:                 episode reward: -0.2500,                 loss: nan
env3_second_0:                 episode reward: 0.2500,                 loss: nan
env4_first_0:                 episode reward: -0.5500,                 loss: nan
env4_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.0408s / 35934.6676 s
env0_first_0:                 episode reward: -3.6500,                 loss: nan
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0121
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
env2_first_0:                 episode reward: -3.6500,                 loss: nan
env2_second_0:                 episode reward: 3.6500,                 loss: nan
env3_first_0:                 episode reward: -3.6500,                 loss: nan
env3_second_0:                 episode reward: 3.6500,                 loss: nan
env4_first_0:                 episode reward: -3.6500,                 loss: nan
env4_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.6608s / 36130.3284 s
env0_first_0:                 episode reward: 4.1500,                 loss: nan
env0_second_0:                 episode reward: -4.1500,                 loss: 0.0124
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
env2_first_0:                 episode reward: 4.1500,                 loss: nan
env2_second_0:                 episode reward: -4.1500,                 loss: nan
env3_first_0:                 episode reward: 4.1500,                 loss: nan
env3_second_0:                 episode reward: -4.1500,                 loss: nan
env4_first_0:                 episode reward: 4.1500,                 loss: nan
env4_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.1896s / 36325.5180 s
env0_first_0:                 episode reward: 7.2000,                 loss: nan
env0_second_0:                 episode reward: -7.2000,                 loss: 0.0122
env1_first_0:                 episode reward: 7.5500,                 loss: nan
env1_second_0:                 episode reward: -7.5500,                 loss: nan
env2_first_0:                 episode reward: 7.3500,                 loss: nan
env2_second_0:                 episode reward: -7.3500,                 loss: nan
env3_first_0:                 episode reward: 7.7000,                 loss: nan
env3_second_0:                 episode reward: -7.7000,                 loss: nan
env4_first_0:                 episode reward: 7.2000,                 loss: nan
env4_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.7427s / 36521.2607 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0130
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.1154s / 36716.3761 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0117
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.5070s / 36911.8831 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0102
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
env2_first_0:                 episode reward: 3.6000,                 loss: nan
env2_second_0:                 episode reward: -3.6000,                 loss: nan
env3_first_0:                 episode reward: 3.6000,                 loss: nan
env3_second_0:                 episode reward: -3.6000,                 loss: nan
env4_first_0:                 episode reward: 3.6000,                 loss: nan
env4_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.3006s / 37106.1837 s
env0_first_0:                 episode reward: -2.0000,                 loss: nan
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0104
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
env2_first_0:                 episode reward: -2.0000,                 loss: nan
env2_second_0:                 episode reward: 2.0000,                 loss: nan
env3_first_0:                 episode reward: -2.0500,                 loss: nan
env3_second_0:                 episode reward: 2.0500,                 loss: nan
env4_first_0:                 episode reward: -2.0500,                 loss: nan
env4_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.9859s / 37301.1696 s
env0_first_0:                 episode reward: -3.9500,                 loss: nan
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0106
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
env2_first_0:                 episode reward: -4.0500,                 loss: nan
env2_second_0:                 episode reward: 4.0500,                 loss: nan
env3_first_0:                 episode reward: -4.0000,                 loss: nan
env3_second_0:                 episode reward: 4.0000,                 loss: nan
env4_first_0:                 episode reward: -4.0000,                 loss: nan
env4_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 196.0050s / 37497.1746 s
env0_first_0:                 episode reward: 5.9500,                 loss: nan
env0_second_0:                 episode reward: -5.9500,                 loss: 0.0116
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
env2_first_0:                 episode reward: 5.9500,                 loss: nan
env2_second_0:                 episode reward: -5.9500,                 loss: nan
env3_first_0:                 episode reward: 5.9500,                 loss: nan
env3_second_0:                 episode reward: -5.9500,                 loss: nan
env4_first_0:                 episode reward: 5.9500,                 loss: nan
env4_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.9827s / 37692.1573 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0109
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.3474s / 37886.5047 s
env0_first_0:                 episode reward: 6.2500,                 loss: nan
env0_second_0:                 episode reward: -6.2500,                 loss: 0.0094
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
env2_first_0:                 episode reward: 6.2500,                 loss: nan
env2_second_0:                 episode reward: -6.2500,                 loss: nan
env3_first_0:                 episode reward: 6.2500,                 loss: nan
env3_second_0:                 episode reward: -6.2500,                 loss: nan
env4_first_0:                 episode reward: 6.2500,                 loss: nan
env4_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.9731s / 38082.4778 s
env0_first_0:                 episode reward: 4.5000,                 loss: nan
env0_second_0:                 episode reward: -4.5000,                 loss: 0.0082
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
env2_first_0:                 episode reward: 4.5000,                 loss: nan
env2_second_0:                 episode reward: -4.5000,                 loss: nan
env3_first_0:                 episode reward: 4.5000,                 loss: nan
env3_second_0:                 episode reward: -4.5000,                 loss: nan
env4_first_0:                 episode reward: 4.5000,                 loss: nan
env4_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.3549s / 38277.8327 s
env0_first_0:                 episode reward: 6.6500,                 loss: nan
env0_second_0:                 episode reward: -6.6500,                 loss: 0.0083
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
env2_first_0:                 episode reward: 6.6500,                 loss: nan
env2_second_0:                 episode reward: -6.6500,                 loss: nan
env3_first_0:                 episode reward: 6.6500,                 loss: nan
env3_second_0:                 episode reward: -6.6500,                 loss: nan
env4_first_0:                 episode reward: 6.6500,                 loss: nan
env4_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 196.0716s / 38473.9043 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0117
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.0000,                 loss: nan
env3_second_0:                 episode reward: -8.0000,                 loss: nan
env4_first_0:                 episode reward: 8.0000,                 loss: nan
env4_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.8388s / 38669.7432 s
env0_first_0:                 episode reward: 7.5000,                 loss: nan
env0_second_0:                 episode reward: -7.5000,                 loss: 0.0121
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
env2_first_0:                 episode reward: 7.5000,                 loss: nan
env2_second_0:                 episode reward: -7.5000,                 loss: nan
env3_first_0:                 episode reward: 7.5000,                 loss: nan
env3_second_0:                 episode reward: -7.5000,                 loss: nan
env4_first_0:                 episode reward: 7.5000,                 loss: nan
env4_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 196.0557s / 38865.7989 s
env0_first_0:                 episode reward: 7.0500,                 loss: nan
env0_second_0:                 episode reward: -7.0500,                 loss: 0.0106
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
env2_first_0:                 episode reward: 7.6000,                 loss: nan
env2_second_0:                 episode reward: -7.6000,                 loss: nan
env3_first_0:                 episode reward: 7.0500,                 loss: nan
env3_second_0:                 episode reward: -7.0500,                 loss: nan
env4_first_0:                 episode reward: 7.0500,                 loss: nan
env4_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.4652s / 39061.2641 s
env0_first_0:                 episode reward: 7.4500,                 loss: nan
env0_second_0:                 episode reward: -7.4500,                 loss: 0.0112
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
env2_first_0:                 episode reward: 7.4500,                 loss: nan
env2_second_0:                 episode reward: -7.4500,                 loss: nan
env3_first_0:                 episode reward: 7.4500,                 loss: nan
env3_second_0:                 episode reward: -7.4500,                 loss: nan
env4_first_0:                 episode reward: 7.4500,                 loss: nan
env4_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.9312s / 39256.1953 s
env0_first_0:                 episode reward: 5.3500,                 loss: nan
env0_second_0:                 episode reward: -5.3500,                 loss: 0.0119
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
env2_first_0:                 episode reward: 5.3500,                 loss: nan
env2_second_0:                 episode reward: -5.3500,                 loss: nan
env3_first_0:                 episode reward: 5.3500,                 loss: nan
env3_second_0:                 episode reward: -5.3500,                 loss: nan
env4_first_0:                 episode reward: 5.3500,                 loss: nan
env4_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.9325s / 39451.1277 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0117
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
env2_first_0:                 episode reward: 2.4500,                 loss: nan
env2_second_0:                 episode reward: -2.4500,                 loss: nan
env3_first_0:                 episode reward: 1.9000,                 loss: nan
env3_second_0:                 episode reward: -1.9000,                 loss: nan
env4_first_0:                 episode reward: 2.4500,                 loss: nan
env4_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.1488s / 39646.2765 s
env0_first_0:                 episode reward: -2.8500,                 loss: nan
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0089
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
env2_first_0:                 episode reward: -2.8500,                 loss: nan
env2_second_0:                 episode reward: 2.8500,                 loss: nan
env3_first_0:                 episode reward: -2.8500,                 loss: nan
env3_second_0:                 episode reward: 2.8500,                 loss: nan
env4_first_0:                 episode reward: -2.8500,                 loss: nan
env4_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.3148s / 39841.5913 s
env0_first_0:                 episode reward: -1.8500,                 loss: nan
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0093
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
env2_first_0:                 episode reward: -1.8500,                 loss: nan
env2_second_0:                 episode reward: 1.8500,                 loss: nan
env3_first_0:                 episode reward: -1.8500,                 loss: nan
env3_second_0:                 episode reward: 1.8500,                 loss: nan
env4_first_0:                 episode reward: -1.8500,                 loss: nan
env4_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.9291s / 40037.5205 s
env0_first_0:                 episode reward: -3.9500,                 loss: nan
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0100
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
env2_first_0:                 episode reward: -3.9500,                 loss: nan
env2_second_0:                 episode reward: 3.9500,                 loss: nan
env3_first_0:                 episode reward: -3.9500,                 loss: nan
env3_second_0:                 episode reward: 3.9500,                 loss: nan
env4_first_0:                 episode reward: -3.9500,                 loss: nan
env4_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.7455s / 40232.2660 s
env0_first_0:                 episode reward: -3.7500,                 loss: nan
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0105
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
env2_first_0:                 episode reward: -3.7500,                 loss: nan
env2_second_0:                 episode reward: 3.7500,                 loss: nan
env3_first_0:                 episode reward: -3.8500,                 loss: nan
env3_second_0:                 episode reward: 3.8500,                 loss: nan
env4_first_0:                 episode reward: -3.7500,                 loss: nan
env4_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.7071s / 40426.9731 s
env0_first_0:                 episode reward: -3.8000,                 loss: nan
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0105
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
env2_first_0:                 episode reward: -3.8000,                 loss: nan
env2_second_0:                 episode reward: 3.8000,                 loss: nan
env3_first_0:                 episode reward: -3.8000,                 loss: nan
env3_second_0:                 episode reward: 3.8000,                 loss: nan
env4_first_0:                 episode reward: -3.8000,                 loss: nan
env4_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.2352s / 40622.2083 s
env0_first_0:                 episode reward: -2.6500,                 loss: nan
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0103
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
env2_first_0:                 episode reward: -2.6500,                 loss: nan
env2_second_0:                 episode reward: 2.6500,                 loss: nan
env3_first_0:                 episode reward: -2.6500,                 loss: nan
env3_second_0:                 episode reward: 2.6500,                 loss: nan
env4_first_0:                 episode reward: -2.6500,                 loss: nan
env4_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 195.3020s / 40817.5103 s
env0_first_0:                 episode reward: -4.6000,                 loss: nan
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0093
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
env2_first_0:                 episode reward: -4.6000,                 loss: nan
env2_second_0:                 episode reward: 4.6000,                 loss: nan
env3_first_0:                 episode reward: -4.6000,                 loss: nan
env3_second_0:                 episode reward: 4.6000,                 loss: nan
env4_first_0:                 episode reward: -4.6000,                 loss: nan
env4_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.9681s / 41010.4784 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0300
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0089
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
env2_first_0:                 episode reward: 2.1500,                 loss: nan
env2_second_0:                 episode reward: -2.1500,                 loss: nan
env3_first_0:                 episode reward: 2.0500,                 loss: nan
env3_second_0:                 episode reward: -2.0500,                 loss: nan
env4_first_0:                 episode reward: 2.0500,                 loss: nan
env4_second_0:                 episode reward: -2.0500,                 loss: nan
Score delta: 10.4, update the opponent.
Episode: 4461/10000 (44.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.9152s / 41204.3936 s
env0_first_0:                 episode reward: 6.0500,                 loss: 0.0124
env0_second_0:                 episode reward: -6.0500,                 loss: 0.0254
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
env2_first_0:                 episode reward: 6.2500,                 loss: nan
env2_second_0:                 episode reward: -6.2500,                 loss: nan
env3_first_0:                 episode reward: 6.0500,                 loss: nan
env3_second_0:                 episode reward: -6.0500,                 loss: nan
env4_first_0:                 episode reward: 5.9000,                 loss: nan
env4_second_0:                 episode reward: -5.9000,                 loss: nan
Score delta: 10.2, update the opponent.
Episode: 4481/10000 (44.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 193.9089s / 41398.3026 s
env0_first_0:                 episode reward: 1.4500,                 loss: nan
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0122
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
env2_first_0:                 episode reward: 1.4500,                 loss: nan
env2_second_0:                 episode reward: -1.4500,                 loss: nan
env3_first_0:                 episode reward: 1.4500,                 loss: nan
env3_second_0:                 episode reward: -1.4500,                 loss: nan
env4_first_0:                 episode reward: 1.4500,                 loss: nan
env4_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.5306s / 41592.8332 s
env0_first_0:                 episode reward: -0.9000,                 loss: nan
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0117
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
env2_first_0:                 episode reward: -0.9000,                 loss: nan
env2_second_0:                 episode reward: 0.9000,                 loss: nan
env3_first_0:                 episode reward: -0.9000,                 loss: nan
env3_second_0:                 episode reward: 0.9000,                 loss: nan
env4_first_0:                 episode reward: -0.9000,                 loss: nan
env4_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.6396s / 41787.4728 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0427
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0109
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
env2_first_0:                 episode reward: -5.4000,                 loss: nan
env2_second_0:                 episode reward: 5.4000,                 loss: nan
env3_first_0:                 episode reward: -5.4500,                 loss: nan
env3_second_0:                 episode reward: 5.4500,                 loss: nan
env4_first_0:                 episode reward: -5.4500,                 loss: nan
env4_second_0:                 episode reward: 5.4500,                 loss: nan
Score delta: 10.4, update the opponent.
Episode: 4541/10000 (45.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 194.0847s / 41981.5575 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0222
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0360
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
env2_first_0:                 episode reward: 1.1500,                 loss: nan
env2_second_0:                 episode reward: -1.1500,                 loss: nan
env3_first_0:                 episode reward: 0.5500,                 loss: nan
env3_second_0:                 episode reward: -0.5500,                 loss: nan
env4_first_0:                 episode reward: 0.9000,                 loss: nan
env4_second_0:                 episode reward: -0.9000,                 loss: nan
Score delta: 10.4, update the opponent.
Episode: 4561/10000 (45.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 192.8540s / 42174.4115 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0186
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 191.6781s / 42366.0895 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0147
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 189.0537s / 42555.1432 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0143
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 179.9808s / 42735.1240 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0136
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 178.2618s / 42913.3859 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0124
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 172.1911s / 43085.5770 s
env0_first_0:                 episode reward: 1.3000,                 loss: nan
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0107
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
env2_first_0:                 episode reward: 1.3000,                 loss: nan
env2_second_0:                 episode reward: -1.3000,                 loss: nan
env3_first_0:                 episode reward: 1.3000,                 loss: nan
env3_second_0:                 episode reward: -1.3000,                 loss: nan
env4_first_0:                 episode reward: 1.3000,                 loss: nan
env4_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 173.1705s / 43258.7475 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0105
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
env2_first_0:                 episode reward: 0.2000,                 loss: nan
env2_second_0:                 episode reward: -0.2000,                 loss: nan
env3_first_0:                 episode reward: 0.2000,                 loss: nan
env3_second_0:                 episode reward: -0.2000,                 loss: nan
env4_first_0:                 episode reward: 0.2000,                 loss: nan
env4_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.2980s / 43425.0455 s
env0_first_0:                 episode reward: 0.0500,                 loss: nan
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0103
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
env2_first_0:                 episode reward: 0.0500,                 loss: nan
env2_second_0:                 episode reward: -0.0500,                 loss: nan
env3_first_0:                 episode reward: 0.0500,                 loss: nan
env3_second_0:                 episode reward: -0.0500,                 loss: nan
env4_first_0:                 episode reward: 0.0500,                 loss: nan
env4_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.5276s / 43590.5731 s
env0_first_0:                 episode reward: 1.0000,                 loss: nan
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0113
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
env2_first_0:                 episode reward: 1.0000,                 loss: nan
env2_second_0:                 episode reward: -1.0000,                 loss: nan
env3_first_0:                 episode reward: 1.0000,                 loss: nan
env3_second_0:                 episode reward: -1.0000,                 loss: nan
env4_first_0:                 episode reward: 1.0000,                 loss: nan
env4_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.1904s / 43754.7635 s
env0_first_0:                 episode reward: 0.3500,                 loss: nan
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0124
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
env2_first_0:                 episode reward: 0.3500,                 loss: nan
env2_second_0:                 episode reward: -0.3500,                 loss: nan
env3_first_0:                 episode reward: 0.3500,                 loss: nan
env3_second_0:                 episode reward: -0.3500,                 loss: nan
env4_first_0:                 episode reward: 0.3500,                 loss: nan
env4_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.1733s / 43919.9368 s
env0_first_0:                 episode reward: 1.0000,                 loss: nan
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0128
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
env2_first_0:                 episode reward: 1.0000,                 loss: nan
env2_second_0:                 episode reward: -1.0000,                 loss: nan
env3_first_0:                 episode reward: 1.4000,                 loss: nan
env3_second_0:                 episode reward: -1.4000,                 loss: nan
env4_first_0:                 episode reward: 1.4000,                 loss: nan
env4_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.3902s / 44085.3269 s
env0_first_0:                 episode reward: 1.6000,                 loss: nan
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0129
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
env2_first_0:                 episode reward: 1.6000,                 loss: nan
env2_second_0:                 episode reward: -1.6000,                 loss: nan
env3_first_0:                 episode reward: 1.6000,                 loss: nan
env3_second_0:                 episode reward: -1.6000,                 loss: nan
env4_first_0:                 episode reward: 1.6000,                 loss: nan
env4_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.1701s / 44250.4971 s
env0_first_0:                 episode reward: 1.1500,                 loss: nan
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0137
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
env2_first_0:                 episode reward: 1.1500,                 loss: nan
env2_second_0:                 episode reward: -1.1500,                 loss: nan
env3_first_0:                 episode reward: 1.1500,                 loss: nan
env3_second_0:                 episode reward: -1.1500,                 loss: nan
env4_first_0:                 episode reward: 1.1500,                 loss: nan
env4_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.3345s / 44415.8316 s
env0_first_0:                 episode reward: 1.4500,                 loss: nan
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0141
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
env2_first_0:                 episode reward: 1.4500,                 loss: nan
env2_second_0:                 episode reward: -1.4500,                 loss: nan
env3_first_0:                 episode reward: 1.4500,                 loss: nan
env3_second_0:                 episode reward: -1.4500,                 loss: nan
env4_first_0:                 episode reward: 1.4500,                 loss: nan
env4_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.1044s / 44578.9360 s
env0_first_0:                 episode reward: 1.2000,                 loss: nan
env0_second_0:                 episode reward: -1.2000,                 loss: 0.0149
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
env2_first_0:                 episode reward: 1.2000,                 loss: nan
env2_second_0:                 episode reward: -1.2000,                 loss: nan
env3_first_0:                 episode reward: 1.2000,                 loss: nan
env3_second_0:                 episode reward: -1.2000,                 loss: nan
env4_first_0:                 episode reward: 1.2000,                 loss: nan
env4_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.3353s / 44744.2713 s
env0_first_0:                 episode reward: 1.2000,                 loss: nan
env0_second_0:                 episode reward: -1.2000,                 loss: 0.0160
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
env2_first_0:                 episode reward: 1.2000,                 loss: nan
env2_second_0:                 episode reward: -1.2000,                 loss: nan
env3_first_0:                 episode reward: 1.2000,                 loss: nan
env3_second_0:                 episode reward: -1.2000,                 loss: nan
env4_first_0:                 episode reward: 1.2000,                 loss: nan
env4_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.4150s / 44909.6863 s
env0_first_0:                 episode reward: 1.4000,                 loss: nan
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0184
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
env2_first_0:                 episode reward: 1.4000,                 loss: nan
env2_second_0:                 episode reward: -1.4000,                 loss: nan
env3_first_0:                 episode reward: 1.4000,                 loss: nan
env3_second_0:                 episode reward: -1.4000,                 loss: nan
env4_first_0:                 episode reward: 1.4000,                 loss: nan
env4_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.9425s / 45076.6289 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0182
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.7895s / 45242.4183 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0174
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.1316s / 45407.5499 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0171
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.3926s / 45571.9425 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0187
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.0273s / 45737.9698 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0202
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.0943s / 45903.0641 s
env0_first_0:                 episode reward: 1.5500,                 loss: nan
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0236
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
env2_first_0:                 episode reward: 1.5500,                 loss: nan
env2_second_0:                 episode reward: -1.5500,                 loss: nan
env3_first_0:                 episode reward: 1.5500,                 loss: nan
env3_second_0:                 episode reward: -1.5500,                 loss: nan
env4_first_0:                 episode reward: 1.5500,                 loss: nan
env4_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.8078s / 46065.8719 s
env0_first_0:                 episode reward: 1.0500,                 loss: nan
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0284
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
env2_first_0:                 episode reward: 1.4000,                 loss: nan
env2_second_0:                 episode reward: -1.4000,                 loss: nan
env3_first_0:                 episode reward: 1.0500,                 loss: nan
env3_second_0:                 episode reward: -1.0500,                 loss: nan
env4_first_0:                 episode reward: 1.4000,                 loss: nan
env4_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.0957s / 46229.9675 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0305
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.9319s / 46392.8994 s
env0_first_0:                 episode reward: 1.7000,                 loss: nan
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0288
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
env2_first_0:                 episode reward: 1.7000,                 loss: nan
env2_second_0:                 episode reward: -1.7000,                 loss: nan
env3_first_0:                 episode reward: 1.7000,                 loss: nan
env3_second_0:                 episode reward: -1.7000,                 loss: nan
env4_first_0:                 episode reward: 1.7000,                 loss: nan
env4_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.5024s / 46556.4018 s
env0_first_0:                 episode reward: 1.5500,                 loss: nan
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0245
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
env2_first_0:                 episode reward: 1.5500,                 loss: nan
env2_second_0:                 episode reward: -1.5500,                 loss: nan
env3_first_0:                 episode reward: 1.6000,                 loss: nan
env3_second_0:                 episode reward: -1.6000,                 loss: nan
env4_first_0:                 episode reward: 1.6000,                 loss: nan
env4_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.6742s / 46719.0761 s
env0_first_0:                 episode reward: 1.7500,                 loss: nan
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0187
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
env2_first_0:                 episode reward: 1.8000,                 loss: nan
env2_second_0:                 episode reward: -1.8000,                 loss: nan
env3_first_0:                 episode reward: 1.8000,                 loss: nan
env3_second_0:                 episode reward: -1.8000,                 loss: nan
env4_first_0:                 episode reward: 1.7500,                 loss: nan
env4_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.6602s / 46883.7362 s
env0_first_0:                 episode reward: -1.0000,                 loss: nan
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0156
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
env2_first_0:                 episode reward: -1.0000,                 loss: nan
env2_second_0:                 episode reward: 1.0000,                 loss: nan
env3_first_0:                 episode reward: -1.0000,                 loss: nan
env3_second_0:                 episode reward: 1.0000,                 loss: nan
env4_first_0:                 episode reward: -1.0000,                 loss: nan
env4_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.6597s / 47047.3960 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0160
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
env2_first_0:                 episode reward: 2.8000,                 loss: nan
env2_second_0:                 episode reward: -2.8000,                 loss: nan
env3_first_0:                 episode reward: 2.8000,                 loss: nan
env3_second_0:                 episode reward: -2.8000,                 loss: nan
env4_first_0:                 episode reward: 2.8000,                 loss: nan
env4_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.4311s / 47210.8271 s
env0_first_0:                 episode reward: 2.1000,                 loss: nan
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0157
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
env2_first_0:                 episode reward: 2.1000,                 loss: nan
env2_second_0:                 episode reward: -2.1000,                 loss: nan
env3_first_0:                 episode reward: 2.1000,                 loss: nan
env3_second_0:                 episode reward: -2.1000,                 loss: nan
env4_first_0:                 episode reward: 2.1000,                 loss: nan
env4_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.7557s / 47374.5828 s
env0_first_0:                 episode reward: 2.2500,                 loss: nan
env0_second_0:                 episode reward: -2.2500,                 loss: 0.0144
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
env2_first_0:                 episode reward: 2.2500,                 loss: nan
env2_second_0:                 episode reward: -2.2500,                 loss: nan
env3_first_0:                 episode reward: 2.2500,                 loss: nan
env3_second_0:                 episode reward: -2.2500,                 loss: nan
env4_first_0:                 episode reward: 2.2500,                 loss: nan
env4_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.1618s / 47538.7445 s
env0_first_0:                 episode reward: 2.1000,                 loss: nan
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0143
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
env2_first_0:                 episode reward: 2.1000,                 loss: nan
env2_second_0:                 episode reward: -2.1000,                 loss: nan
env3_first_0:                 episode reward: 2.1000,                 loss: nan
env3_second_0:                 episode reward: -2.1000,                 loss: nan
env4_first_0:                 episode reward: 2.1000,                 loss: nan
env4_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.7842s / 47704.5287 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0143
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
env2_first_0:                 episode reward: 2.8500,                 loss: nan
env2_second_0:                 episode reward: -2.8500,                 loss: nan
env3_first_0:                 episode reward: 2.8500,                 loss: nan
env3_second_0:                 episode reward: -2.8500,                 loss: nan
env4_first_0:                 episode reward: 2.8500,                 loss: nan
env4_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.2508s / 47868.7795 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0144
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
env2_first_0:                 episode reward: 3.1000,                 loss: nan
env2_second_0:                 episode reward: -3.1000,                 loss: nan
env3_first_0:                 episode reward: 3.1000,                 loss: nan
env3_second_0:                 episode reward: -3.1000,                 loss: nan
env4_first_0:                 episode reward: 3.1000,                 loss: nan
env4_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.6490s / 48034.4285 s
env0_first_0:                 episode reward: 0.4500,                 loss: nan
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0146
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
env2_first_0:                 episode reward: 0.4500,                 loss: nan
env2_second_0:                 episode reward: -0.4500,                 loss: nan
env3_first_0:                 episode reward: 0.4500,                 loss: nan
env3_second_0:                 episode reward: -0.4500,                 loss: nan
env4_first_0:                 episode reward: 0.4500,                 loss: nan
env4_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.8676s / 48199.2961 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0157
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.7080s / 48364.0041 s
env0_first_0:                 episode reward: 1.3500,                 loss: nan
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0176
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
env2_first_0:                 episode reward: 1.3500,                 loss: nan
env2_second_0:                 episode reward: -1.3500,                 loss: nan
env3_first_0:                 episode reward: 1.3500,                 loss: nan
env3_second_0:                 episode reward: -1.3500,                 loss: nan
env4_first_0:                 episode reward: 1.3500,                 loss: nan
env4_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.3466s / 48530.3508 s
env0_first_0:                 episode reward: 1.1000,                 loss: nan
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0183
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
env2_first_0:                 episode reward: 1.1000,                 loss: nan
env2_second_0:                 episode reward: -1.1000,                 loss: nan
env3_first_0:                 episode reward: 1.1000,                 loss: nan
env3_second_0:                 episode reward: -1.1000,                 loss: nan
env4_first_0:                 episode reward: 1.1000,                 loss: nan
env4_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.8120s / 48694.1628 s
env0_first_0:                 episode reward: 0.1500,                 loss: nan
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0189
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
env2_first_0:                 episode reward: 0.1500,                 loss: nan
env2_second_0:                 episode reward: -0.1500,                 loss: nan
env3_first_0:                 episode reward: 0.1500,                 loss: nan
env3_second_0:                 episode reward: -0.1500,                 loss: nan
env4_first_0:                 episode reward: 0.1500,                 loss: nan
env4_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.2980s / 48857.4608 s
env0_first_0:                 episode reward: 0.4000,                 loss: nan
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0170
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
env2_first_0:                 episode reward: 0.4000,                 loss: nan
env2_second_0:                 episode reward: -0.4000,                 loss: nan
env3_first_0:                 episode reward: 0.4000,                 loss: nan
env3_second_0:                 episode reward: -0.4000,                 loss: nan
env4_first_0:                 episode reward: 0.4000,                 loss: nan
env4_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 167.5107s / 49024.9715 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0147
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
env2_first_0:                 episode reward: -0.5500,                 loss: nan
env2_second_0:                 episode reward: 0.5500,                 loss: nan
env3_first_0:                 episode reward: -0.5500,                 loss: nan
env3_second_0:                 episode reward: 0.5500,                 loss: nan
env4_first_0:                 episode reward: -0.5500,                 loss: nan
env4_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.1711s / 49191.1426 s
env0_first_0:                 episode reward: 0.1000,                 loss: nan
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0131
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
env2_first_0:                 episode reward: 0.1000,                 loss: nan
env2_second_0:                 episode reward: -0.1000,                 loss: nan
env3_first_0:                 episode reward: 0.1000,                 loss: nan
env3_second_0:                 episode reward: -0.1000,                 loss: nan
env4_first_0:                 episode reward: 0.1000,                 loss: nan
env4_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.2564s / 49357.3990 s
env0_first_0:                 episode reward: -1.0500,                 loss: nan
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0131
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
env2_first_0:                 episode reward: -1.0500,                 loss: nan
env2_second_0:                 episode reward: 1.0500,                 loss: nan
env3_first_0:                 episode reward: -1.0500,                 loss: nan
env3_second_0:                 episode reward: 1.0500,                 loss: nan
env4_first_0:                 episode reward: -1.0500,                 loss: nan
env4_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.9572s / 49523.3562 s
env0_first_0:                 episode reward: 0.5500,                 loss: nan
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0127
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
env2_first_0:                 episode reward: 0.5500,                 loss: nan
env2_second_0:                 episode reward: -0.5500,                 loss: nan
env3_first_0:                 episode reward: 0.5500,                 loss: nan
env3_second_0:                 episode reward: -0.5500,                 loss: nan
env4_first_0:                 episode reward: 0.5500,                 loss: nan
env4_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 167.0828s / 49690.4390 s
env0_first_0:                 episode reward: -1.3500,                 loss: nan
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0130
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
env2_first_0:                 episode reward: -1.3500,                 loss: nan
env2_second_0:                 episode reward: 1.3500,                 loss: nan
env3_first_0:                 episode reward: -1.1000,                 loss: nan
env3_second_0:                 episode reward: 1.1000,                 loss: nan
env4_first_0:                 episode reward: -1.3500,                 loss: nan
env4_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.7759s / 49856.2150 s
env0_first_0:                 episode reward: -1.2500,                 loss: nan
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0130
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
env2_first_0:                 episode reward: -1.2500,                 loss: nan
env2_second_0:                 episode reward: 1.2500,                 loss: nan
env3_first_0:                 episode reward: -1.2500,                 loss: nan
env3_second_0:                 episode reward: 1.2500,                 loss: nan
env4_first_0:                 episode reward: -1.2500,                 loss: nan
env4_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.1698s / 50019.3848 s
env0_first_0:                 episode reward: 0.7000,                 loss: nan
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0127
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
env2_first_0:                 episode reward: 0.7000,                 loss: nan
env2_second_0:                 episode reward: -0.7000,                 loss: nan
env3_first_0:                 episode reward: 0.7000,                 loss: nan
env3_second_0:                 episode reward: -0.7000,                 loss: nan
env4_first_0:                 episode reward: 0.7000,                 loss: nan
env4_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.8558s / 50184.2406 s
env0_first_0:                 episode reward: 1.2000,                 loss: nan
env0_second_0:                 episode reward: -1.2000,                 loss: 0.0123
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
env2_first_0:                 episode reward: 1.2000,                 loss: nan
env2_second_0:                 episode reward: -1.2000,                 loss: nan
env3_first_0:                 episode reward: 0.7500,                 loss: nan
env3_second_0:                 episode reward: -0.7500,                 loss: nan
env4_first_0:                 episode reward: 1.2000,                 loss: nan
env4_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.4870s / 50348.7277 s
env0_first_0:                 episode reward: -1.5000,                 loss: nan
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0124
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
env2_first_0:                 episode reward: -1.5000,                 loss: nan
env2_second_0:                 episode reward: 1.5000,                 loss: nan
env3_first_0:                 episode reward: -1.5000,                 loss: nan
env3_second_0:                 episode reward: 1.5000,                 loss: nan
env4_first_0:                 episode reward: -1.5000,                 loss: nan
env4_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.3962s / 50514.1239 s
env0_first_0:                 episode reward: -3.1500,                 loss: nan
env0_second_0:                 episode reward: 3.1500,                 loss: 0.0122
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
env2_first_0:                 episode reward: -3.1500,                 loss: nan
env2_second_0:                 episode reward: 3.1500,                 loss: nan
env3_first_0:                 episode reward: -3.1500,                 loss: nan
env3_second_0:                 episode reward: 3.1500,                 loss: nan
env4_first_0:                 episode reward: -3.1500,                 loss: nan
env4_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.6403s / 50678.7642 s
env0_first_0:                 episode reward: -1.9000,                 loss: nan
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0119
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
env2_first_0:                 episode reward: -1.9000,                 loss: nan
env2_second_0:                 episode reward: 1.9000,                 loss: nan
env3_first_0:                 episode reward: -2.3500,                 loss: nan
env3_second_0:                 episode reward: 2.3500,                 loss: nan
env4_first_0:                 episode reward: -2.3500,                 loss: nan
env4_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.2372s / 50843.0015 s
env0_first_0:                 episode reward: -1.7500,                 loss: nan
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0121
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
env2_first_0:                 episode reward: -1.7500,                 loss: nan
env2_second_0:                 episode reward: 1.7500,                 loss: nan
env3_first_0:                 episode reward: -1.7500,                 loss: nan
env3_second_0:                 episode reward: 1.7500,                 loss: nan
env4_first_0:                 episode reward: -1.7500,                 loss: nan
env4_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.5716s / 51008.5730 s
env0_first_0:                 episode reward: -2.4500,                 loss: nan
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0119
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
env2_first_0:                 episode reward: -2.7000,                 loss: nan
env2_second_0:                 episode reward: 2.7000,                 loss: nan
env3_first_0:                 episode reward: -2.7000,                 loss: nan
env3_second_0:                 episode reward: 2.7000,                 loss: nan
env4_first_0:                 episode reward: -2.4500,                 loss: nan
env4_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.2846s / 51171.8576 s
env0_first_0:                 episode reward: -2.2500,                 loss: nan
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0116
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
env2_first_0:                 episode reward: -2.2500,                 loss: nan
env2_second_0:                 episode reward: 2.2500,                 loss: nan
env3_first_0:                 episode reward: -2.2500,                 loss: nan
env3_second_0:                 episode reward: 2.2500,                 loss: nan
env4_first_0:                 episode reward: -2.2500,                 loss: nan
env4_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.1188s / 51336.9764 s
env0_first_0:                 episode reward: -2.8500,                 loss: nan
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0112
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
env2_first_0:                 episode reward: -2.8500,                 loss: nan
env2_second_0:                 episode reward: 2.8500,                 loss: nan
env3_first_0:                 episode reward: -2.8500,                 loss: nan
env3_second_0:                 episode reward: 2.8500,                 loss: nan
env4_first_0:                 episode reward: -2.8500,                 loss: nan
env4_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.0684s / 51503.0448 s
env0_first_0:                 episode reward: -2.1500,                 loss: nan
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0113
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
env2_first_0:                 episode reward: -2.2500,                 loss: nan
env2_second_0:                 episode reward: 2.2500,                 loss: nan
env3_first_0:                 episode reward: -2.2500,                 loss: nan
env3_second_0:                 episode reward: 2.2500,                 loss: nan
env4_first_0:                 episode reward: -2.1500,                 loss: nan
env4_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.5567s / 51667.6015 s
env0_first_0:                 episode reward: 0.7000,                 loss: nan
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0109
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
env2_first_0:                 episode reward: 0.7000,                 loss: nan
env2_second_0:                 episode reward: -0.7000,                 loss: nan
env3_first_0:                 episode reward: 0.7000,                 loss: nan
env3_second_0:                 episode reward: -0.7000,                 loss: nan
env4_first_0:                 episode reward: 0.7000,                 loss: nan
env4_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.3739s / 51832.9754 s
env0_first_0:                 episode reward: -3.7500,                 loss: nan
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0109
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
env2_first_0:                 episode reward: -3.6500,                 loss: nan
env2_second_0:                 episode reward: 3.6500,                 loss: nan
env3_first_0:                 episode reward: -3.7500,                 loss: nan
env3_second_0:                 episode reward: 3.7500,                 loss: nan
env4_first_0:                 episode reward: -3.6500,                 loss: nan
env4_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.5947s / 51998.5701 s
env0_first_0:                 episode reward: -2.2000,                 loss: nan
env0_second_0:                 episode reward: 2.2000,                 loss: 0.0109
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
env2_first_0:                 episode reward: -2.2000,                 loss: nan
env2_second_0:                 episode reward: 2.2000,                 loss: nan
env3_first_0:                 episode reward: -2.2000,                 loss: nan
env3_second_0:                 episode reward: 2.2000,                 loss: nan
env4_first_0:                 episode reward: -2.2000,                 loss: nan
env4_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.6410s / 52164.2111 s
env0_first_0:                 episode reward: -4.2500,                 loss: nan
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0111
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
env2_first_0:                 episode reward: -4.2500,                 loss: nan
env2_second_0:                 episode reward: 4.2500,                 loss: nan
env3_first_0:                 episode reward: -4.2500,                 loss: nan
env3_second_0:                 episode reward: 4.2500,                 loss: nan
env4_first_0:                 episode reward: -4.2500,                 loss: nan
env4_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.1870s / 52327.3981 s
env0_first_0:                 episode reward: -0.8000,                 loss: nan
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0109
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
env2_first_0:                 episode reward: -1.1000,                 loss: nan
env2_second_0:                 episode reward: 1.1000,                 loss: nan
env3_first_0:                 episode reward: -0.8000,                 loss: nan
env3_second_0:                 episode reward: 0.8000,                 loss: nan
env4_first_0:                 episode reward: -0.8000,                 loss: nan
env4_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.5156s / 52490.9137 s
env0_first_0:                 episode reward: -0.3000,                 loss: nan
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0108
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
env2_first_0:                 episode reward: -0.3000,                 loss: nan
env2_second_0:                 episode reward: 0.3000,                 loss: nan
env3_first_0:                 episode reward: -0.3000,                 loss: nan
env3_second_0:                 episode reward: 0.3000,                 loss: nan
env4_first_0:                 episode reward: -0.3000,                 loss: nan
env4_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.0614s / 52654.9751 s
env0_first_0:                 episode reward: 0.4500,                 loss: nan
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0110
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
env2_first_0:                 episode reward: 0.4500,                 loss: nan
env2_second_0:                 episode reward: -0.4500,                 loss: nan
env3_first_0:                 episode reward: 0.4500,                 loss: nan
env3_second_0:                 episode reward: -0.4500,                 loss: nan
env4_first_0:                 episode reward: 0.4500,                 loss: nan
env4_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.6742s / 52819.6493 s
env0_first_0:                 episode reward: -1.3500,                 loss: nan
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0112
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
env2_first_0:                 episode reward: -1.3500,                 loss: nan
env2_second_0:                 episode reward: 1.3500,                 loss: nan
env3_first_0:                 episode reward: -1.3500,                 loss: nan
env3_second_0:                 episode reward: 1.3500,                 loss: nan
env4_first_0:                 episode reward: -1.3500,                 loss: nan
env4_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.5503s / 52984.1996 s
env0_first_0:                 episode reward: -2.8500,                 loss: nan
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0109
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
env2_first_0:                 episode reward: -2.8500,                 loss: nan
env2_second_0:                 episode reward: 2.8500,                 loss: nan
env3_first_0:                 episode reward: -2.8500,                 loss: nan
env3_second_0:                 episode reward: 2.8500,                 loss: nan
env4_first_0:                 episode reward: -2.8500,                 loss: nan
env4_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.7151s / 53147.9147 s
env0_first_0:                 episode reward: -2.1000,                 loss: nan
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0107
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
env2_first_0:                 episode reward: -2.1000,                 loss: nan
env2_second_0:                 episode reward: 2.1000,                 loss: nan
env3_first_0:                 episode reward: -2.1000,                 loss: nan
env3_second_0:                 episode reward: 2.1000,                 loss: nan
env4_first_0:                 episode reward: -2.1000,                 loss: nan
env4_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.6756s / 53311.5903 s
env0_first_0:                 episode reward: -1.0500,                 loss: nan
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0105
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
env2_first_0:                 episode reward: -1.0500,                 loss: nan
env2_second_0:                 episode reward: 1.0500,                 loss: nan
env3_first_0:                 episode reward: -1.0500,                 loss: nan
env3_second_0:                 episode reward: 1.0500,                 loss: nan
env4_first_0:                 episode reward: -1.0500,                 loss: nan
env4_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.9010s / 53477.4913 s
env0_first_0:                 episode reward: -1.3000,                 loss: nan
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0107
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
env2_first_0:                 episode reward: -1.3000,                 loss: nan
env2_second_0:                 episode reward: 1.3000,                 loss: nan
env3_first_0:                 episode reward: -1.3000,                 loss: nan
env3_second_0:                 episode reward: 1.3000,                 loss: nan
env4_first_0:                 episode reward: -1.3000,                 loss: nan
env4_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.1896s / 53642.6809 s
env0_first_0:                 episode reward: -1.6000,                 loss: nan
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0106
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
env2_first_0:                 episode reward: -1.6000,                 loss: nan
env2_second_0:                 episode reward: 1.6000,                 loss: nan
env3_first_0:                 episode reward: -1.6000,                 loss: nan
env3_second_0:                 episode reward: 1.6000,                 loss: nan
env4_first_0:                 episode reward: -1.6000,                 loss: nan
env4_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.9573s / 53806.6382 s
env0_first_0:                 episode reward: -2.2000,                 loss: nan
env0_second_0:                 episode reward: 2.2000,                 loss: 0.0107
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
env2_first_0:                 episode reward: -2.2000,                 loss: nan
env2_second_0:                 episode reward: 2.2000,                 loss: nan
env3_first_0:                 episode reward: -2.2000,                 loss: nan
env3_second_0:                 episode reward: 2.2000,                 loss: nan
env4_first_0:                 episode reward: -2.2000,                 loss: nan
env4_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.6952s / 53971.3334 s
env0_first_0:                 episode reward: -2.0500,                 loss: nan
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0108
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
env2_first_0:                 episode reward: -2.1500,                 loss: nan
env2_second_0:                 episode reward: 2.1500,                 loss: nan
env3_first_0:                 episode reward: -2.0500,                 loss: nan
env3_second_0:                 episode reward: 2.0500,                 loss: nan
env4_first_0:                 episode reward: -2.1500,                 loss: nan
env4_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.5934s / 54137.9268 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0463
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0110
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
env2_first_0:                 episode reward: -3.4000,                 loss: nan
env2_second_0:                 episode reward: 3.4000,                 loss: nan
env3_first_0:                 episode reward: -3.4000,                 loss: nan
env3_second_0:                 episode reward: 3.4000,                 loss: nan
env4_first_0:                 episode reward: -3.5500,                 loss: nan
env4_second_0:                 episode reward: 3.5500,                 loss: nan
Score delta: 10.6, update the opponent.
Episode: 6021/10000 (60.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.8672s / 54302.7940 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0263
env0_second_0:                 episode reward: 2.5000,                 loss: nan
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
env2_first_0:                 episode reward: -2.3000,                 loss: nan
env2_second_0:                 episode reward: 2.3000,                 loss: nan
env3_first_0:                 episode reward: -2.3000,                 loss: nan
env3_second_0:                 episode reward: 2.3000,                 loss: nan
env4_first_0:                 episode reward: -2.3000,                 loss: nan
env4_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.7111s / 54467.5051 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0190
env0_second_0:                 episode reward: 3.2500,                 loss: nan
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
env2_first_0:                 episode reward: -3.3000,                 loss: nan
env2_second_0:                 episode reward: 3.3000,                 loss: nan
env3_first_0:                 episode reward: -3.2500,                 loss: nan
env3_second_0:                 episode reward: 3.2500,                 loss: nan
env4_first_0:                 episode reward: -3.2500,                 loss: nan
env4_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.9993s / 54631.5045 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0192
env0_second_0:                 episode reward: 4.9500,                 loss: nan
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
env2_first_0:                 episode reward: -5.2500,                 loss: nan
env2_second_0:                 episode reward: 5.2500,                 loss: nan
env3_first_0:                 episode reward: -5.2500,                 loss: nan
env3_second_0:                 episode reward: 5.2500,                 loss: nan
env4_first_0:                 episode reward: -5.0500,                 loss: nan
env4_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.8798s / 54796.3843 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0236
env0_second_0:                 episode reward: 5.4500,                 loss: nan
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
env2_first_0:                 episode reward: -5.4500,                 loss: nan
env2_second_0:                 episode reward: 5.4500,                 loss: nan
env3_first_0:                 episode reward: -5.4500,                 loss: nan
env3_second_0:                 episode reward: 5.4500,                 loss: nan
env4_first_0:                 episode reward: -5.4500,                 loss: nan
env4_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.2547s / 54959.6390 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0164
env0_second_0:                 episode reward: 3.1000,                 loss: nan
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
env2_first_0:                 episode reward: -2.9500,                 loss: nan
env2_second_0:                 episode reward: 2.9500,                 loss: nan
env3_first_0:                 episode reward: -3.0000,                 loss: nan
env3_second_0:                 episode reward: 3.0000,                 loss: nan
env4_first_0:                 episode reward: -3.1000,                 loss: nan
env4_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.2764s / 55121.9154 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0166
env0_second_0:                 episode reward: 2.3500,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
env2_first_0:                 episode reward: -2.4500,                 loss: nan
env2_second_0:                 episode reward: 2.4500,                 loss: nan
env3_first_0:                 episode reward: -2.4500,                 loss: nan
env3_second_0:                 episode reward: 2.4500,                 loss: nan
env4_first_0:                 episode reward: -2.4500,                 loss: nan
env4_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.5364s / 55284.4517 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0181
env0_second_0:                 episode reward: -1.8000,                 loss: nan
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
env2_first_0:                 episode reward: 1.8000,                 loss: nan
env2_second_0:                 episode reward: -1.8000,                 loss: nan
env3_first_0:                 episode reward: 1.8000,                 loss: nan
env3_second_0:                 episode reward: -1.8000,                 loss: nan
env4_first_0:                 episode reward: 1.8000,                 loss: nan
env4_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.6180s / 55449.0697 s
env0_first_0:                 episode reward: 5.5000,                 loss: 0.0181
env0_second_0:                 episode reward: -5.5000,                 loss: 0.0183
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
env2_first_0:                 episode reward: 5.5000,                 loss: nan
env2_second_0:                 episode reward: -5.5000,                 loss: nan
env3_first_0:                 episode reward: 5.5000,                 loss: nan
env3_second_0:                 episode reward: -5.5000,                 loss: nan
env4_first_0:                 episode reward: 5.5000,                 loss: nan
env4_second_0:                 episode reward: -5.5000,                 loss: nan
Score delta: 10.2, update the opponent.
Episode: 6181/10000 (61.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.8128s / 55614.8825 s
env0_first_0:                 episode reward: 5.4000,                 loss: nan
env0_second_0:                 episode reward: -5.4000,                 loss: 0.0125
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
env2_first_0:                 episode reward: 5.4000,                 loss: nan
env2_second_0:                 episode reward: -5.4000,                 loss: nan
env3_first_0:                 episode reward: 5.4000,                 loss: nan
env3_second_0:                 episode reward: -5.4000,                 loss: nan
env4_first_0:                 episode reward: 5.4000,                 loss: nan
env4_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.3350s / 55781.2175 s
env0_first_0:                 episode reward: 7.4500,                 loss: nan
env0_second_0:                 episode reward: -7.4500,                 loss: 0.0132
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
env2_first_0:                 episode reward: 7.4500,                 loss: nan
env2_second_0:                 episode reward: -7.4500,                 loss: nan
env3_first_0:                 episode reward: 7.4500,                 loss: nan
env3_second_0:                 episode reward: -7.4500,                 loss: nan
env4_first_0:                 episode reward: 7.4500,                 loss: nan
env4_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.9014s / 55947.1189 s
env0_first_0:                 episode reward: 7.1500,                 loss: nan
env0_second_0:                 episode reward: -7.1500,                 loss: 0.0150
env1_first_0:                 episode reward: 7.1500,                 loss: nan
env1_second_0:                 episode reward: -7.1500,                 loss: nan
env2_first_0:                 episode reward: 7.1500,                 loss: nan
env2_second_0:                 episode reward: -7.1500,                 loss: nan
env3_first_0:                 episode reward: 7.1500,                 loss: nan
env3_second_0:                 episode reward: -7.1500,                 loss: nan
env4_first_0:                 episode reward: 7.1500,                 loss: nan
env4_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.2532s / 56110.3722 s
env0_first_0:                 episode reward: 7.9000,                 loss: nan
env0_second_0:                 episode reward: -7.9000,                 loss: 0.0171
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
env2_first_0:                 episode reward: 7.9000,                 loss: nan
env2_second_0:                 episode reward: -7.9000,                 loss: nan
env3_first_0:                 episode reward: 7.9000,                 loss: nan
env3_second_0:                 episode reward: -7.9000,                 loss: nan
env4_first_0:                 episode reward: 7.9000,                 loss: nan
env4_second_0:                 episode reward: -7.9000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.5657s / 56275.9379 s
env0_first_0:                 episode reward: 6.3000,                 loss: nan
env0_second_0:                 episode reward: -6.3000,                 loss: 0.0175
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
env2_first_0:                 episode reward: 6.3000,                 loss: nan
env2_second_0:                 episode reward: -6.3000,                 loss: nan
env3_first_0:                 episode reward: 6.3000,                 loss: nan
env3_second_0:                 episode reward: -6.3000,                 loss: nan
env4_first_0:                 episode reward: 6.3000,                 loss: nan
env4_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.3342s / 56438.2721 s
env0_first_0:                 episode reward: 5.5000,                 loss: nan
env0_second_0:                 episode reward: -5.5000,                 loss: 0.0178
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
env2_first_0:                 episode reward: 5.5000,                 loss: nan
env2_second_0:                 episode reward: -5.5000,                 loss: nan
env3_first_0:                 episode reward: 5.5000,                 loss: nan
env3_second_0:                 episode reward: -5.5000,                 loss: nan
env4_first_0:                 episode reward: 5.5000,                 loss: nan
env4_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.7562s / 56602.0283 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0204
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
env2_first_0:                 episode reward: 2.6000,                 loss: nan
env2_second_0:                 episode reward: -2.6000,                 loss: nan
env3_first_0:                 episode reward: 3.0000,                 loss: nan
env3_second_0:                 episode reward: -3.0000,                 loss: nan
env4_first_0:                 episode reward: 3.0000,                 loss: nan
env4_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.5184s / 56764.5467 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0210
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
env2_first_0:                 episode reward: 3.2000,                 loss: nan
env2_second_0:                 episode reward: -3.2000,                 loss: nan
env3_first_0:                 episode reward: 3.2000,                 loss: nan
env3_second_0:                 episode reward: -3.2000,                 loss: nan
env4_first_0:                 episode reward: 3.2000,                 loss: nan
env4_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.9499s / 56929.4966 s
env0_first_0:                 episode reward: 4.9500,                 loss: nan
env0_second_0:                 episode reward: -4.9500,                 loss: 0.0199
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
env2_first_0:                 episode reward: 4.9500,                 loss: nan
env2_second_0:                 episode reward: -4.9500,                 loss: nan
env3_first_0:                 episode reward: 4.9500,                 loss: nan
env3_second_0:                 episode reward: -4.9500,                 loss: nan
env4_first_0:                 episode reward: 4.9500,                 loss: nan
env4_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.9312s / 57095.4278 s
env0_first_0:                 episode reward: 5.8500,                 loss: nan
env0_second_0:                 episode reward: -5.8500,                 loss: 0.0180
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
env2_first_0:                 episode reward: 5.8500,                 loss: nan
env2_second_0:                 episode reward: -5.8500,                 loss: nan
env3_first_0:                 episode reward: 5.8500,                 loss: nan
env3_second_0:                 episode reward: -5.8500,                 loss: nan
env4_first_0:                 episode reward: 5.9500,                 loss: nan
env4_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.6195s / 57260.0473 s
env0_first_0:                 episode reward: 5.8500,                 loss: nan
env0_second_0:                 episode reward: -5.8500,                 loss: 0.0163
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
env2_first_0:                 episode reward: 5.5000,                 loss: nan
env2_second_0:                 episode reward: -5.5000,                 loss: nan
env3_first_0:                 episode reward: 5.8000,                 loss: nan
env3_second_0:                 episode reward: -5.8000,                 loss: nan
env4_first_0:                 episode reward: 5.5000,                 loss: nan
env4_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.9566s / 57425.0039 s
env0_first_0:                 episode reward: 6.3500,                 loss: nan
env0_second_0:                 episode reward: -6.3500,                 loss: 0.0168
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
env2_first_0:                 episode reward: 6.3500,                 loss: nan
env2_second_0:                 episode reward: -6.3500,                 loss: nan
env3_first_0:                 episode reward: 6.3500,                 loss: nan
env3_second_0:                 episode reward: -6.3500,                 loss: nan
env4_first_0:                 episode reward: 6.3000,                 loss: nan
env4_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.6835s / 57589.6873 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0173
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
env2_first_0:                 episode reward: 2.9000,                 loss: nan
env2_second_0:                 episode reward: -2.9000,                 loss: nan
env3_first_0:                 episode reward: 2.9000,                 loss: nan
env3_second_0:                 episode reward: -2.9000,                 loss: nan
env4_first_0:                 episode reward: 2.9000,                 loss: nan
env4_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.0192s / 57754.7065 s
env0_first_0:                 episode reward: 4.4500,                 loss: nan
env0_second_0:                 episode reward: -4.4500,                 loss: 0.0160
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
env2_first_0:                 episode reward: 4.4500,                 loss: nan
env2_second_0:                 episode reward: -4.4500,                 loss: nan
env3_first_0:                 episode reward: 4.4500,                 loss: nan
env3_second_0:                 episode reward: -4.4500,                 loss: nan
env4_first_0:                 episode reward: 4.4500,                 loss: nan
env4_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.9372s / 57918.6438 s
env0_first_0:                 episode reward: 0.7500,                 loss: nan
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0150
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
env2_first_0:                 episode reward: 0.7000,                 loss: nan
env2_second_0:                 episode reward: -0.7000,                 loss: nan
env3_first_0:                 episode reward: 0.7500,                 loss: nan
env3_second_0:                 episode reward: -0.7500,                 loss: nan
env4_first_0:                 episode reward: 0.7500,                 loss: nan
env4_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.1734s / 58084.8172 s
env0_first_0:                 episode reward: -0.4500,                 loss: nan
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0153
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
env2_first_0:                 episode reward: 0.0000,                 loss: nan
env2_second_0:                 episode reward: 0.0000,                 loss: nan
env3_first_0:                 episode reward: 0.0000,                 loss: nan
env3_second_0:                 episode reward: 0.0000,                 loss: nan
env4_first_0:                 episode reward: -0.4500,                 loss: nan
env4_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.8899s / 58248.7070 s
env0_first_0:                 episode reward: 4.4000,                 loss: nan
env0_second_0:                 episode reward: -4.4000,                 loss: 0.0150
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
env2_first_0:                 episode reward: 4.4000,                 loss: nan
env2_second_0:                 episode reward: -4.4000,                 loss: nan
env3_first_0:                 episode reward: 4.4000,                 loss: nan
env3_second_0:                 episode reward: -4.4000,                 loss: nan
env4_first_0:                 episode reward: 4.4000,                 loss: nan
env4_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.7912s / 58413.4982 s
env0_first_0:                 episode reward: 5.3000,                 loss: nan
env0_second_0:                 episode reward: -5.3000,                 loss: 0.0138
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
env2_first_0:                 episode reward: 5.3000,                 loss: nan
env2_second_0:                 episode reward: -5.3000,                 loss: nan
env3_first_0:                 episode reward: 5.3000,                 loss: nan
env3_second_0:                 episode reward: -5.3000,                 loss: nan
env4_first_0:                 episode reward: 5.3000,                 loss: nan
env4_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.3688s / 58577.8670 s
env0_first_0:                 episode reward: 5.2500,                 loss: nan
env0_second_0:                 episode reward: -5.2500,                 loss: 0.0127
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
env2_first_0:                 episode reward: 5.2500,                 loss: nan
env2_second_0:                 episode reward: -5.2500,                 loss: nan
env3_first_0:                 episode reward: 5.2500,                 loss: nan
env3_second_0:                 episode reward: -5.2500,                 loss: nan
env4_first_0:                 episode reward: 5.2000,                 loss: nan
env4_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.7803s / 58742.6473 s
env0_first_0:                 episode reward: 4.2000,                 loss: nan
env0_second_0:                 episode reward: -4.2000,                 loss: 0.0124
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
env2_first_0:                 episode reward: 4.2000,                 loss: nan
env2_second_0:                 episode reward: -4.2000,                 loss: nan
env3_first_0:                 episode reward: 4.2000,                 loss: nan
env3_second_0:                 episode reward: -4.2000,                 loss: nan
env4_first_0:                 episode reward: 4.2000,                 loss: nan
env4_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.5246s / 58908.1719 s
env0_first_0:                 episode reward: 6.0000,                 loss: nan
env0_second_0:                 episode reward: -6.0000,                 loss: 0.0120
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
env2_first_0:                 episode reward: 6.0000,                 loss: nan
env2_second_0:                 episode reward: -6.0000,                 loss: nan
env3_first_0:                 episode reward: 6.0000,                 loss: nan
env3_second_0:                 episode reward: -6.0000,                 loss: nan
env4_first_0:                 episode reward: 6.0000,                 loss: nan
env4_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.5085s / 59072.6804 s
env0_first_0:                 episode reward: 0.1500,                 loss: nan
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0121
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
env2_first_0:                 episode reward: 0.1500,                 loss: nan
env2_second_0:                 episode reward: -0.1500,                 loss: nan
env3_first_0:                 episode reward: 0.1500,                 loss: nan
env3_second_0:                 episode reward: -0.1500,                 loss: nan
env4_first_0:                 episode reward: 0.1500,                 loss: nan
env4_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.8908s / 59237.5712 s
env0_first_0:                 episode reward: 4.0000,                 loss: nan
env0_second_0:                 episode reward: -4.0000,                 loss: 0.0117
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
env2_first_0:                 episode reward: 4.0000,                 loss: nan
env2_second_0:                 episode reward: -4.0000,                 loss: nan
env3_first_0:                 episode reward: 4.0000,                 loss: nan
env3_second_0:                 episode reward: -4.0000,                 loss: nan
env4_first_0:                 episode reward: 4.0000,                 loss: nan
env4_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.5052s / 59404.0764 s
env0_first_0:                 episode reward: 1.1500,                 loss: nan
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0118
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
env2_first_0:                 episode reward: 1.1500,                 loss: nan
env2_second_0:                 episode reward: -1.1500,                 loss: nan
env3_first_0:                 episode reward: 1.9000,                 loss: nan
env3_second_0:                 episode reward: -1.9000,                 loss: nan
env4_first_0:                 episode reward: 1.1500,                 loss: nan
env4_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.5616s / 59569.6380 s
env0_first_0:                 episode reward: 4.4500,                 loss: nan
env0_second_0:                 episode reward: -4.4500,                 loss: 0.0120
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
env2_first_0:                 episode reward: 4.4500,                 loss: nan
env2_second_0:                 episode reward: -4.4500,                 loss: nan
env3_first_0:                 episode reward: 4.4500,                 loss: nan
env3_second_0:                 episode reward: -4.4500,                 loss: nan
env4_first_0:                 episode reward: 4.4500,                 loss: nan
env4_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.4455s / 59734.0836 s
env0_first_0:                 episode reward: 1.0500,                 loss: nan
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0121
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
env2_first_0:                 episode reward: 1.0500,                 loss: nan
env2_second_0:                 episode reward: -1.0500,                 loss: nan
env3_first_0:                 episode reward: 0.9500,                 loss: nan
env3_second_0:                 episode reward: -0.9500,                 loss: nan
env4_first_0:                 episode reward: 0.9500,                 loss: nan
env4_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.0379s / 59900.1215 s
env0_first_0:                 episode reward: 2.3500,                 loss: nan
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0112
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
env2_first_0:                 episode reward: 1.6000,                 loss: nan
env2_second_0:                 episode reward: -1.6000,                 loss: nan
env3_first_0:                 episode reward: 2.5000,                 loss: nan
env3_second_0:                 episode reward: -2.5000,                 loss: nan
env4_first_0:                 episode reward: 2.3500,                 loss: nan
env4_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.7878s / 60064.9092 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0111
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
env2_first_0:                 episode reward: 3.1000,                 loss: nan
env2_second_0:                 episode reward: -3.1000,                 loss: nan
env3_first_0:                 episode reward: 2.5500,                 loss: nan
env3_second_0:                 episode reward: -2.5500,                 loss: nan
env4_first_0:                 episode reward: 3.1000,                 loss: nan
env4_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.5988s / 60229.5080 s
env0_first_0:                 episode reward: 1.4000,                 loss: nan
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0110
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
env2_first_0:                 episode reward: 1.4000,                 loss: nan
env2_second_0:                 episode reward: -1.4000,                 loss: nan
env3_first_0:                 episode reward: 1.4000,                 loss: nan
env3_second_0:                 episode reward: -1.4000,                 loss: nan
env4_first_0:                 episode reward: 1.4000,                 loss: nan
env4_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.2484s / 60394.7564 s
env0_first_0:                 episode reward: 1.5000,                 loss: nan
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0108
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
env2_first_0:                 episode reward: 1.5000,                 loss: nan
env2_second_0:                 episode reward: -1.5000,                 loss: nan
env3_first_0:                 episode reward: 1.5000,                 loss: nan
env3_second_0:                 episode reward: -1.5000,                 loss: nan
env4_first_0:                 episode reward: 1.6000,                 loss: nan
env4_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.5739s / 60558.3303 s
env0_first_0:                 episode reward: -1.1500,                 loss: nan
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0110
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
env2_first_0:                 episode reward: -1.1500,                 loss: nan
env2_second_0:                 episode reward: 1.1500,                 loss: nan
env3_first_0:                 episode reward: -1.1500,                 loss: nan
env3_second_0:                 episode reward: 1.1500,                 loss: nan
env4_first_0:                 episode reward: -1.1500,                 loss: nan
env4_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.8513s / 60723.1816 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0107
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
env2_first_0:                 episode reward: -0.5000,                 loss: nan
env2_second_0:                 episode reward: 0.5000,                 loss: nan
env3_first_0:                 episode reward: -0.5000,                 loss: nan
env3_second_0:                 episode reward: 0.5000,                 loss: nan
env4_first_0:                 episode reward: -0.6500,                 loss: nan
env4_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.3446s / 60887.5261 s
env0_first_0:                 episode reward: 1.7000,                 loss: nan
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0111
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
env2_first_0:                 episode reward: 1.7000,                 loss: nan
env2_second_0:                 episode reward: -1.7000,                 loss: nan
env3_first_0:                 episode reward: 1.7000,                 loss: nan
env3_second_0:                 episode reward: -1.7000,                 loss: nan
env4_first_0:                 episode reward: 1.7000,                 loss: nan
env4_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.4567s / 61049.9828 s
env0_first_0:                 episode reward: 1.7000,                 loss: nan
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0110
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
env2_first_0:                 episode reward: 1.7000,                 loss: nan
env2_second_0:                 episode reward: -1.7000,                 loss: nan
env3_first_0:                 episode reward: 1.7000,                 loss: nan
env3_second_0:                 episode reward: -1.7000,                 loss: nan
env4_first_0:                 episode reward: 1.7000,                 loss: nan
env4_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.0690s / 61215.0518 s
env0_first_0:                 episode reward: 0.9000,                 loss: nan
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0112
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
env2_first_0:                 episode reward: 0.9000,                 loss: nan
env2_second_0:                 episode reward: -0.9000,                 loss: nan
env3_first_0:                 episode reward: 0.4500,                 loss: nan
env3_second_0:                 episode reward: -0.4500,                 loss: nan
env4_first_0:                 episode reward: 0.4500,                 loss: nan
env4_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.1883s / 61380.2401 s
env0_first_0:                 episode reward: -0.5000,                 loss: nan
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0113
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
env2_first_0:                 episode reward: -0.5000,                 loss: nan
env2_second_0:                 episode reward: 0.5000,                 loss: nan
env3_first_0:                 episode reward: -0.5000,                 loss: nan
env3_second_0:                 episode reward: 0.5000,                 loss: nan
env4_first_0:                 episode reward: -0.5000,                 loss: nan
env4_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 169.2910s / 61549.5311 s
env0_first_0:                 episode reward: -1.3500,                 loss: nan
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0112
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
env2_first_0:                 episode reward: -1.3500,                 loss: nan
env2_second_0:                 episode reward: 1.3500,                 loss: nan
env3_first_0:                 episode reward: -1.3500,                 loss: nan
env3_second_0:                 episode reward: 1.3500,                 loss: nan
env4_first_0:                 episode reward: -1.3500,                 loss: nan
env4_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.4902s / 61715.0213 s
env0_first_0:                 episode reward: -3.9500,                 loss: nan
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0114
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
env2_first_0:                 episode reward: -3.9500,                 loss: nan
env2_second_0:                 episode reward: 3.9500,                 loss: nan
env3_first_0:                 episode reward: -3.9500,                 loss: nan
env3_second_0:                 episode reward: 3.9500,                 loss: nan
env4_first_0:                 episode reward: -3.9500,                 loss: nan
env4_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.3836s / 61878.4049 s
env0_first_0:                 episode reward: -3.3000,                 loss: nan
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0112
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
env2_first_0:                 episode reward: -3.3000,                 loss: nan
env2_second_0:                 episode reward: 3.3000,                 loss: nan
env3_first_0:                 episode reward: -3.5500,                 loss: nan
env3_second_0:                 episode reward: 3.5500,                 loss: nan
env4_first_0:                 episode reward: -3.5500,                 loss: nan
env4_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.6937s / 62043.0986 s
env0_first_0:                 episode reward: -2.3500,                 loss: nan
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0114
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
env2_first_0:                 episode reward: -2.3500,                 loss: nan
env2_second_0:                 episode reward: 2.3500,                 loss: nan
env3_first_0:                 episode reward: -2.3500,                 loss: nan
env3_second_0:                 episode reward: 2.3500,                 loss: nan
env4_first_0:                 episode reward: -2.3500,                 loss: nan
env4_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.6932s / 62206.7917 s
env0_first_0:                 episode reward: -3.9500,                 loss: nan
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0114
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
env2_first_0:                 episode reward: -3.9000,                 loss: nan
env2_second_0:                 episode reward: 3.9000,                 loss: nan
env3_first_0:                 episode reward: -3.9000,                 loss: nan
env3_second_0:                 episode reward: 3.9000,                 loss: nan
env4_first_0:                 episode reward: -3.9000,                 loss: nan
env4_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.8460s / 62372.6378 s
env0_first_0:                 episode reward: -3.8500,                 loss: nan
env0_second_0:                 episode reward: 3.8500,                 loss: 0.0113
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
env2_first_0:                 episode reward: -4.1000,                 loss: nan
env2_second_0:                 episode reward: 4.1000,                 loss: nan
env3_first_0:                 episode reward: -3.8500,                 loss: nan
env3_second_0:                 episode reward: 3.8500,                 loss: nan
env4_first_0:                 episode reward: -4.1000,                 loss: nan
env4_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.6851s / 62536.3229 s
env0_first_0:                 episode reward: -4.0000,                 loss: nan
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0115
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
env2_first_0:                 episode reward: -4.3500,                 loss: nan
env2_second_0:                 episode reward: 4.3500,                 loss: nan
env3_first_0:                 episode reward: -4.1500,                 loss: nan
env3_second_0:                 episode reward: 4.1500,                 loss: nan
env4_first_0:                 episode reward: -4.0000,                 loss: nan
env4_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 166.8872s / 62703.2100 s
env0_first_0:                 episode reward: -3.3000,                 loss: nan
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0116
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
env2_first_0:                 episode reward: -3.3000,                 loss: nan
env2_second_0:                 episode reward: 3.3000,                 loss: nan
env3_first_0:                 episode reward: -3.3000,                 loss: nan
env3_second_0:                 episode reward: 3.3000,                 loss: nan
env4_first_0:                 episode reward: -3.3000,                 loss: nan
env4_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.2530s / 62867.4630 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0331
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0120
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
env2_first_0:                 episode reward: -3.8000,                 loss: nan
env2_second_0:                 episode reward: 3.8000,                 loss: nan
env3_first_0:                 episode reward: -3.8000,                 loss: nan
env3_second_0:                 episode reward: 3.8000,                 loss: nan
env4_first_0:                 episode reward: -3.8000,                 loss: nan
env4_second_0:                 episode reward: 3.8000,                 loss: nan
Score delta: 10.6, update the opponent.
Episode: 7081/10000 (70.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.5045s / 63031.9675 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0183
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
env2_first_0:                 episode reward: -1.3000,                 loss: nan
env2_second_0:                 episode reward: 1.3000,                 loss: nan
env3_first_0:                 episode reward: -1.3000,                 loss: nan
env3_second_0:                 episode reward: 1.3000,                 loss: nan
env4_first_0:                 episode reward: -1.3000,                 loss: nan
env4_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.6274s / 63195.5949 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0129
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
env2_first_0:                 episode reward: -0.4000,                 loss: nan
env2_second_0:                 episode reward: 0.4000,                 loss: nan
env3_first_0:                 episode reward: -0.4000,                 loss: nan
env3_second_0:                 episode reward: 0.4000,                 loss: nan
env4_first_0:                 episode reward: -0.4000,                 loss: nan
env4_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.7338s / 63359.3287 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0137
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
env2_first_0:                 episode reward: -1.8500,                 loss: nan
env2_second_0:                 episode reward: 1.8500,                 loss: nan
env3_first_0:                 episode reward: -1.6500,                 loss: nan
env3_second_0:                 episode reward: 1.6500,                 loss: nan
env4_first_0:                 episode reward: -1.8500,                 loss: nan
env4_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.9568s / 63524.2856 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0164
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
env2_first_0:                 episode reward: -1.8000,                 loss: nan
env2_second_0:                 episode reward: 1.8000,                 loss: nan
env3_first_0:                 episode reward: -1.8000,                 loss: nan
env3_second_0:                 episode reward: 1.8000,                 loss: nan
env4_first_0:                 episode reward: -1.8000,                 loss: nan
env4_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.4379s / 63689.7234 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0203
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
env2_first_0:                 episode reward: -1.1000,                 loss: nan
env2_second_0:                 episode reward: 1.1000,                 loss: nan
env3_first_0:                 episode reward: -0.7000,                 loss: nan
env3_second_0:                 episode reward: 0.7000,                 loss: nan
env4_first_0:                 episode reward: -0.7000,                 loss: nan
env4_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.9730s / 63853.6964 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0226
env0_second_0:                 episode reward: -5.0000,                 loss: 0.0407
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
env2_first_0:                 episode reward: 5.0000,                 loss: nan
env2_second_0:                 episode reward: -5.0000,                 loss: nan
env3_first_0:                 episode reward: 5.0000,                 loss: nan
env3_second_0:                 episode reward: -5.0000,                 loss: nan
env4_first_0:                 episode reward: 5.0000,                 loss: nan
env4_second_0:                 episode reward: -5.0000,                 loss: nan
Score delta: 10.6, update the opponent.
Episode: 7201/10000 (72.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.2922s / 64018.9887 s
env0_first_0:                 episode reward: 0.0500,                 loss: nan
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0252
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
env2_first_0:                 episode reward: -0.1500,                 loss: nan
env2_second_0:                 episode reward: 0.1500,                 loss: nan
env3_first_0:                 episode reward: -0.1500,                 loss: nan
env3_second_0:                 episode reward: 0.1500,                 loss: nan
env4_first_0:                 episode reward: 0.0500,                 loss: nan
env4_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.5971s / 64184.5857 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0506
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0206
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
env2_first_0:                 episode reward: -5.9000,                 loss: nan
env2_second_0:                 episode reward: 5.9000,                 loss: nan
env3_first_0:                 episode reward: -6.0000,                 loss: nan
env3_second_0:                 episode reward: 6.0000,                 loss: nan
env4_first_0:                 episode reward: -5.9500,                 loss: nan
env4_second_0:                 episode reward: 5.9500,                 loss: nan
Score delta: 10.4, update the opponent.
Episode: 7241/10000 (72.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.4659s / 64349.0516 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0309
env0_second_0:                 episode reward: 7.0000,                 loss: nan
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
env2_first_0:                 episode reward: -6.9000,                 loss: nan
env2_second_0:                 episode reward: 6.9000,                 loss: nan
env3_first_0:                 episode reward: -6.9500,                 loss: nan
env3_second_0:                 episode reward: 6.9500,                 loss: nan
env4_first_0:                 episode reward: -6.9500,                 loss: nan
env4_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.6425s / 64512.6941 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0182
env0_second_0:                 episode reward: 5.4000,                 loss: nan
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
env2_first_0:                 episode reward: -5.4000,                 loss: nan
env2_second_0:                 episode reward: 5.4000,                 loss: nan
env3_first_0:                 episode reward: -5.4000,                 loss: nan
env3_second_0:                 episode reward: 5.4000,                 loss: nan
env4_first_0:                 episode reward: -5.4000,                 loss: nan
env4_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.4622s / 64676.1564 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0148
env0_second_0:                 episode reward: 5.1000,                 loss: nan
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
env2_first_0:                 episode reward: -5.1000,                 loss: nan
env2_second_0:                 episode reward: 5.1000,                 loss: nan
env3_first_0:                 episode reward: -5.1000,                 loss: nan
env3_second_0:                 episode reward: 5.1000,                 loss: nan
env4_first_0:                 episode reward: -5.1000,                 loss: nan
env4_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.4613s / 64841.6177 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0174
env0_second_0:                 episode reward: 3.6000,                 loss: nan
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
env2_first_0:                 episode reward: -3.2000,                 loss: nan
env2_second_0:                 episode reward: 3.2000,                 loss: nan
env3_first_0:                 episode reward: -3.6500,                 loss: nan
env3_second_0:                 episode reward: 3.6500,                 loss: nan
env4_first_0:                 episode reward: -3.6500,                 loss: nan
env4_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.7096s / 65005.3272 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0219
env0_second_0:                 episode reward: 3.5500,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
env2_first_0:                 episode reward: -3.5500,                 loss: nan
env2_second_0:                 episode reward: 3.5500,                 loss: nan
env3_first_0:                 episode reward: -3.5500,                 loss: nan
env3_second_0:                 episode reward: 3.5500,                 loss: nan
env4_first_0:                 episode reward: -3.5500,                 loss: nan
env4_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.7479s / 65169.0751 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0262
env0_second_0:                 episode reward: 4.5500,                 loss: nan
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
env2_first_0:                 episode reward: -4.5500,                 loss: nan
env2_second_0:                 episode reward: 4.5500,                 loss: nan
env3_first_0:                 episode reward: -4.5500,                 loss: nan
env3_second_0:                 episode reward: 4.5500,                 loss: nan
env4_first_0:                 episode reward: -4.5500,                 loss: nan
env4_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.0279s / 65332.1030 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0237
env0_second_0:                 episode reward: 3.9500,                 loss: nan
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
env2_first_0:                 episode reward: -3.9500,                 loss: nan
env2_second_0:                 episode reward: 3.9500,                 loss: nan
env3_first_0:                 episode reward: -3.9500,                 loss: nan
env3_second_0:                 episode reward: 3.9500,                 loss: nan
env4_first_0:                 episode reward: -3.9500,                 loss: nan
env4_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.3646s / 65494.4676 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0243
env0_second_0:                 episode reward: 5.6000,                 loss: nan
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
env2_first_0:                 episode reward: -5.6000,                 loss: nan
env2_second_0:                 episode reward: 5.6000,                 loss: nan
env3_first_0:                 episode reward: -5.6000,                 loss: nan
env3_second_0:                 episode reward: 5.6000,                 loss: nan
env4_first_0:                 episode reward: -5.6000,                 loss: nan
env4_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.5129s / 65657.9805 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0217
env0_second_0:                 episode reward: 4.9500,                 loss: nan
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
env2_first_0:                 episode reward: -5.0000,                 loss: nan
env2_second_0:                 episode reward: 5.0000,                 loss: nan
env3_first_0:                 episode reward: -4.9000,                 loss: nan
env3_second_0:                 episode reward: 4.9000,                 loss: nan
env4_first_0:                 episode reward: -4.9000,                 loss: nan
env4_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.4630s / 65822.4436 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0176
env0_second_0:                 episode reward: 4.5000,                 loss: nan
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
env2_first_0:                 episode reward: -4.4000,                 loss: nan
env2_second_0:                 episode reward: 4.4000,                 loss: nan
env3_first_0:                 episode reward: -4.4500,                 loss: nan
env3_second_0:                 episode reward: 4.4500,                 loss: nan
env4_first_0:                 episode reward: -4.4000,                 loss: nan
env4_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.7990s / 65985.2426 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0176
env0_second_0:                 episode reward: -2.0500,                 loss: nan
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
env2_first_0:                 episode reward: 1.8000,                 loss: nan
env2_second_0:                 episode reward: -1.8000,                 loss: nan
env3_first_0:                 episode reward: 1.8000,                 loss: nan
env3_second_0:                 episode reward: -1.8000,                 loss: nan
env4_first_0:                 episode reward: 2.0500,                 loss: nan
env4_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.3877s / 66148.6303 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0154
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
env2_first_0:                 episode reward: 0.0500,                 loss: nan
env2_second_0:                 episode reward: -0.0500,                 loss: nan
env3_first_0:                 episode reward: 0.0500,                 loss: nan
env3_second_0:                 episode reward: -0.0500,                 loss: nan
env4_first_0:                 episode reward: 0.0500,                 loss: nan
env4_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.0770s / 66310.7073 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0143
env0_second_0:                 episode reward: 3.4000,                 loss: nan
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
env2_first_0:                 episode reward: -3.2000,                 loss: nan
env2_second_0:                 episode reward: 3.2000,                 loss: nan
env3_first_0:                 episode reward: -3.2000,                 loss: nan
env3_second_0:                 episode reward: 3.2000,                 loss: nan
env4_first_0:                 episode reward: -3.2000,                 loss: nan
env4_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.6884s / 66474.3957 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0143
env0_second_0:                 episode reward: 3.5000,                 loss: nan
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
env2_first_0:                 episode reward: -3.5000,                 loss: nan
env2_second_0:                 episode reward: 3.5000,                 loss: nan
env3_first_0:                 episode reward: -3.5000,                 loss: nan
env3_second_0:                 episode reward: 3.5000,                 loss: nan
env4_first_0:                 episode reward: -3.5000,                 loss: nan
env4_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.8102s / 66637.2059 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0144
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
env2_first_0:                 episode reward: -1.1000,                 loss: nan
env2_second_0:                 episode reward: 1.1000,                 loss: nan
env3_first_0:                 episode reward: -1.1000,                 loss: nan
env3_second_0:                 episode reward: 1.1000,                 loss: nan
env4_first_0:                 episode reward: -1.1000,                 loss: nan
env4_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.8629s / 66801.0688 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0146
env0_second_0:                 episode reward: -0.5500,                 loss: nan
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
env2_first_0:                 episode reward: 0.6500,                 loss: nan
env2_second_0:                 episode reward: -0.6500,                 loss: nan
env3_first_0:                 episode reward: 0.6500,                 loss: nan
env3_second_0:                 episode reward: -0.6500,                 loss: nan
env4_first_0:                 episode reward: 0.5500,                 loss: nan
env4_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 161.9273s / 66962.9960 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0151
env0_second_0:                 episode reward: -1.1500,                 loss: nan
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
env2_first_0:                 episode reward: 1.1500,                 loss: nan
env2_second_0:                 episode reward: -1.1500,                 loss: nan
env3_first_0:                 episode reward: 1.1500,                 loss: nan
env3_second_0:                 episode reward: -1.1500,                 loss: nan
env4_first_0:                 episode reward: 1.1500,                 loss: nan
env4_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.7441s / 67125.7401 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0155
env0_second_0:                 episode reward: -0.8000,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
env2_first_0:                 episode reward: 0.8000,                 loss: nan
env2_second_0:                 episode reward: -0.8000,                 loss: nan
env3_first_0:                 episode reward: 1.1000,                 loss: nan
env3_second_0:                 episode reward: -1.1000,                 loss: nan
env4_first_0:                 episode reward: 0.8000,                 loss: nan
env4_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.3907s / 67288.1308 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0158
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
env2_first_0:                 episode reward: 0.9000,                 loss: nan
env2_second_0:                 episode reward: -0.9000,                 loss: nan
env3_first_0:                 episode reward: 1.0500,                 loss: nan
env3_second_0:                 episode reward: -1.0500,                 loss: nan
env4_first_0:                 episode reward: 0.9500,                 loss: nan
env4_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 161.5627s / 67449.6936 s
env0_first_0:                 episode reward: 2.2000,                 loss: 0.0169
env0_second_0:                 episode reward: -2.2000,                 loss: nan
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
env2_first_0:                 episode reward: 2.2000,                 loss: nan
env2_second_0:                 episode reward: -2.2000,                 loss: nan
env3_first_0:                 episode reward: 2.2000,                 loss: nan
env3_second_0:                 episode reward: -2.2000,                 loss: nan
env4_first_0:                 episode reward: 2.2000,                 loss: nan
env4_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.9020s / 67613.5956 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0165
env0_second_0:                 episode reward: -1.9500,                 loss: nan
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
env2_first_0:                 episode reward: 1.9500,                 loss: nan
env2_second_0:                 episode reward: -1.9500,                 loss: nan
env3_first_0:                 episode reward: 1.9500,                 loss: nan
env3_second_0:                 episode reward: -1.9500,                 loss: nan
env4_first_0:                 episode reward: 1.9500,                 loss: nan
env4_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.3119s / 67776.9075 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0167
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
env2_first_0:                 episode reward: 0.6500,                 loss: nan
env2_second_0:                 episode reward: -0.6500,                 loss: nan
env3_first_0:                 episode reward: 0.6500,                 loss: nan
env3_second_0:                 episode reward: -0.6500,                 loss: nan
env4_first_0:                 episode reward: 0.6500,                 loss: nan
env4_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.2172s / 67939.1247 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0167
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
env2_first_0:                 episode reward: -1.6000,                 loss: nan
env2_second_0:                 episode reward: 1.6000,                 loss: nan
env3_first_0:                 episode reward: -1.6000,                 loss: nan
env3_second_0:                 episode reward: 1.6000,                 loss: nan
env4_first_0:                 episode reward: -1.4000,                 loss: nan
env4_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.3753s / 68102.5000 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0167
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
env2_first_0:                 episode reward: -0.2500,                 loss: nan
env2_second_0:                 episode reward: 0.2500,                 loss: nan
env3_first_0:                 episode reward: -0.2500,                 loss: nan
env3_second_0:                 episode reward: 0.2500,                 loss: nan
env4_first_0:                 episode reward: -0.1500,                 loss: nan
env4_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.1394s / 68266.6394 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0163
env0_second_0:                 episode reward: -1.5000,                 loss: nan
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
env2_first_0:                 episode reward: 1.4500,                 loss: nan
env2_second_0:                 episode reward: -1.4500,                 loss: nan
env3_first_0:                 episode reward: 1.5000,                 loss: nan
env3_second_0:                 episode reward: -1.5000,                 loss: nan
env4_first_0:                 episode reward: 1.4500,                 loss: nan
env4_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 165.1652s / 68431.8045 s
env0_first_0:                 episode reward: 2.0000,                 loss: 0.0166
env0_second_0:                 episode reward: -2.0000,                 loss: nan
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.0000,                 loss: nan
env2_second_0:                 episode reward: -2.0000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: 2.0000,                 loss: nan
env4_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 160.6849s / 68592.4894 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0165
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
env2_first_0:                 episode reward: -0.3000,                 loss: nan
env2_second_0:                 episode reward: 0.3000,                 loss: nan
env3_first_0:                 episode reward: -0.3000,                 loss: nan
env3_second_0:                 episode reward: 0.3000,                 loss: nan
env4_first_0:                 episode reward: -0.3000,                 loss: nan
env4_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.3657s / 68754.8551 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0157
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
env2_first_0:                 episode reward: -0.8000,                 loss: nan
env2_second_0:                 episode reward: 0.8000,                 loss: nan
env3_first_0:                 episode reward: -0.8000,                 loss: nan
env3_second_0:                 episode reward: 0.8000,                 loss: nan
env4_first_0:                 episode reward: -0.8000,                 loss: nan
env4_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.1973s / 68917.0524 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0155
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
env2_first_0:                 episode reward: -0.6000,                 loss: nan
env2_second_0:                 episode reward: 0.6000,                 loss: nan
env3_first_0:                 episode reward: -0.6000,                 loss: nan
env3_second_0:                 episode reward: 0.6000,                 loss: nan
env4_first_0:                 episode reward: -0.6000,                 loss: nan
env4_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 163.1256s / 69080.1780 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0157
env0_second_0:                 episode reward: -4.1000,                 loss: 0.0417
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
env2_first_0:                 episode reward: 4.1000,                 loss: nan
env2_second_0:                 episode reward: -4.1000,                 loss: nan
env3_first_0:                 episode reward: 4.1000,                 loss: nan
env3_second_0:                 episode reward: -4.1000,                 loss: nan
env4_first_0:                 episode reward: 4.1000,                 loss: nan
env4_second_0:                 episode reward: -4.1000,                 loss: nan
Score delta: 12.4, update the opponent.
Episode: 7841/10000 (78.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.8316s / 69243.0096 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0196
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
env2_first_0:                 episode reward: 3.7500,                 loss: nan
env2_second_0:                 episode reward: -3.7500,                 loss: nan
env3_first_0:                 episode reward: 3.7500,                 loss: nan
env3_second_0:                 episode reward: -3.7500,                 loss: nan
env4_first_0:                 episode reward: 3.7500,                 loss: nan
env4_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.2560s / 69405.2656 s
env0_first_0:                 episode reward: 7.5500,                 loss: nan
env0_second_0:                 episode reward: -7.5500,                 loss: 0.0123
env1_first_0:                 episode reward: 7.5500,                 loss: nan
env1_second_0:                 episode reward: -7.5500,                 loss: nan
env2_first_0:                 episode reward: 7.5500,                 loss: nan
env2_second_0:                 episode reward: -7.5500,                 loss: nan
env3_first_0:                 episode reward: 7.5500,                 loss: nan
env3_second_0:                 episode reward: -7.5500,                 loss: nan
env4_first_0:                 episode reward: 7.5500,                 loss: nan
env4_second_0:                 episode reward: -7.5500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 164.3950s / 69569.6606 s
env0_first_0:                 episode reward: 6.4000,                 loss: nan
env0_second_0:                 episode reward: -6.4000,                 loss: 0.0121
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
env2_first_0:                 episode reward: 6.4000,                 loss: nan
env2_second_0:                 episode reward: -6.4000,                 loss: nan
env3_first_0:                 episode reward: 6.4000,                 loss: nan
env3_second_0:                 episode reward: -6.4000,                 loss: nan
env4_first_0:                 episode reward: 6.4000,                 loss: nan
env4_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 161.3408s / 69731.0014 s
env0_first_0:                 episode reward: 6.5000,                 loss: nan
env0_second_0:                 episode reward: -6.5000,                 loss: 0.0129
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
env2_first_0:                 episode reward: 6.5000,                 loss: nan
env2_second_0:                 episode reward: -6.5000,                 loss: nan
env3_first_0:                 episode reward: 6.5000,                 loss: nan
env3_second_0:                 episode reward: -6.5000,                 loss: nan
env4_first_0:                 episode reward: 6.5000,                 loss: nan
env4_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 161.3436s / 69892.3451 s
env0_first_0:                 episode reward: 7.2500,                 loss: nan
env0_second_0:                 episode reward: -7.2500,                 loss: 0.0128
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
env2_first_0:                 episode reward: 7.2500,                 loss: nan
env2_second_0:                 episode reward: -7.2500,                 loss: nan
env3_first_0:                 episode reward: 7.2500,                 loss: nan
env3_second_0:                 episode reward: -7.2500,                 loss: nan
env4_first_0:                 episode reward: 7.2500,                 loss: nan
env4_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 159.9761s / 70052.3212 s
env0_first_0:                 episode reward: 7.0500,                 loss: nan
env0_second_0:                 episode reward: -7.0500,                 loss: 0.0132
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
env2_first_0:                 episode reward: 7.0500,                 loss: nan
env2_second_0:                 episode reward: -7.0500,                 loss: nan
env3_first_0:                 episode reward: 7.0500,                 loss: nan
env3_second_0:                 episode reward: -7.0500,                 loss: nan
env4_first_0:                 episode reward: 7.0500,                 loss: nan
env4_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 161.0830s / 70213.4042 s
env0_first_0:                 episode reward: 7.0000,                 loss: nan
env0_second_0:                 episode reward: -7.0000,                 loss: 0.0131
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
env2_first_0:                 episode reward: 7.0000,                 loss: nan
env2_second_0:                 episode reward: -7.0000,                 loss: nan
env3_first_0:                 episode reward: 7.0000,                 loss: nan
env3_second_0:                 episode reward: -7.0000,                 loss: nan
env4_first_0:                 episode reward: 7.0000,                 loss: nan
env4_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 160.4395s / 70373.8437 s
env0_first_0:                 episode reward: 7.0000,                 loss: nan
env0_second_0:                 episode reward: -7.0000,                 loss: 0.0124
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
env2_first_0:                 episode reward: 7.0000,                 loss: nan
env2_second_0:                 episode reward: -7.0000,                 loss: nan
env3_first_0:                 episode reward: 7.0000,                 loss: nan
env3_second_0:                 episode reward: -7.0000,                 loss: nan
env4_first_0:                 episode reward: 7.0000,                 loss: nan
env4_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 159.6485s / 70533.4922 s
env0_first_0:                 episode reward: 7.0000,                 loss: nan
env0_second_0:                 episode reward: -7.0000,                 loss: 0.0126
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
env2_first_0:                 episode reward: 7.0000,                 loss: nan
env2_second_0:                 episode reward: -7.0000,                 loss: nan
env3_first_0:                 episode reward: 7.0000,                 loss: nan
env3_second_0:                 episode reward: -7.0000,                 loss: nan
env4_first_0:                 episode reward: 7.0000,                 loss: nan
env4_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 160.1671s / 70693.6593 s
env0_first_0:                 episode reward: 2.3000,                 loss: nan
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0139
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
env2_first_0:                 episode reward: 2.3000,                 loss: nan
env2_second_0:                 episode reward: -2.3000,                 loss: nan
env3_first_0:                 episode reward: 2.3000,                 loss: nan
env3_second_0:                 episode reward: -2.3000,                 loss: nan
env4_first_0:                 episode reward: 2.3000,                 loss: nan
env4_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 161.0395s / 70854.6988 s
env0_first_0:                 episode reward: 0.6000,                 loss: nan
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0134
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
env2_first_0:                 episode reward: -0.1500,                 loss: nan
env2_second_0:                 episode reward: 0.1500,                 loss: nan
env3_first_0:                 episode reward: -0.1500,                 loss: nan
env3_second_0:                 episode reward: 0.1500,                 loss: nan
env4_first_0:                 episode reward: 0.6000,                 loss: nan
env4_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 158.3205s / 71013.0193 s
env0_first_0:                 episode reward: 5.7000,                 loss: nan
env0_second_0:                 episode reward: -5.7000,                 loss: 0.0135
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
env2_first_0:                 episode reward: 5.7000,                 loss: nan
env2_second_0:                 episode reward: -5.7000,                 loss: nan
env3_first_0:                 episode reward: 5.7000,                 loss: nan
env3_second_0:                 episode reward: -5.7000,                 loss: nan
env4_first_0:                 episode reward: 5.7000,                 loss: nan
env4_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 156.9477s / 71169.9670 s
env0_first_0:                 episode reward: 6.5500,                 loss: nan
env0_second_0:                 episode reward: -6.5500,                 loss: 0.0141
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
env2_first_0:                 episode reward: 6.5500,                 loss: nan
env2_second_0:                 episode reward: -6.5500,                 loss: nan
env3_first_0:                 episode reward: 6.5500,                 loss: nan
env3_second_0:                 episode reward: -6.5500,                 loss: nan
env4_first_0:                 episode reward: 6.5500,                 loss: nan
env4_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 159.9175s / 71329.8844 s
env0_first_0:                 episode reward: 5.1000,                 loss: nan
env0_second_0:                 episode reward: -5.1000,                 loss: 0.0146
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
env2_first_0:                 episode reward: 5.1000,                 loss: nan
env2_second_0:                 episode reward: -5.1000,                 loss: nan
env3_first_0:                 episode reward: 5.1000,                 loss: nan
env3_second_0:                 episode reward: -5.1000,                 loss: nan
env4_first_0:                 episode reward: 5.1000,                 loss: nan
env4_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 160.8134s / 71490.6978 s
env0_first_0:                 episode reward: 6.1500,                 loss: nan
env0_second_0:                 episode reward: -6.1500,                 loss: 0.0159
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
env2_first_0:                 episode reward: 6.1500,                 loss: nan
env2_second_0:                 episode reward: -6.1500,                 loss: nan
env3_first_0:                 episode reward: 6.1500,                 loss: nan
env3_second_0:                 episode reward: -6.1500,                 loss: nan
env4_first_0:                 episode reward: 6.1500,                 loss: nan
env4_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 159.6338s / 71650.3316 s
env0_first_0:                 episode reward: 7.0000,                 loss: nan
env0_second_0:                 episode reward: -7.0000,                 loss: 0.0143
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
env2_first_0:                 episode reward: 7.0000,                 loss: nan
env2_second_0:                 episode reward: -7.0000,                 loss: nan
env3_first_0:                 episode reward: 7.0000,                 loss: nan
env3_second_0:                 episode reward: -7.0000,                 loss: nan
env4_first_0:                 episode reward: 7.0000,                 loss: nan
env4_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 160.0984s / 71810.4300 s
env0_first_0:                 episode reward: 5.8000,                 loss: nan
env0_second_0:                 episode reward: -5.8000,                 loss: 0.0129
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
env2_first_0:                 episode reward: 5.8000,                 loss: nan
env2_second_0:                 episode reward: -5.8000,                 loss: nan
env3_first_0:                 episode reward: 5.8000,                 loss: nan
env3_second_0:                 episode reward: -5.8000,                 loss: nan
env4_first_0:                 episode reward: 5.8000,                 loss: nan
env4_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 162.0101s / 71972.4401 s
env0_first_0:                 episode reward: 6.0500,                 loss: nan
env0_second_0:                 episode reward: -6.0500,                 loss: 0.0127
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
env2_first_0:                 episode reward: 6.0500,                 loss: nan
env2_second_0:                 episode reward: -6.0500,                 loss: nan
env3_first_0:                 episode reward: 6.0500,                 loss: nan
env3_second_0:                 episode reward: -6.0500,                 loss: nan
env4_first_0:                 episode reward: 6.0500,                 loss: nan
env4_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 160.5567s / 72132.9968 s
env0_first_0:                 episode reward: 4.2000,                 loss: nan
env0_second_0:                 episode reward: -4.2000,                 loss: 0.0115
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
env2_first_0:                 episode reward: 3.7000,                 loss: nan
env2_second_0:                 episode reward: -3.7000,                 loss: nan
env3_first_0:                 episode reward: 4.4000,                 loss: nan
env3_second_0:                 episode reward: -4.4000,                 loss: nan
env4_first_0:                 episode reward: 4.2000,                 loss: nan
env4_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 160.9716s / 72293.9684 s
env0_first_0:                 episode reward: 6.8500,                 loss: nan
env0_second_0:                 episode reward: -6.8500,                 loss: 0.0104
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
env2_first_0:                 episode reward: 6.8500,                 loss: nan
env2_second_0:                 episode reward: -6.8500,                 loss: nan
env3_first_0:                 episode reward: 6.8500,                 loss: nan
env3_second_0:                 episode reward: -6.8500,                 loss: nan
env4_first_0:                 episode reward: 6.8500,                 loss: nan
env4_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 156.6525s / 72450.6209 s
env0_first_0:                 episode reward: 5.1500,                 loss: nan
env0_second_0:                 episode reward: -5.1500,                 loss: 0.0114
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
env2_first_0:                 episode reward: 5.1500,                 loss: nan
env2_second_0:                 episode reward: -5.1500,                 loss: nan
env3_first_0:                 episode reward: 5.1500,                 loss: nan
env3_second_0:                 episode reward: -5.1500,                 loss: nan
env4_first_0:                 episode reward: 5.1500,                 loss: nan
env4_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 157.4623s / 72608.0832 s
env0_first_0:                 episode reward: 4.9500,                 loss: nan
env0_second_0:                 episode reward: -4.9500,                 loss: 0.0122
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
env2_first_0:                 episode reward: 4.9500,                 loss: nan
env2_second_0:                 episode reward: -4.9500,                 loss: nan
env3_first_0:                 episode reward: 4.8000,                 loss: nan
env3_second_0:                 episode reward: -4.8000,                 loss: nan
env4_first_0:                 episode reward: 4.9500,                 loss: nan
env4_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 157.6698s / 72765.7530 s
env0_first_0:                 episode reward: 5.6500,                 loss: nan
env0_second_0:                 episode reward: -5.6500,                 loss: 0.0121
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
env2_first_0:                 episode reward: 5.6500,                 loss: nan
env2_second_0:                 episode reward: -5.6500,                 loss: nan
env3_first_0:                 episode reward: 5.6500,                 loss: nan
env3_second_0:                 episode reward: -5.6500,                 loss: nan
env4_first_0:                 episode reward: 5.6500,                 loss: nan
env4_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 157.3424s / 72923.0954 s
env0_first_0:                 episode reward: 3.8000,                 loss: nan
env0_second_0:                 episode reward: -3.8000,                 loss: 0.0104
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
env2_first_0:                 episode reward: 3.8000,                 loss: nan
env2_second_0:                 episode reward: -3.8000,                 loss: nan
env3_first_0:                 episode reward: 3.9000,                 loss: nan
env3_second_0:                 episode reward: -3.9000,                 loss: nan
env4_first_0:                 episode reward: 3.8000,                 loss: nan
env4_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 157.3548s / 73080.4502 s
env0_first_0:                 episode reward: 5.1500,                 loss: nan
env0_second_0:                 episode reward: -5.1500,                 loss: 0.0103
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
env2_first_0:                 episode reward: 5.1500,                 loss: nan
env2_second_0:                 episode reward: -5.1500,                 loss: nan
env3_first_0:                 episode reward: 5.1500,                 loss: nan
env3_second_0:                 episode reward: -5.1500,                 loss: nan
env4_first_0:                 episode reward: 5.1500,                 loss: nan
env4_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 155.6898s / 73236.1399 s
env0_first_0:                 episode reward: 6.1000,                 loss: nan
env0_second_0:                 episode reward: -6.1000,                 loss: 0.0100
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
env2_first_0:                 episode reward: 6.3500,                 loss: nan
env2_second_0:                 episode reward: -6.3500,                 loss: nan
env3_first_0:                 episode reward: 6.1000,                 loss: nan
env3_second_0:                 episode reward: -6.1000,                 loss: nan
env4_first_0:                 episode reward: 6.3500,                 loss: nan
env4_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 157.2701s / 73393.4100 s
env0_first_0:                 episode reward: 6.3500,                 loss: nan
env0_second_0:                 episode reward: -6.3500,                 loss: 0.0098
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
env2_first_0:                 episode reward: 6.3500,                 loss: nan
env2_second_0:                 episode reward: -6.3500,                 loss: nan
env3_first_0:                 episode reward: 6.3500,                 loss: nan
env3_second_0:                 episode reward: -6.3500,                 loss: nan
env4_first_0:                 episode reward: 6.3500,                 loss: nan
env4_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 157.3185s / 73550.7284 s
env0_first_0:                 episode reward: 4.0500,                 loss: nan
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0099
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
env2_first_0:                 episode reward: 4.0500,                 loss: nan
env2_second_0:                 episode reward: -4.0500,                 loss: nan
env3_first_0:                 episode reward: 4.0500,                 loss: nan
env3_second_0:                 episode reward: -4.0500,                 loss: nan
env4_first_0:                 episode reward: 4.0500,                 loss: nan
env4_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 156.3791s / 73707.1075 s
env0_first_0:                 episode reward: 1.3000,                 loss: nan
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0112
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
env2_first_0:                 episode reward: 1.3000,                 loss: nan
env2_second_0:                 episode reward: -1.3000,                 loss: nan
env3_first_0:                 episode reward: 1.3000,                 loss: nan
env3_second_0:                 episode reward: -1.3000,                 loss: nan
env4_first_0:                 episode reward: 1.3000,                 loss: nan
env4_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 155.9355s / 73863.0430 s
env0_first_0:                 episode reward: 4.9000,                 loss: nan
env0_second_0:                 episode reward: -4.9000,                 loss: 0.0123
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
env2_first_0:                 episode reward: 4.9000,                 loss: nan
env2_second_0:                 episode reward: -4.9000,                 loss: nan
env3_first_0:                 episode reward: 5.1000,                 loss: nan
env3_second_0:                 episode reward: -5.1000,                 loss: nan
env4_first_0:                 episode reward: 5.1000,                 loss: nan
env4_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 155.9682s / 74019.0112 s
env0_first_0:                 episode reward: 3.9500,                 loss: nan
env0_second_0:                 episode reward: -3.9500,                 loss: 0.0121
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
env2_first_0:                 episode reward: 3.9500,                 loss: nan
env2_second_0:                 episode reward: -3.9500,                 loss: nan
env3_first_0:                 episode reward: 3.7000,                 loss: nan
env3_second_0:                 episode reward: -3.7000,                 loss: nan
env4_first_0:                 episode reward: 3.7000,                 loss: nan
env4_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 153.6592s / 74172.6705 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0125
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
env2_first_0:                 episode reward: 3.7500,                 loss: nan
env2_second_0:                 episode reward: -3.7500,                 loss: nan
env3_first_0:                 episode reward: 3.7500,                 loss: nan
env3_second_0:                 episode reward: -3.7500,                 loss: nan
env4_first_0:                 episode reward: 3.7500,                 loss: nan
env4_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 157.6666s / 74330.3370 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0131
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
env2_first_0:                 episode reward: 3.4000,                 loss: nan
env2_second_0:                 episode reward: -3.4000,                 loss: nan
env3_first_0:                 episode reward: 3.4000,                 loss: nan
env3_second_0:                 episode reward: -3.4000,                 loss: nan
env4_first_0:                 episode reward: 3.4000,                 loss: nan
env4_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 153.9296s / 74484.2666 s
env0_first_0:                 episode reward: 4.9000,                 loss: nan
env0_second_0:                 episode reward: -4.9000,                 loss: 0.0137
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
env2_first_0:                 episode reward: 4.9000,                 loss: nan
env2_second_0:                 episode reward: -4.9000,                 loss: nan
env3_first_0:                 episode reward: 4.9000,                 loss: nan
env3_second_0:                 episode reward: -4.9000,                 loss: nan
env4_first_0:                 episode reward: 4.9000,                 loss: nan
env4_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 154.3893s / 74638.6559 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0141
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
env2_first_0:                 episode reward: 3.3500,                 loss: nan
env2_second_0:                 episode reward: -3.3500,                 loss: nan
env3_first_0:                 episode reward: 3.3500,                 loss: nan
env3_second_0:                 episode reward: -3.3500,                 loss: nan
env4_first_0:                 episode reward: 3.7000,                 loss: nan
env4_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.7864s / 74791.4423 s
env0_first_0:                 episode reward: 3.9500,                 loss: nan
env0_second_0:                 episode reward: -3.9500,                 loss: 0.0138
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
env2_first_0:                 episode reward: 3.9500,                 loss: nan
env2_second_0:                 episode reward: -3.9500,                 loss: nan
env3_first_0:                 episode reward: 3.9500,                 loss: nan
env3_second_0:                 episode reward: -3.9500,                 loss: nan
env4_first_0:                 episode reward: 3.9500,                 loss: nan
env4_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 155.4213s / 74946.8637 s
env0_first_0:                 episode reward: 4.5500,                 loss: nan
env0_second_0:                 episode reward: -4.5500,                 loss: 0.0136
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
env2_first_0:                 episode reward: 4.4500,                 loss: nan
env2_second_0:                 episode reward: -4.4500,                 loss: nan
env3_first_0:                 episode reward: 4.5500,                 loss: nan
env3_second_0:                 episode reward: -4.5500,                 loss: nan
env4_first_0:                 episode reward: 4.4500,                 loss: nan
env4_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 154.6999s / 75101.5635 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0135
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
env2_first_0:                 episode reward: 3.5500,                 loss: nan
env2_second_0:                 episode reward: -3.5500,                 loss: nan
env3_first_0:                 episode reward: 3.7500,                 loss: nan
env3_second_0:                 episode reward: -3.7500,                 loss: nan
env4_first_0:                 episode reward: 3.6000,                 loss: nan
env4_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 154.1551s / 75255.7186 s
env0_first_0:                 episode reward: -0.4500,                 loss: nan
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0133
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
env2_first_0:                 episode reward: -0.8000,                 loss: nan
env2_second_0:                 episode reward: 0.8000,                 loss: nan
env3_first_0:                 episode reward: -0.6500,                 loss: nan
env3_second_0:                 episode reward: 0.6500,                 loss: nan
env4_first_0:                 episode reward: -0.4000,                 loss: nan
env4_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 154.1287s / 75409.8473 s
env0_first_0:                 episode reward: 0.7000,                 loss: nan
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0133
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
env2_first_0:                 episode reward: 0.7000,                 loss: nan
env2_second_0:                 episode reward: -0.7000,                 loss: nan
env3_first_0:                 episode reward: 1.1000,                 loss: nan
env3_second_0:                 episode reward: -1.1000,                 loss: nan
env4_first_0:                 episode reward: 0.7000,                 loss: nan
env4_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.9096s / 75562.7568 s
env0_first_0:                 episode reward: 0.7500,                 loss: nan
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0143
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
env2_first_0:                 episode reward: 0.5500,                 loss: nan
env2_second_0:                 episode reward: -0.5500,                 loss: nan
env3_first_0:                 episode reward: 0.7500,                 loss: nan
env3_second_0:                 episode reward: -0.7500,                 loss: nan
env4_first_0:                 episode reward: 0.7500,                 loss: nan
env4_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 154.6982s / 75717.4550 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0148
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
env2_first_0:                 episode reward: -0.3500,                 loss: nan
env2_second_0:                 episode reward: 0.3500,                 loss: nan
env3_first_0:                 episode reward: -0.3500,                 loss: nan
env3_second_0:                 episode reward: 0.3500,                 loss: nan
env4_first_0:                 episode reward: -0.3500,                 loss: nan
env4_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.9900s / 75868.4450 s
env0_first_0:                 episode reward: -0.2500,                 loss: nan
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0147
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
env2_first_0:                 episode reward: -0.5500,                 loss: nan
env2_second_0:                 episode reward: 0.5500,                 loss: nan
env3_first_0:                 episode reward: -0.5000,                 loss: nan
env3_second_0:                 episode reward: 0.5000,                 loss: nan
env4_first_0:                 episode reward: -0.3000,                 loss: nan
env4_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.0114s / 76020.4564 s
env0_first_0:                 episode reward: -2.1500,                 loss: nan
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0143
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
env2_first_0:                 episode reward: -2.1500,                 loss: nan
env2_second_0:                 episode reward: 2.1500,                 loss: nan
env3_first_0:                 episode reward: -2.1500,                 loss: nan
env3_second_0:                 episode reward: 2.1500,                 loss: nan
env4_first_0:                 episode reward: -2.1500,                 loss: nan
env4_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.6282s / 76173.0845 s
env0_first_0:                 episode reward: -2.3000,                 loss: nan
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0145
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
env2_first_0:                 episode reward: -2.3500,                 loss: nan
env2_second_0:                 episode reward: 2.3500,                 loss: nan
env3_first_0:                 episode reward: -2.3000,                 loss: nan
env3_second_0:                 episode reward: 2.3000,                 loss: nan
env4_first_0:                 episode reward: -2.3000,                 loss: nan
env4_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.1472s / 76324.2317 s
env0_first_0:                 episode reward: -1.4500,                 loss: nan
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0141
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
env2_first_0:                 episode reward: -1.4500,                 loss: nan
env2_second_0:                 episode reward: 1.4500,                 loss: nan
env3_first_0:                 episode reward: -1.4500,                 loss: nan
env3_second_0:                 episode reward: 1.4500,                 loss: nan
env4_first_0:                 episode reward: -1.4500,                 loss: nan
env4_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.5853s / 76475.8170 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0142
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
env2_first_0:                 episode reward: -0.3500,                 loss: nan
env2_second_0:                 episode reward: 0.3500,                 loss: nan
env3_first_0:                 episode reward: -0.3500,                 loss: nan
env3_second_0:                 episode reward: 0.3500,                 loss: nan
env4_first_0:                 episode reward: -0.3500,                 loss: nan
env4_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.4804s / 76627.2975 s
env0_first_0:                 episode reward: -1.6000,                 loss: nan
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0137
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
env2_first_0:                 episode reward: -1.5500,                 loss: nan
env2_second_0:                 episode reward: 1.5500,                 loss: nan
env3_first_0:                 episode reward: -1.7000,                 loss: nan
env3_second_0:                 episode reward: 1.7000,                 loss: nan
env4_first_0:                 episode reward: -1.7500,                 loss: nan
env4_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.8092s / 76779.1066 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0436
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0142
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
env2_first_0:                 episode reward: -3.5500,                 loss: nan
env2_second_0:                 episode reward: 3.5500,                 loss: nan
env3_first_0:                 episode reward: -3.5500,                 loss: nan
env3_second_0:                 episode reward: 3.5500,                 loss: nan
env4_first_0:                 episode reward: -3.5500,                 loss: nan
env4_second_0:                 episode reward: 3.5500,                 loss: nan
Score delta: 10.2, update the opponent.
Episode: 8821/10000 (88.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 153.2048s / 76932.3114 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0307
env0_second_0:                 episode reward: 4.9500,                 loss: nan
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
env2_first_0:                 episode reward: -5.3000,                 loss: nan
env2_second_0:                 episode reward: 5.3000,                 loss: nan
env3_first_0:                 episode reward: -4.9500,                 loss: nan
env3_second_0:                 episode reward: 4.9500,                 loss: nan
env4_first_0:                 episode reward: -4.9500,                 loss: nan
env4_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.0431s / 77083.3545 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0188
env0_second_0:                 episode reward: 2.5000,                 loss: nan
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
env2_first_0:                 episode reward: -2.5000,                 loss: nan
env2_second_0:                 episode reward: 2.5000,                 loss: nan
env3_first_0:                 episode reward: -2.5000,                 loss: nan
env3_second_0:                 episode reward: 2.5000,                 loss: nan
env4_first_0:                 episode reward: -2.5000,                 loss: nan
env4_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.7297s / 77235.0842 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0162
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
env2_first_0:                 episode reward: -1.3500,                 loss: nan
env2_second_0:                 episode reward: 1.3500,                 loss: nan
env3_first_0:                 episode reward: -1.3500,                 loss: nan
env3_second_0:                 episode reward: 1.3500,                 loss: nan
env4_first_0:                 episode reward: -1.3500,                 loss: nan
env4_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 153.2000s / 77388.2842 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0190
env0_second_0:                 episode reward: -1.2000,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
env2_first_0:                 episode reward: 1.2000,                 loss: nan
env2_second_0:                 episode reward: -1.2000,                 loss: nan
env3_first_0:                 episode reward: 1.2000,                 loss: nan
env3_second_0:                 episode reward: -1.2000,                 loss: nan
env4_first_0:                 episode reward: 1.2000,                 loss: nan
env4_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.0209s / 77539.3051 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0218
env0_second_0:                 episode reward: 3.9000,                 loss: nan
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
env2_first_0:                 episode reward: -4.1000,                 loss: nan
env2_second_0:                 episode reward: 4.1000,                 loss: nan
env3_first_0:                 episode reward: -3.9000,                 loss: nan
env3_second_0:                 episode reward: 3.9000,                 loss: nan
env4_first_0:                 episode reward: -3.9000,                 loss: nan
env4_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.5628s / 77691.8680 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0211
env0_second_0:                 episode reward: 4.6000,                 loss: nan
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
env2_first_0:                 episode reward: -4.6000,                 loss: nan
env2_second_0:                 episode reward: 4.6000,                 loss: nan
env3_first_0:                 episode reward: -4.6000,                 loss: nan
env3_second_0:                 episode reward: 4.6000,                 loss: nan
env4_first_0:                 episode reward: -4.6000,                 loss: nan
env4_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.1843s / 77843.0523 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0212
env0_second_0:                 episode reward: 5.4000,                 loss: nan
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
env2_first_0:                 episode reward: -5.4000,                 loss: nan
env2_second_0:                 episode reward: 5.4000,                 loss: nan
env3_first_0:                 episode reward: -5.3000,                 loss: nan
env3_second_0:                 episode reward: 5.3000,                 loss: nan
env4_first_0:                 episode reward: -5.3000,                 loss: nan
env4_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.9044s / 77995.9567 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.0208
env0_second_0:                 episode reward: 3.1500,                 loss: nan
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
env2_first_0:                 episode reward: -3.1500,                 loss: nan
env2_second_0:                 episode reward: 3.1500,                 loss: nan
env3_first_0:                 episode reward: -3.1500,                 loss: nan
env3_second_0:                 episode reward: 3.1500,                 loss: nan
env4_first_0:                 episode reward: -3.1500,                 loss: nan
env4_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.3604s / 78148.3171 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0189
env0_second_0:                 episode reward: 3.4000,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
env2_first_0:                 episode reward: -3.4000,                 loss: nan
env2_second_0:                 episode reward: 3.4000,                 loss: nan
env3_first_0:                 episode reward: -3.4000,                 loss: nan
env3_second_0:                 episode reward: 3.4000,                 loss: nan
env4_first_0:                 episode reward: -3.4000,                 loss: nan
env4_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 148.9880s / 78297.3051 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0186
env0_second_0:                 episode reward: 2.7000,                 loss: nan
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
env2_first_0:                 episode reward: -2.7000,                 loss: nan
env2_second_0:                 episode reward: 2.7000,                 loss: nan
env3_first_0:                 episode reward: -2.7000,                 loss: nan
env3_second_0:                 episode reward: 2.7000,                 loss: nan
env4_first_0:                 episode reward: -2.7000,                 loss: nan
env4_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.3084s / 78446.6136 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0153
env0_second_0:                 episode reward: 2.0500,                 loss: nan
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
env2_first_0:                 episode reward: -2.0500,                 loss: nan
env2_second_0:                 episode reward: 2.0500,                 loss: nan
env3_first_0:                 episode reward: -2.0500,                 loss: nan
env3_second_0:                 episode reward: 2.0500,                 loss: nan
env4_first_0:                 episode reward: -2.0500,                 loss: nan
env4_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.1467s / 78596.7603 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0125
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
env2_first_0:                 episode reward: -0.8000,                 loss: nan
env2_second_0:                 episode reward: 0.8000,                 loss: nan
env3_first_0:                 episode reward: -0.8000,                 loss: nan
env3_second_0:                 episode reward: 0.8000,                 loss: nan
env4_first_0:                 episode reward: -0.8000,                 loss: nan
env4_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 154.4250s / 78751.1852 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0124
env0_second_0:                 episode reward: -0.7000,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
env2_first_0:                 episode reward: 0.7000,                 loss: nan
env2_second_0:                 episode reward: -0.7000,                 loss: nan
env3_first_0:                 episode reward: 0.7000,                 loss: nan
env3_second_0:                 episode reward: -0.7000,                 loss: nan
env4_first_0:                 episode reward: 0.7000,                 loss: nan
env4_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.0107s / 78902.1959 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0130
env0_second_0:                 episode reward: -1.6500,                 loss: nan
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
env2_first_0:                 episode reward: 1.4500,                 loss: nan
env2_second_0:                 episode reward: -1.4500,                 loss: nan
env3_first_0:                 episode reward: 1.6500,                 loss: nan
env3_second_0:                 episode reward: -1.6500,                 loss: nan
env4_first_0:                 episode reward: 1.4500,                 loss: nan
env4_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.7088s / 79052.9047 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0129
env0_second_0:                 episode reward: -1.1500,                 loss: nan
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
env2_first_0:                 episode reward: 1.1500,                 loss: nan
env2_second_0:                 episode reward: -1.1500,                 loss: nan
env3_first_0:                 episode reward: 1.6500,                 loss: nan
env3_second_0:                 episode reward: -1.6500,                 loss: nan
env4_first_0:                 episode reward: 1.1500,                 loss: nan
env4_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.1565s / 79204.0612 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0126
env0_second_0:                 episode reward: -1.8500,                 loss: nan
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
env2_first_0:                 episode reward: 1.8500,                 loss: nan
env2_second_0:                 episode reward: -1.8500,                 loss: nan
env3_first_0:                 episode reward: 1.8500,                 loss: nan
env3_second_0:                 episode reward: -1.8500,                 loss: nan
env4_first_0:                 episode reward: 1.8500,                 loss: nan
env4_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.2856s / 79354.3468 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.0120
env0_second_0:                 episode reward: -2.8500,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
env2_first_0:                 episode reward: 3.0000,                 loss: nan
env2_second_0:                 episode reward: -3.0000,                 loss: nan
env3_first_0:                 episode reward: 3.0000,                 loss: nan
env3_second_0:                 episode reward: -3.0000,                 loss: nan
env4_first_0:                 episode reward: 2.7500,                 loss: nan
env4_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.3236s / 79505.6704 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0114
env0_second_0:                 episode reward: -6.2000,                 loss: 0.0387
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
env2_first_0:                 episode reward: 6.4000,                 loss: nan
env2_second_0:                 episode reward: -6.4000,                 loss: nan
env3_first_0:                 episode reward: 6.4000,                 loss: nan
env3_second_0:                 episode reward: -6.4000,                 loss: nan
env4_first_0:                 episode reward: 6.4000,                 loss: nan
env4_second_0:                 episode reward: -6.4000,                 loss: nan
Score delta: 11.0, update the opponent.
Episode: 9181/10000 (91.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.3511s / 79656.0215 s
env0_first_0:                 episode reward: 7.3500,                 loss: nan
env0_second_0:                 episode reward: -7.3500,                 loss: 0.0232
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
env2_first_0:                 episode reward: 7.4500,                 loss: nan
env2_second_0:                 episode reward: -7.4500,                 loss: nan
env3_first_0:                 episode reward: 7.2500,                 loss: nan
env3_second_0:                 episode reward: -7.2500,                 loss: nan
env4_first_0:                 episode reward: 7.3500,                 loss: nan
env4_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.7579s / 79805.7794 s
env0_first_0:                 episode reward: 7.9000,                 loss: nan
env0_second_0:                 episode reward: -7.9000,                 loss: 0.0165
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
env2_first_0:                 episode reward: 7.9000,                 loss: nan
env2_second_0:                 episode reward: -7.9000,                 loss: nan
env3_first_0:                 episode reward: 7.9000,                 loss: nan
env3_second_0:                 episode reward: -7.9000,                 loss: nan
env4_first_0:                 episode reward: 7.9000,                 loss: nan
env4_second_0:                 episode reward: -7.9000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.0369s / 79954.8163 s
env0_first_0:                 episode reward: 7.9000,                 loss: nan
env0_second_0:                 episode reward: -7.9000,                 loss: 0.0151
env1_first_0:                 episode reward: 7.1500,                 loss: nan
env1_second_0:                 episode reward: -7.1500,                 loss: nan
env2_first_0:                 episode reward: 7.1500,                 loss: nan
env2_second_0:                 episode reward: -7.1500,                 loss: nan
env3_first_0:                 episode reward: 7.1500,                 loss: nan
env3_second_0:                 episode reward: -7.1500,                 loss: nan
env4_first_0:                 episode reward: 7.1500,                 loss: nan
env4_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.8757s / 80105.6919 s
env0_first_0:                 episode reward: 7.6000,                 loss: nan
env0_second_0:                 episode reward: -7.6000,                 loss: 0.0171
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
env2_first_0:                 episode reward: 7.3500,                 loss: nan
env2_second_0:                 episode reward: -7.3500,                 loss: nan
env3_first_0:                 episode reward: 7.6000,                 loss: nan
env3_second_0:                 episode reward: -7.6000,                 loss: nan
env4_first_0:                 episode reward: 7.3500,                 loss: nan
env4_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.1940s / 80257.8859 s
env0_first_0:                 episode reward: 4.7000,                 loss: nan
env0_second_0:                 episode reward: -4.7000,                 loss: 0.0180
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
env2_first_0:                 episode reward: 4.7000,                 loss: nan
env2_second_0:                 episode reward: -4.7000,                 loss: nan
env3_first_0:                 episode reward: 4.7000,                 loss: nan
env3_second_0:                 episode reward: -4.7000,                 loss: nan
env4_first_0:                 episode reward: 4.7000,                 loss: nan
env4_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 153.9369s / 80411.8228 s
env0_first_0:                 episode reward: 5.5000,                 loss: nan
env0_second_0:                 episode reward: -5.5000,                 loss: 0.0201
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
env2_first_0:                 episode reward: 5.6500,                 loss: nan
env2_second_0:                 episode reward: -5.6500,                 loss: nan
env3_first_0:                 episode reward: 5.3500,                 loss: nan
env3_second_0:                 episode reward: -5.3500,                 loss: nan
env4_first_0:                 episode reward: 5.5000,                 loss: nan
env4_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.5990s / 80563.4219 s
env0_first_0:                 episode reward: 4.8500,                 loss: nan
env0_second_0:                 episode reward: -4.8500,                 loss: 0.0223
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
env2_first_0:                 episode reward: 4.8500,                 loss: nan
env2_second_0:                 episode reward: -4.8500,                 loss: nan
env3_first_0:                 episode reward: 4.8500,                 loss: nan
env3_second_0:                 episode reward: -4.8500,                 loss: nan
env4_first_0:                 episode reward: 4.8500,                 loss: nan
env4_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.2126s / 80712.6345 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0216
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
env2_first_0:                 episode reward: 2.9500,                 loss: nan
env2_second_0:                 episode reward: -2.9500,                 loss: nan
env3_first_0:                 episode reward: 2.9500,                 loss: nan
env3_second_0:                 episode reward: -2.9500,                 loss: nan
env4_first_0:                 episode reward: 2.9500,                 loss: nan
env4_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.0385s / 80863.6730 s
env0_first_0:                 episode reward: 5.0000,                 loss: nan
env0_second_0:                 episode reward: -5.0000,                 loss: 0.0182
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
env2_first_0:                 episode reward: 4.4500,                 loss: nan
env2_second_0:                 episode reward: -4.4500,                 loss: nan
env3_first_0:                 episode reward: 5.0000,                 loss: nan
env3_second_0:                 episode reward: -5.0000,                 loss: nan
env4_first_0:                 episode reward: 5.0000,                 loss: nan
env4_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 148.7710s / 81012.4440 s
env0_first_0:                 episode reward: 6.3500,                 loss: nan
env0_second_0:                 episode reward: -6.3500,                 loss: 0.0202
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
env2_first_0:                 episode reward: 6.3500,                 loss: nan
env2_second_0:                 episode reward: -6.3500,                 loss: nan
env3_first_0:                 episode reward: 6.3500,                 loss: nan
env3_second_0:                 episode reward: -6.3500,                 loss: nan
env4_first_0:                 episode reward: 6.5000,                 loss: nan
env4_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.3881s / 81163.8321 s
env0_first_0:                 episode reward: 6.5000,                 loss: nan
env0_second_0:                 episode reward: -6.5000,                 loss: 0.0188
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
env2_first_0:                 episode reward: 6.5000,                 loss: nan
env2_second_0:                 episode reward: -6.5000,                 loss: nan
env3_first_0:                 episode reward: 6.5000,                 loss: nan
env3_second_0:                 episode reward: -6.5000,                 loss: nan
env4_first_0:                 episode reward: 6.5000,                 loss: nan
env4_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.3442s / 81314.1763 s
env0_first_0:                 episode reward: 6.2000,                 loss: nan
env0_second_0:                 episode reward: -6.2000,                 loss: 0.0149
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
env2_first_0:                 episode reward: 6.2000,                 loss: nan
env2_second_0:                 episode reward: -6.2000,                 loss: nan
env3_first_0:                 episode reward: 6.2000,                 loss: nan
env3_second_0:                 episode reward: -6.2000,                 loss: nan
env4_first_0:                 episode reward: 6.2000,                 loss: nan
env4_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.7963s / 81464.9725 s
env0_first_0:                 episode reward: 5.2500,                 loss: nan
env0_second_0:                 episode reward: -5.2500,                 loss: 0.0134
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
env2_first_0:                 episode reward: 4.6000,                 loss: nan
env2_second_0:                 episode reward: -4.6000,                 loss: nan
env3_first_0:                 episode reward: 5.2500,                 loss: nan
env3_second_0:                 episode reward: -5.2500,                 loss: nan
env4_first_0:                 episode reward: 4.6000,                 loss: nan
env4_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.6620s / 81615.6346 s
env0_first_0:                 episode reward: 1.6500,                 loss: nan
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0110
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
env2_first_0:                 episode reward: 1.6000,                 loss: nan
env2_second_0:                 episode reward: -1.6000,                 loss: nan
env3_first_0:                 episode reward: 1.6000,                 loss: nan
env3_second_0:                 episode reward: -1.6000,                 loss: nan
env4_first_0:                 episode reward: 1.6500,                 loss: nan
env4_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.3424s / 81765.9769 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0113
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
env2_first_0:                 episode reward: 3.4500,                 loss: nan
env2_second_0:                 episode reward: -3.4500,                 loss: nan
env3_first_0:                 episode reward: 3.3500,                 loss: nan
env3_second_0:                 episode reward: -3.3500,                 loss: nan
env4_first_0:                 episode reward: 3.2500,                 loss: nan
env4_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.4035s / 81916.3805 s
env0_first_0:                 episode reward: 3.8500,                 loss: nan
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0122
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
env2_first_0:                 episode reward: 3.8500,                 loss: nan
env2_second_0:                 episode reward: -3.8500,                 loss: nan
env3_first_0:                 episode reward: 3.8500,                 loss: nan
env3_second_0:                 episode reward: -3.8500,                 loss: nan
env4_first_0:                 episode reward: 3.8500,                 loss: nan
env4_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.2610s / 82068.6414 s
env0_first_0:                 episode reward: 1.9500,                 loss: nan
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0115
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
env2_first_0:                 episode reward: 1.9500,                 loss: nan
env2_second_0:                 episode reward: -1.9500,                 loss: nan
env3_first_0:                 episode reward: 1.9500,                 loss: nan
env3_second_0:                 episode reward: -1.9500,                 loss: nan
env4_first_0:                 episode reward: 1.9500,                 loss: nan
env4_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.8646s / 82218.5060 s
env0_first_0:                 episode reward: -0.2000,                 loss: nan
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0122
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
env2_first_0:                 episode reward: -0.2000,                 loss: nan
env2_second_0:                 episode reward: 0.2000,                 loss: nan
env3_first_0:                 episode reward: -0.2000,                 loss: nan
env3_second_0:                 episode reward: 0.2000,                 loss: nan
env4_first_0:                 episode reward: -0.2000,                 loss: nan
env4_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.8155s / 82369.3215 s
env0_first_0:                 episode reward: 4.3000,                 loss: nan
env0_second_0:                 episode reward: -4.3000,                 loss: 0.0118
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
env2_first_0:                 episode reward: 4.3000,                 loss: nan
env2_second_0:                 episode reward: -4.3000,                 loss: nan
env3_first_0:                 episode reward: 4.3000,                 loss: nan
env3_second_0:                 episode reward: -4.3000,                 loss: nan
env4_first_0:                 episode reward: 4.3000,                 loss: nan
env4_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.2491s / 82519.5706 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0110
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
env2_first_0:                 episode reward: 3.3000,                 loss: nan
env2_second_0:                 episode reward: -3.3000,                 loss: nan
env3_first_0:                 episode reward: 3.3000,                 loss: nan
env3_second_0:                 episode reward: -3.3000,                 loss: nan
env4_first_0:                 episode reward: 3.3000,                 loss: nan
env4_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.0458s / 82671.6164 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0111
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
env2_first_0:                 episode reward: 3.3500,                 loss: nan
env2_second_0:                 episode reward: -3.3500,                 loss: nan
env3_first_0:                 episode reward: 3.3500,                 loss: nan
env3_second_0:                 episode reward: -3.3500,                 loss: nan
env4_first_0:                 episode reward: 3.2000,                 loss: nan
env4_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.6016s / 82822.2180 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0115
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
env2_first_0:                 episode reward: 2.8000,                 loss: nan
env2_second_0:                 episode reward: -2.8000,                 loss: nan
env3_first_0:                 episode reward: 2.8000,                 loss: nan
env3_second_0:                 episode reward: -2.8000,                 loss: nan
env4_first_0:                 episode reward: 2.8000,                 loss: nan
env4_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.2845s / 82972.5026 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0120
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
env2_first_0:                 episode reward: 0.4500,                 loss: nan
env2_second_0:                 episode reward: -0.4500,                 loss: nan
env3_first_0:                 episode reward: 0.5000,                 loss: nan
env3_second_0:                 episode reward: -0.5000,                 loss: nan
env4_first_0:                 episode reward: 0.4500,                 loss: nan
env4_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 152.2294s / 83124.7319 s
env0_first_0:                 episode reward: 3.9500,                 loss: nan
env0_second_0:                 episode reward: -3.9500,                 loss: 0.0112
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
env2_first_0:                 episode reward: 3.9500,                 loss: nan
env2_second_0:                 episode reward: -3.9500,                 loss: nan
env3_first_0:                 episode reward: 3.9500,                 loss: nan
env3_second_0:                 episode reward: -3.9500,                 loss: nan
env4_first_0:                 episode reward: 3.9500,                 loss: nan
env4_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.1484s / 83275.8803 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0117
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
env2_first_0:                 episode reward: 2.4500,                 loss: nan
env2_second_0:                 episode reward: -2.4500,                 loss: nan
env3_first_0:                 episode reward: 2.4500,                 loss: nan
env3_second_0:                 episode reward: -2.4500,                 loss: nan
env4_first_0:                 episode reward: 2.4500,                 loss: nan
env4_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 151.6811s / 83427.5614 s
env0_first_0:                 episode reward: 4.3500,                 loss: nan
env0_second_0:                 episode reward: -4.3500,                 loss: 0.0115
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
env2_first_0:                 episode reward: 4.3500,                 loss: nan
env2_second_0:                 episode reward: -4.3500,                 loss: nan
env3_first_0:                 episode reward: 4.3500,                 loss: nan
env3_second_0:                 episode reward: -4.3500,                 loss: nan
env4_first_0:                 episode reward: 4.3500,                 loss: nan
env4_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.5280s / 83577.0895 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0115
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
env2_first_0:                 episode reward: 3.5000,                 loss: nan
env2_second_0:                 episode reward: -3.5000,                 loss: nan
env3_first_0:                 episode reward: 3.7000,                 loss: nan
env3_second_0:                 episode reward: -3.7000,                 loss: nan
env4_first_0:                 episode reward: 3.7000,                 loss: nan
env4_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.3012s / 83726.3906 s
env0_first_0:                 episode reward: 3.9000,                 loss: nan
env0_second_0:                 episode reward: -3.9000,                 loss: 0.0116
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
env2_first_0:                 episode reward: 3.9000,                 loss: nan
env2_second_0:                 episode reward: -3.9000,                 loss: nan
env3_first_0:                 episode reward: 3.9000,                 loss: nan
env3_second_0:                 episode reward: -3.9000,                 loss: nan
env4_first_0:                 episode reward: 3.9000,                 loss: nan
env4_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.3342s / 83875.7248 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0117
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
env2_first_0:                 episode reward: 2.1000,                 loss: nan
env2_second_0:                 episode reward: -2.1000,                 loss: nan
env3_first_0:                 episode reward: 2.8500,                 loss: nan
env3_second_0:                 episode reward: -2.8500,                 loss: nan
env4_first_0:                 episode reward: 2.8500,                 loss: nan
env4_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.8200s / 84025.5449 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0115
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
env2_first_0:                 episode reward: 3.2500,                 loss: nan
env2_second_0:                 episode reward: -3.2500,                 loss: nan
env3_first_0:                 episode reward: 3.2500,                 loss: nan
env3_second_0:                 episode reward: -3.2500,                 loss: nan
env4_first_0:                 episode reward: 3.2500,                 loss: nan
env4_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 148.7788s / 84174.3237 s
env0_first_0:                 episode reward: 4.4500,                 loss: nan
env0_second_0:                 episode reward: -4.4500,                 loss: 0.0116
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
env2_first_0:                 episode reward: 4.4500,                 loss: nan
env2_second_0:                 episode reward: -4.4500,                 loss: nan
env3_first_0:                 episode reward: 4.4500,                 loss: nan
env3_second_0:                 episode reward: -4.4500,                 loss: nan
env4_first_0:                 episode reward: 4.4500,                 loss: nan
env4_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.0610s / 84323.3847 s
env0_first_0:                 episode reward: 4.3000,                 loss: nan
env0_second_0:                 episode reward: -4.3000,                 loss: 0.0115
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
env2_first_0:                 episode reward: 4.3000,                 loss: nan
env2_second_0:                 episode reward: -4.3000,                 loss: nan
env3_first_0:                 episode reward: 4.3000,                 loss: nan
env3_second_0:                 episode reward: -4.3000,                 loss: nan
env4_first_0:                 episode reward: 4.3000,                 loss: nan
env4_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.5245s / 84472.9092 s
env0_first_0:                 episode reward: 1.2500,                 loss: nan
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0115
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
env2_first_0:                 episode reward: 1.2000,                 loss: nan
env2_second_0:                 episode reward: -1.2000,                 loss: nan
env3_first_0:                 episode reward: 1.2500,                 loss: nan
env3_second_0:                 episode reward: -1.2500,                 loss: nan
env4_first_0:                 episode reward: 1.2000,                 loss: nan
env4_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.1141s / 84623.0233 s
env0_first_0:                 episode reward: 3.8000,                 loss: nan
env0_second_0:                 episode reward: -3.8000,                 loss: 0.0111
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
env2_first_0:                 episode reward: 3.4500,                 loss: nan
env2_second_0:                 episode reward: -3.4500,                 loss: nan
env3_first_0:                 episode reward: 3.8000,                 loss: nan
env3_second_0:                 episode reward: -3.8000,                 loss: nan
env4_first_0:                 episode reward: 3.8000,                 loss: nan
env4_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 148.3754s / 84771.3987 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0109
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
env2_first_0:                 episode reward: 3.1000,                 loss: nan
env2_second_0:                 episode reward: -3.1000,                 loss: nan
env3_first_0:                 episode reward: 3.1000,                 loss: nan
env3_second_0:                 episode reward: -3.1000,                 loss: nan
env4_first_0:                 episode reward: 3.1000,                 loss: nan
env4_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 148.3683s / 84919.7669 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0113
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
env2_first_0:                 episode reward: -0.3500,                 loss: nan
env2_second_0:                 episode reward: 0.3500,                 loss: nan
env3_first_0:                 episode reward: -0.3500,                 loss: nan
env3_second_0:                 episode reward: 0.3500,                 loss: nan
env4_first_0:                 episode reward: -0.3000,                 loss: nan
env4_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.5922s / 85069.3592 s
env0_first_0:                 episode reward: 2.2000,                 loss: nan
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0115
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
env2_first_0:                 episode reward: 2.2000,                 loss: nan
env2_second_0:                 episode reward: -2.2000,                 loss: nan
env3_first_0:                 episode reward: 2.2000,                 loss: nan
env3_second_0:                 episode reward: -2.2000,                 loss: nan
env4_first_0:                 episode reward: 2.2000,                 loss: nan
env4_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 147.5634s / 85216.9226 s
env0_first_0:                 episode reward: -1.5000,                 loss: nan
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0114
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
env2_first_0:                 episode reward: -1.5000,                 loss: nan
env2_second_0:                 episode reward: 1.5000,                 loss: nan
env3_first_0:                 episode reward: -1.5000,                 loss: nan
env3_second_0:                 episode reward: 1.5000,                 loss: nan
env4_first_0:                 episode reward: -1.5000,                 loss: nan
env4_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 149.6379s / 85366.5604 s
env0_first_0:                 episode reward: 1.3500,                 loss: nan
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0114
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
env2_first_0:                 episode reward: 1.4000,                 loss: nan
env2_second_0:                 episode reward: -1.4000,                 loss: nan
env3_first_0:                 episode reward: 1.4000,                 loss: nan
env3_second_0:                 episode reward: -1.4000,                 loss: nan
env4_first_0:                 episode reward: 1.6500,                 loss: nan
env4_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 150.2858s / 85516.8463 s
env0_first_0:                 episode reward: 4.1000,                 loss: nan
env0_second_0:                 episode reward: -4.1000,                 loss: 0.0115
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
env2_first_0:                 episode reward: 4.1000,                 loss: nan
env2_second_0:                 episode reward: -4.1000,                 loss: nan
env3_first_0:                 episode reward: 4.1000,                 loss: nan
env3_second_0:                 episode reward: -4.1000,                 loss: nan
env4_first_0:                 episode reward: 4.1000,                 loss: nan
env4_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 147.9705s / 85664.8167 sLoad pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_first_0:                 episode reward: 3.8500,                 loss: nan
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0114
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
env2_first_0:                 episode reward: 3.9000,                 loss: nan
env2_second_0:                 episode reward: -3.9000,                 loss: nan
env3_first_0:                 episode reward: 3.8500,                 loss: nan
env3_second_0:                 episode reward: -3.8500,                 loss: nan
env4_first_0:                 episode reward: 3.8500,                 loss: nan
env4_second_0:                 episode reward: -3.8500,                 loss: nan
