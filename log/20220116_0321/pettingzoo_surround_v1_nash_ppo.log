pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [31, 19]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220116_0321/pettingzoo_surround_v1_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220116_0321/pettingzoo_surround_v1_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 1061.0,                last time consumption/overall running time: 14.0891s / 14.0891 s
env0_first_0:                 episode reward: 9.0000,                 loss: -0.0207
env0_second_0:                 episode reward: -9.0000,                 loss: 0.0003
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1434.65,                last time consumption/overall running time: 367.4512s / 381.5403 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0382
env0_second_0:                 episode reward: 1.8500,                 loss: -0.0306
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1677.5,                last time consumption/overall running time: 427.1422s / 808.6825 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0412
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0393
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1996.0,                last time consumption/overall running time: 507.8586s / 1316.5411 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.0474
env0_second_0:                 episode reward: 2.8500,                 loss: -0.0450
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 2067.2,                last time consumption/overall running time: 526.7441s / 1843.2852 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.0693
env0_second_0:                 episode reward: 3.6500,                 loss: -0.0669
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 2214.55,                last time consumption/overall running time: 564.5772s / 2407.8624 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.0682
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0696
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 2234.55,                last time consumption/overall running time: 567.0371s / 2974.8995 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.0838
env0_second_0:                 episode reward: 3.3000,                 loss: -0.0850
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 2142.9,                last time consumption/overall running time: 545.1678s / 3520.0673 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.0821
env0_second_0:                 episode reward: 4.3000,                 loss: -0.0804
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 2195.0,                last time consumption/overall running time: 560.1649s / 4080.2322 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.0826
env0_second_0:                 episode reward: 3.8000,                 loss: -0.0812
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 2288.95,                last time consumption/overall running time: 580.3277s / 4660.5599 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.0923
env0_second_0:                 episode reward: 3.2000,                 loss: -0.0916
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 2184.9,                last time consumption/overall running time: 556.1095s / 5216.6694 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0993
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0991
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 2111.0,                last time consumption/overall running time: 539.0500s / 5755.7194 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.1111
env0_second_0:                 episode reward: 3.9500,                 loss: -0.1112
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 2095.15,                last time consumption/overall running time: 535.1842s / 6290.9036 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.0944
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0931
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 2176.25,                last time consumption/overall running time: 555.8170s / 6846.7206 s
env0_first_0:                 episode reward: -3.9000,                 loss: -0.0947
env0_second_0:                 episode reward: 3.9000,                 loss: -0.0914
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2193.3,                last time consumption/overall running time: 562.1510s / 7408.8716 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.0862
env0_second_0:                 episode reward: 4.1000,                 loss: -0.0824
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 2316.25,                last time consumption/overall running time: 591.8012s / 8000.6728 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.0842
env0_second_0:                 episode reward: 2.4000,                 loss: -0.0780
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 2245.75,                last time consumption/overall running time: 573.5864s / 8574.2591 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.1033
env0_second_0:                 episode reward: 4.3500,                 loss: -0.0981
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 2196.45,                last time consumption/overall running time: 560.0987s / 9134.3579 s
env0_first_0:                 episode reward: -4.0500,                 loss: -0.1170
env0_second_0:                 episode reward: 4.0500,                 loss: -0.1127
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2202.4,                last time consumption/overall running time: 559.8884s / 9694.2463 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1237
env0_second_0:                 episode reward: 4.4000,                 loss: -0.1210
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 2386.7,                last time consumption/overall running time: 606.8256s / 10301.0719 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1112
env0_second_0:                 episode reward: 1.8500,                 loss: -0.1043
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 2333.15,                last time consumption/overall running time: 592.2366s / 10893.3085 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.1059
env0_second_0:                 episode reward: 4.4500,                 loss: -0.1032
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 2376.2,                last time consumption/overall running time: 603.3813s / 11496.6899 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.0920
env0_second_0:                 episode reward: 2.9000,                 loss: -0.0884
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 2348.85,                last time consumption/overall running time: 599.0516s / 12095.7415 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1095
env0_second_0:                 episode reward: 2.4500,                 loss: -0.1020
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 2383.8,                last time consumption/overall running time: 601.1353s / 12696.8769 s
env0_first_0:                 episode reward: -3.1500,                 loss: -0.1198
env0_second_0:                 episode reward: 3.1500,                 loss: -0.1112
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 2281.7,                last time consumption/overall running time: 577.6693s / 13274.5462 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.1357
env0_second_0:                 episode reward: 3.6000,                 loss: -0.1267
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2131.6,                last time consumption/overall running time: 539.0024s / 13813.5486 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.1343
env0_second_0:                 episode reward: 4.8500,                 loss: -0.1335
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 2300.9,                last time consumption/overall running time: 580.3378s / 14393.8863 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.1236
env0_second_0:                 episode reward: 3.4500,                 loss: -0.1192
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 2238.0,                last time consumption/overall running time: 568.4175s / 14962.3038 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.1371
env0_second_0:                 episode reward: 4.8500,                 loss: -0.1334
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 2345.2,                last time consumption/overall running time: 593.4586s / 15555.7625 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.1344
env0_second_0:                 episode reward: 3.3000,                 loss: -0.1284
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 2331.1,                last time consumption/overall running time: 587.6906s / 16143.4531 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.1228
env0_second_0:                 episode reward: 4.5000,                 loss: -0.1157
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2225.0,                last time consumption/overall running time: 563.9851s / 16707.4382 s
env0_first_0:                 episode reward: -5.0000,                 loss: -0.1328
env0_second_0:                 episode reward: 5.0000,                 loss: -0.1270
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2162.9,                last time consumption/overall running time: 547.5085s / 17254.9467 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.1371
env0_second_0:                 episode reward: 5.3500,                 loss: -0.1292
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2320.3,                last time consumption/overall running time: 585.8995s / 17840.8461 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.1317
env0_second_0:                 episode reward: 5.0500,                 loss: -0.1250
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2189.2,                last time consumption/overall running time: 554.9237s / 18395.7698 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1524
env0_second_0:                 episode reward: 5.4500,                 loss: -0.1469
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2299.1,                last time consumption/overall running time: 582.8531s / 18978.6229 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.1298
env0_second_0:                 episode reward: 4.7000,                 loss: -0.1187
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2308.15,                last time consumption/overall running time: 583.5070s / 19562.1298 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.1211
env0_second_0:                 episode reward: 3.8500,                 loss: -0.1100
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2091.5,                last time consumption/overall running time: 529.9752s / 20092.1051 s
env0_first_0:                 episode reward: -6.9000,                 loss: -0.1500
env0_second_0:                 episode reward: 6.9000,                 loss: -0.1399
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2079.4,                last time consumption/overall running time: 526.2204s / 20618.3255 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1568
env0_second_0:                 episode reward: 7.1000,                 loss: -0.1483
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2269.9,                last time consumption/overall running time: 571.7922s / 21190.1176 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.1525
env0_second_0:                 episode reward: 6.0000,                 loss: -0.1410
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2103.2,                last time consumption/overall running time: 532.5111s / 21722.6287 s
env0_first_0:                 episode reward: -4.0500,                 loss: -0.1560
env0_second_0:                 episode reward: 4.0500,                 loss: -0.1432
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2266.2,                last time consumption/overall running time: 572.1732s / 22294.8020 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1320
env0_second_0:                 episode reward: 4.4000,                 loss: -0.1185
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2469.85,                last time consumption/overall running time: 624.7631s / 22919.5651 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.1309
env0_second_0:                 episode reward: 2.8500,                 loss: -0.1146
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2641.85,                last time consumption/overall running time: 667.9066s / 23587.4716 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1233
env0_second_0:                 episode reward: 3.7500,                 loss: -0.0812
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2713.8,                last time consumption/overall running time: 683.8647s / 24271.3364 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1184
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0870
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2799.1,                last time consumption/overall running time: 703.6985s / 24975.0349 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.1198
env0_second_0:                 episode reward: 2.5500,                 loss: -0.1052
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2989.7,                last time consumption/overall running time: 751.7172s / 25726.7521 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1093
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0941
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2789.3,                last time consumption/overall running time: 699.9440s / 26426.6962 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1129
env0_second_0:                 episode reward: 4.2000,                 loss: -0.0980
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2731.0,                last time consumption/overall running time: 685.9621s / 27112.6583 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.0980
env0_second_0:                 episode reward: 3.0500,                 loss: -0.0798
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2909.35,                last time consumption/overall running time: 727.2019s / 27839.8602 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1055
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0893
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2690.1,                last time consumption/overall running time: 673.2624s / 28513.1225 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1104
env0_second_0:                 episode reward: 1.0500,                 loss: -0.0985
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2844.15,                last time consumption/overall running time: 710.3356s / 29223.4581 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1177
env0_second_0:                 episode reward: 2.3500,                 loss: -0.1006
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2870.85,                last time consumption/overall running time: 717.2796s / 29940.7377 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.1100
env0_second_0:                 episode reward: 1.3500,                 loss: -0.0829
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 3046.45,                last time consumption/overall running time: 759.9164s / 30700.6541 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1155
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0946
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2891.55,                last time consumption/overall running time: 723.6187s / 31424.2727 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1135
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0917
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 3101.2,                last time consumption/overall running time: 773.1183s / 32197.3910 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1065
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0931
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 3147.45,                last time consumption/overall running time: 786.6093s / 32984.0003 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1097
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0985
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2978.75,                last time consumption/overall running time: 743.4811s / 33727.4813 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1177
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1010
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 3065.3,                last time consumption/overall running time: 764.7620s / 34492.2433 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1194
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0988
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 3063.2,                last time consumption/overall running time: 764.7908s / 35257.0341 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1085
env0_second_0:                 episode reward: 1.8500,                 loss: -0.0929
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 3077.1,                last time consumption/overall running time: 767.3802s / 36024.4143 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1025
env0_second_0:                 episode reward: -1.0500,                 loss: -0.0721
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 3258.05,                last time consumption/overall running time: 817.1125s / 36841.5267 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1081
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0840
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2925.2,                last time consumption/overall running time: 734.0225s / 37575.5493 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1120
env0_second_0:                 episode reward: -1.4000,                 loss: -0.0925
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 3083.65,                last time consumption/overall running time: 770.5293s / 38346.0786 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.1128
env0_second_0:                 episode reward: 1.2000,                 loss: -0.0862
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 3064.95,                last time consumption/overall running time: 764.9140s / 39110.9926 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1134
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0880
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 3018.2,                last time consumption/overall running time: 752.9619s / 39863.9545 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1278
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0993
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 3065.05,                last time consumption/overall running time: 765.9835s / 40629.9380 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1155
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0892
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 3386.8,                last time consumption/overall running time: 847.1230s / 41477.0609 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1126
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0688
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 3067.6,                last time consumption/overall running time: 767.3054s / 42244.3663 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1089
env0_second_0:                 episode reward: -3.6000,                 loss: -0.0771
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 3044.75,                last time consumption/overall running time: 760.4900s / 43004.8563 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1201
env0_second_0:                 episode reward: -3.8000,                 loss: -0.0940
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 2642.5,                last time consumption/overall running time: 661.0472s / 43665.9034 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.1107
env0_second_0:                 episode reward: -4.5000,                 loss: -0.0779
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 3015.2,                last time consumption/overall running time: 752.7850s / 44418.6885 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1189
env0_second_0:                 episode reward: -2.2000,                 loss: -0.0913
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 3337.45,                last time consumption/overall running time: 829.2226s / 45247.9111 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1251
env0_second_0:                 episode reward: -1.6500,                 loss: -0.0993
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 3070.1,                last time consumption/overall running time: 766.5331s / 46014.4442 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1194
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0879
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 3272.85,                last time consumption/overall running time: 817.4703s / 46831.9145 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1183
env0_second_0:                 episode reward: -1.7000,                 loss: -0.0918
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 3099.75,                last time consumption/overall running time: 773.3511s / 47605.2657 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1290
env0_second_0:                 episode reward: -2.1000,                 loss: -0.0997
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 3210.1,                last time consumption/overall running time: 802.0958s / 48407.3615 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1184
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0898
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 3096.8,                last time consumption/overall running time: 775.6290s / 49182.9905 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1235
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0895
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 3212.15,                last time consumption/overall running time: 801.9510s / 49984.9415 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1256
env0_second_0:                 episode reward: -1.6500,                 loss: -0.0998
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 3194.0,                last time consumption/overall running time: 798.8032s / 50783.7447 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1244
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0974
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 3161.05,                last time consumption/overall running time: 790.6054s / 51574.3501 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1273
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0896
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 3370.55,                last time consumption/overall running time: 816.0044s / 52390.3544 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1249
env0_second_0:                 episode reward: -1.1000,                 loss: -0.0914
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 3205.05,                last time consumption/overall running time: 741.2509s / 53131.6053 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1224
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0982
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 3031.6,                last time consumption/overall running time: 701.2728s / 53832.8782 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1228
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0854
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 3307.9,                last time consumption/overall running time: 688.8980s / 54521.7761 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1256
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0851
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 3014.45,                last time consumption/overall running time: 619.2644s / 55141.0405 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1253
env0_second_0:                 episode reward: -3.6000,                 loss: -0.0864
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 3160.95,                last time consumption/overall running time: 648.9069s / 55789.9475 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1145
env0_second_0:                 episode reward: -3.2500,                 loss: -0.0663
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 2865.95,                last time consumption/overall running time: 532.2541s / 56322.2015 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.1272
env0_second_0:                 episode reward: -4.2000,                 loss: -0.0861
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 3227.55,                last time consumption/overall running time: 597.7512s / 56919.9528 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1076
env0_second_0:                 episode reward: -1.5000,                 loss: -0.0542
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 3080.65,                last time consumption/overall running time: 573.1309s / 57493.0837 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1161
env0_second_0:                 episode reward: -2.5000,                 loss: -0.0733
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 3141.25,                last time consumption/overall running time: 581.7027s / 58074.7864 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1197
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0757
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 3276.65,                last time consumption/overall running time: 606.8405s / 58681.6268 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1255
env0_second_0:                 episode reward: -1.6000,                 loss: -0.0742
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 2999.15,                last time consumption/overall running time: 555.3859s / 59237.0127 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1236
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0836
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 3091.7,                last time consumption/overall running time: 575.9861s / 59812.9988 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1371
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0917
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 3220.7,                last time consumption/overall running time: 597.7144s / 60410.7132 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1391
env0_second_0:                 episode reward: -2.3500,                 loss: -0.0905
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 3183.95,                last time consumption/overall running time: 591.1154s / 61001.8286 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1357
env0_second_0:                 episode reward: -1.1500,                 loss: -0.0843
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 3104.4,                last time consumption/overall running time: 577.8203s / 61579.6489 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.1316
env0_second_0:                 episode reward: -4.2000,                 loss: -0.0825
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 3484.9,                last time consumption/overall running time: 644.7765s / 62224.4254 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1320
env0_second_0:                 episode reward: -2.8500,                 loss: -0.0835
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 3544.5,                last time consumption/overall running time: 654.6569s / 62879.0823 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1272
env0_second_0:                 episode reward: -0.9000,                 loss: -0.0887
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 3407.1,                last time consumption/overall running time: 629.7439s / 63508.8262 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1343
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0918
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 3589.4,                last time consumption/overall running time: 665.2148s / 64174.0410 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.1382
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0947
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 3466.45,                last time consumption/overall running time: 640.6868s / 64814.7278 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1314
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0819
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 3361.15,                last time consumption/overall running time: 621.5484s / 65436.2761 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.1320
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0791
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 3495.6,                last time consumption/overall running time: 586.3919s / 66022.6680 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.1263
env0_second_0:                 episode reward: 1.1000,                 loss: -0.0587
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 3368.65,                last time consumption/overall running time: 546.6023s / 66569.2703 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1201
env0_second_0:                 episode reward: 1.8000,                 loss: -0.0372
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 3306.65,                last time consumption/overall running time: 535.9761s / 67105.2464 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.1307
env0_second_0:                 episode reward: 2.9000,                 loss: -0.0534
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 3097.55,                last time consumption/overall running time: 503.4869s / 67608.7333 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.1409
env0_second_0:                 episode reward: 4.3500,                 loss: -0.0704
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 3225.65,                last time consumption/overall running time: 525.9357s / 68134.6690 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1413
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0827
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 3221.7,                last time consumption/overall running time: 529.3857s / 68664.0547 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1463
env0_second_0:                 episode reward: 3.7500,                 loss: -0.0939
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 3445.65,                last time consumption/overall running time: 562.1854s / 69226.2401 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1442
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0972
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 3044.85,                last time consumption/overall running time: 497.3470s / 69723.5871 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1452
env0_second_0:                 episode reward: 2.3500,                 loss: -0.1006
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 3157.6,                last time consumption/overall running time: 512.8515s / 70236.4387 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.1587
env0_second_0:                 episode reward: 3.2000,                 loss: -0.1046
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 3086.5,                last time consumption/overall running time: 503.8817s / 70740.3204 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1488
env0_second_0:                 episode reward: 3.7500,                 loss: -0.1009
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 2967.6,                last time consumption/overall running time: 487.3093s / 71227.6297 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.1405
env0_second_0:                 episode reward: 4.8500,                 loss: -0.0909
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 3361.75,                last time consumption/overall running time: 547.9969s / 71775.6266 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1303
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0848
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 3196.15,                last time consumption/overall running time: 521.2440s / 72296.8706 s
env0_first_0:                 episode reward: -4.1500,                 loss: -0.1357
env0_second_0:                 episode reward: 4.1500,                 loss: -0.0857
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 3241.4,                last time consumption/overall running time: 528.1555s / 72825.0262 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.1334
env0_second_0:                 episode reward: 3.2500,                 loss: -0.0830
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 3273.8,                last time consumption/overall running time: 536.4696s / 73361.4958 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.1332
env0_second_0:                 episode reward: 2.6000,                 loss: -0.0786
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 3161.4,                last time consumption/overall running time: 515.7221s / 73877.2178 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.1407
env0_second_0:                 episode reward: 4.4500,                 loss: -0.0750
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 2688.8,                last time consumption/overall running time: 440.3154s / 74317.5332 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.1437
env0_second_0:                 episode reward: 6.5000,                 loss: -0.0878
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 2352.0,                last time consumption/overall running time: 385.7571s / 74703.2903 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.1525
env0_second_0:                 episode reward: 8.0000,                 loss: -0.0969
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 2727.9,                last time consumption/overall running time: 449.2995s / 75152.5898 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.1501
env0_second_0:                 episode reward: 7.6000,                 loss: -0.0825
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 2682.7,                last time consumption/overall running time: 440.4167s / 75593.0065 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1462
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0830
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 2932.0,                last time consumption/overall running time: 478.4265s / 76071.4330 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1507
env0_second_0:                 episode reward: 7.1000,                 loss: -0.0640
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 2959.85,                last time consumption/overall running time: 483.2639s / 76554.6969 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1426
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0703
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 2979.35,                last time consumption/overall running time: 486.3320s / 77041.0289 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.1458
env0_second_0:                 episode reward: 4.6000,                 loss: -0.0810
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 3004.75,                last time consumption/overall running time: 489.0101s / 77530.0390 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.1371
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0828
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 2804.2,                last time consumption/overall running time: 454.9937s / 77985.0328 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.1519
env0_second_0:                 episode reward: 6.6000,                 loss: -0.0980
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 2909.5,                last time consumption/overall running time: 473.4496s / 78458.4824 s
env0_first_0:                 episode reward: -6.9000,                 loss: -0.1387
env0_second_0:                 episode reward: 6.9000,                 loss: -0.0858
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 2945.3,                last time consumption/overall running time: 478.3893s / 78936.8717 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.1352
env0_second_0:                 episode reward: 6.1000,                 loss: -0.0771
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 2961.9,                last time consumption/overall running time: 482.6517s / 79419.5234 s
env0_first_0:                 episode reward: -5.6500,                 loss: -0.1422
env0_second_0:                 episode reward: 5.6500,                 loss: -0.0798
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 2904.6,                last time consumption/overall running time: 478.8334s / 79898.3568 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.1508
env0_second_0:                 episode reward: 4.6000,                 loss: -0.0924
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 3001.0,                last time consumption/overall running time: 494.1839s / 80392.5407 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1534
env0_second_0:                 episode reward: 5.4500,                 loss: -0.0935
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 3142.3,                last time consumption/overall running time: 514.6216s / 80907.1623 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.1581
env0_second_0:                 episode reward: 5.8500,                 loss: -0.0896
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 3193.3,                last time consumption/overall running time: 525.0527s / 81432.2150 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.1461
env0_second_0:                 episode reward: 3.5000,                 loss: -0.0855
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 2666.8,                last time consumption/overall running time: 437.7750s / 81869.9900 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.1601
env0_second_0:                 episode reward: 6.7500,                 loss: 0.3880
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 2430.7,                last time consumption/overall running time: 401.7226s / 82271.7127 s
env0_first_0:                 episode reward: -7.3000,                 loss: -0.1594
env0_second_0:                 episode reward: 7.3000,                 loss: -0.0705
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 2355.75,                last time consumption/overall running time: 389.3431s / 82661.0557 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.1507
env0_second_0:                 episode reward: 6.2000,                 loss: -0.0870
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 2393.3,                last time consumption/overall running time: 395.9164s / 83056.9721 s
env0_first_0:                 episode reward: -7.8000,                 loss: -0.1522
env0_second_0:                 episode reward: 7.8000,                 loss: -0.0668
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 2255.45,                last time consumption/overall running time: 372.5053s / 83429.4774 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.1494
env0_second_0:                 episode reward: 7.5500,                 loss: -0.0906
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 2491.3,                last time consumption/overall running time: 407.5472s / 83837.0246 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.1472
env0_second_0:                 episode reward: 7.3500,                 loss: -0.0777
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1946.75,                last time consumption/overall running time: 323.1838s / 84160.2084 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.1276
env0_second_0:                 episode reward: 8.2500,                 loss: -0.0550
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 2405.0,                last time consumption/overall running time: 396.7133s / 84556.9217 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.1447
env0_second_0:                 episode reward: 7.7500,                 loss: -0.0825
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1916.1,                last time consumption/overall running time: 319.0111s / 84875.9328 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.1328
env0_second_0:                 episode reward: 8.6000,                 loss: -0.0787
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 2419.45,                last time consumption/overall running time: 397.9880s / 85273.9208 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1327
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0839
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 2779.35,                last time consumption/overall running time: 455.8802s / 85729.8011 s
env0_first_0:                 episode reward: -5.1500,                 loss: -0.1358
env0_second_0:                 episode reward: 5.1500,                 loss: -0.0766
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 2556.95,                last time consumption/overall running time: 418.0327s / 86147.8337 s
env0_first_0:                 episode reward: -7.1500,                 loss: -0.1546
env0_second_0:                 episode reward: 7.1500,                 loss: -0.1068
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 2298.0,                last time consumption/overall running time: 379.4293s / 86527.2631 s
env0_first_0:                 episode reward: -7.9500,                 loss: -0.1602
env0_second_0:                 episode reward: 7.9500,                 loss: -0.1161
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 2174.8,                last time consumption/overall running time: 359.9871s / 86887.2502 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.1562
env0_second_0:                 episode reward: 8.2000,                 loss: -0.1008
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 2423.1,                last time consumption/overall running time: 399.1087s / 87286.3589 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1554
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0770
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 2165.0,                last time consumption/overall running time: 357.8807s / 87644.2396 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.1357
env0_second_0:                 episode reward: 7.6000,                 loss: -0.0636
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 2146.55,                last time consumption/overall running time: 357.2122s / 88001.4518 s
env0_first_0:                 episode reward: -7.0500,                 loss: -0.1409
env0_second_0:                 episode reward: 7.0500,                 loss: -0.0738
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 2105.5,                last time consumption/overall running time: 350.1082s / 88351.5600 s
env0_first_0:                 episode reward: -8.7000,                 loss: -0.1600
env0_second_0:                 episode reward: 8.7000,                 loss: -0.0897
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 2222.1,                last time consumption/overall running time: 366.5604s / 88718.1204 s
env0_first_0:                 episode reward: -7.9500,                 loss: -0.1348
env0_second_0:                 episode reward: 7.9500,                 loss: -0.0722
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 2698.7,                last time consumption/overall running time: 442.6504s / 89160.7708 s
env0_first_0:                 episode reward: -6.7000,                 loss: -0.1492
env0_second_0:                 episode reward: 6.7000,                 loss: -0.0996
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 2311.1,                last time consumption/overall running time: 383.1022s / 89543.8730 s
env0_first_0:                 episode reward: -6.4500,                 loss: -0.1357
env0_second_0:                 episode reward: 6.4500,                 loss: -0.0885
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 2311.25,                last time consumption/overall running time: 381.5397s / 89925.4127 s
env0_first_0:                 episode reward: -7.9000,                 loss: -0.1278
env0_second_0:                 episode reward: 7.9000,                 loss: -0.0801
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 2648.4,                last time consumption/overall running time: 435.6677s / 90361.0804 s
env0_first_0:                 episode reward: -6.7000,                 loss: -0.1413
env0_second_0:                 episode reward: 6.7000,                 loss: -0.0838
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 2740.1,                last time consumption/overall running time: 456.1108s / 90817.1912 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.1425
env0_second_0:                 episode reward: 6.1000,                 loss: -0.0995
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 2330.35,                last time consumption/overall running time: 386.2359s / 91203.4272 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.1388
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0811
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 2530.35,                last time consumption/overall running time: 419.4324s / 91622.8596 s
env0_first_0:                 episode reward: -5.1500,                 loss: -0.1425
env0_second_0:                 episode reward: 5.1500,                 loss: -0.0866
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 2587.35,                last time consumption/overall running time: 428.0252s / 92050.8848 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1439
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0906
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 2581.0,                last time consumption/overall running time: 425.1528s / 92476.0376 s
env0_first_0:                 episode reward: -6.7000,                 loss: -0.1441
env0_second_0:                 episode reward: 6.7000,                 loss: -0.1027
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 2406.95,                last time consumption/overall running time: 396.8876s / 92872.9252 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.1501
env0_second_0:                 episode reward: 8.3000,                 loss: -0.1015
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 2496.8,                last time consumption/overall running time: 411.1310s / 93284.0562 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.1441
env0_second_0:                 episode reward: 5.8500,                 loss: -0.0953
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 2660.95,                last time consumption/overall running time: 438.0258s / 93722.0821 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.1557
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0869
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 2561.75,                last time consumption/overall running time: 419.1472s / 94141.2293 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1627
env0_second_0:                 episode reward: 6.1500,                 loss: -0.1075
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 2672.15,                last time consumption/overall running time: 440.2775s / 94581.5068 s
env0_first_0:                 episode reward: -7.4500,                 loss: -0.1604
env0_second_0:                 episode reward: 7.4500,                 loss: -0.1131
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 2882.1,                last time consumption/overall running time: 473.0628s / 95054.5696 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.1549
env0_second_0:                 episode reward: 6.6000,                 loss: -0.0992
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 2694.35,                last time consumption/overall running time: 449.7435s / 95504.3131 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.1758
env0_second_0:                 episode reward: 7.7000,                 loss: -0.1122
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 2483.45,                last time consumption/overall running time: 408.8406s / 95913.1537 s
env0_first_0:                 episode reward: -8.5000,                 loss: -0.1774
env0_second_0:                 episode reward: 8.5000,                 loss: -0.1284
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 2369.7,                last time consumption/overall running time: 394.7692s / 96307.9229 s
env0_first_0:                 episode reward: -8.4000,                 loss: -0.1683
env0_second_0:                 episode reward: 8.4000,                 loss: -0.1225
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 2188.85,                last time consumption/overall running time: 365.1264s / 96673.0493 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.1830
env0_second_0:                 episode reward: 8.8500,                 loss: -0.1360
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 2432.55,                last time consumption/overall running time: 400.0038s / 97073.0531 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.1685
env0_second_0:                 episode reward: 7.8500,                 loss: -0.1209
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1473.2,                last time consumption/overall running time: 247.0807s / 97320.1338 s
env0_first_0:                 episode reward: -9.1000,                 loss: -0.1461
env0_second_0:                 episode reward: 9.1000,                 loss: -0.0956
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1842.5,                last time consumption/overall running time: 308.6031s / 97628.7369 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1489
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0961
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1793.25,                last time consumption/overall running time: 302.9244s / 97931.6614 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1412
env0_second_0:                 episode reward: 9.7000,                 loss: -0.0908
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1884.85,                last time consumption/overall running time: 315.4526s / 98247.1140 s
env0_first_0:                 episode reward: -9.1500,                 loss: -0.1350
env0_second_0:                 episode reward: 9.1500,                 loss: -0.0904
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 2300.3,                last time consumption/overall running time: 379.6477s / 98626.7617 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.1321
env0_second_0:                 episode reward: 7.7000,                 loss: -0.0855
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 2118.25,                last time consumption/overall running time: 351.6300s / 98978.3916 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.1390
env0_second_0:                 episode reward: 8.5500,                 loss: -0.0944
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 2025.65,                last time consumption/overall running time: 337.6323s / 99316.0240 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.1581
env0_second_0:                 episode reward: 9.2000,                 loss: -0.1134
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1939.95,                last time consumption/overall running time: 322.5187s / 99638.5427 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.1400
env0_second_0:                 episode reward: 9.2000,                 loss: -0.1000
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1993.25,                last time consumption/overall running time: 333.0542s / 99971.5969 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1335
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0917
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 2140.7,                last time consumption/overall running time: 354.4294s / 100326.0263 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.1316
env0_second_0:                 episode reward: 8.7500,                 loss: -0.0812
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 2410.35,                last time consumption/overall running time: 398.5104s / 100724.5367 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.1463
env0_second_0:                 episode reward: 7.4000,                 loss: -0.1001
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 2263.6,                last time consumption/overall running time: 375.8940s / 101100.4307 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.1433
env0_second_0:                 episode reward: 8.6500,                 loss: -0.1018
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1897.4,                last time consumption/overall running time: 316.2651s / 101416.6958 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.1529
env0_second_0:                 episode reward: 9.7500,                 loss: -0.1207
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 2034.8,                last time consumption/overall running time: 334.2890s / 101750.9848 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1598
env0_second_0:                 episode reward: 9.5500,                 loss: -0.1047
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 2062.25,                last time consumption/overall running time: 345.9932s / 102096.9780 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1609
env0_second_0:                 episode reward: 9.3500,                 loss: -0.1079
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 2067.6,                last time consumption/overall running time: 340.5944s / 102437.5724 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.1680
env0_second_0:                 episode reward: 9.2500,                 loss: -0.1063
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 2124.9,                last time consumption/overall running time: 353.6825s / 102791.2549 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1587
env0_second_0:                 episode reward: 9.3000,                 loss: -0.1036
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 2171.15,                last time consumption/overall running time: 358.9330s / 103150.1878 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1578
env0_second_0:                 episode reward: 9.3000,                 loss: -0.0923
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 2289.45,                last time consumption/overall running time: 377.9238s / 103528.1117 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.1540
env0_second_0:                 episode reward: 9.0000,                 loss: -0.0902
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 2273.35,                last time consumption/overall running time: 373.1949s / 103901.3066 s
env0_first_0:                 episode reward: -8.7000,                 loss: -0.1625
env0_second_0:                 episode reward: 8.7000,                 loss: -0.1094
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 2308.75,                last time consumption/overall running time: 377.4802s / 104278.7868 s
env0_first_0:                 episode reward: -8.4000,                 loss: -0.1495
env0_second_0:                 episode reward: 8.4000,                 loss: -0.0982
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 2333.2,                last time consumption/overall running time: 383.2685s / 104662.0553 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.1473
env0_second_0:                 episode reward: 7.8500,                 loss: -0.1009
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 2168.8,                last time consumption/overall running time: 355.4311s / 105017.4864 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.1530
env0_second_0:                 episode reward: 9.2000,                 loss: -0.0872
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 2518.75,                last time consumption/overall running time: 411.7095s / 105429.1960 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1316
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0897
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 2406.45,                last time consumption/overall running time: 393.3327s / 105822.5287 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1261
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0850
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1957.45,                last time consumption/overall running time: 326.3354s / 106148.8641 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1197
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0520
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 2205.45,                last time consumption/overall running time: 366.3122s / 106515.1763 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.1071
env0_second_0:                 episode reward: 4.3000,                 loss: -0.0420
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 2572.45,                last time consumption/overall running time: 429.3143s / 106944.4906 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0914
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0373
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 3090.8,                last time consumption/overall running time: 505.2728s / 107449.7634 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.0768
env0_second_0:                 episode reward: 2.3000,                 loss: -0.0362
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 2757.55,                last time consumption/overall running time: 455.8291s / 107905.5925 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.0997
env0_second_0:                 episode reward: 3.7000,                 loss: -0.0539
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 3089.65,                last time consumption/overall running time: 508.2724s / 108413.8649 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0676
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0217
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 2839.7,                last time consumption/overall running time: 466.8304s / 108880.6953 s
env0_first_0:                 episode reward: -4.2500,                 loss: -0.0756
env0_second_0:                 episode reward: 4.2500,                 loss: -0.0323
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 2851.65,                last time consumption/overall running time: 466.1059s / 109346.8013 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.0797
env0_second_0:                 episode reward: 4.5000,                 loss: -0.0452
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 3143.85,                last time consumption/overall running time: 514.6637s / 109861.4649 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.0849
env0_second_0:                 episode reward: 1.6500,                 loss: -0.0504
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 2569.25,                last time consumption/overall running time: 424.3496s / 110285.8145 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.1042
env0_second_0:                 episode reward: 5.3500,                 loss: -0.0636
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 2157.7,                last time consumption/overall running time: 357.2620s / 110643.0766 s
env0_first_0:                 episode reward: -8.1500,                 loss: -0.1233
env0_second_0:                 episode reward: 8.1500,                 loss: -0.0934
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 2381.2,                last time consumption/overall running time: 393.6854s / 111036.7620 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.1277
env0_second_0:                 episode reward: 6.3000,                 loss: -0.0752
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 2617.8,                last time consumption/overall running time: 430.0307s / 111466.7927 s
env0_first_0:                 episode reward: -5.0000,                 loss: -0.1071
env0_second_0:                 episode reward: 5.0000,                 loss: -0.0790
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 2799.35,                last time consumption/overall running time: 459.5337s / 111926.3263 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0999
env0_second_0:                 episode reward: 1.8500,                 loss: -0.0621
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 2776.4,                last time consumption/overall running time: 458.7744s / 112385.1007 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1099
env0_second_0:                 episode reward: 2.1500,                 loss: -0.0796
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 2527.85,                last time consumption/overall running time: 416.8340s / 112801.9347 s
env0_first_0:                 episode reward: -4.1500,                 loss: -0.1144
env0_second_0:                 episode reward: 4.1500,                 loss: -0.0787
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 2693.9,                last time consumption/overall running time: 444.6522s / 113246.5869 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.1319
env0_second_0:                 episode reward: 5.7500,                 loss: -0.0974
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 2014.05,                last time consumption/overall running time: 333.7437s / 113580.3306 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.1457
env0_second_0:                 episode reward: 8.5500,                 loss: -0.1019
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 2436.8,                last time consumption/overall running time: 405.1580s / 113985.4886 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1202
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0582
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1792.05,                last time consumption/overall running time: 296.7046s / 114282.1933 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1267
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0889
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1878.65,                last time consumption/overall running time: 310.6012s / 114592.7945 s
env0_first_0:                 episode reward: -8.9500,                 loss: -0.1368
env0_second_0:                 episode reward: 8.9500,                 loss: -0.0765
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 2252.05,                last time consumption/overall running time: 372.3080s / 114965.1025 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.1291
env0_second_0:                 episode reward: 7.7500,                 loss: -0.0819
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 2176.6,                last time consumption/overall running time: 359.1036s / 115324.2062 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.1174
env0_second_0:                 episode reward: 7.7000,                 loss: -0.0535
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 2679.55,                last time consumption/overall running time: 445.2817s / 115769.4878 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.1020
env0_second_0:                 episode reward: 4.1000,                 loss: -0.0569
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 2845.2,                last time consumption/overall running time: 471.0100s / 116240.4978 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.0884
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0469
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 3041.9,                last time consumption/overall running time: 492.2986s / 116732.7964 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1049
env0_second_0:                 episode reward: -1.7000,                 loss: -0.0507
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 2805.0,                last time consumption/overall running time: 417.4922s / 117150.2886 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.1125
env0_second_0:                 episode reward: 3.4000,                 loss: -0.0655
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 2907.55,                last time consumption/overall running time: 433.3414s / 117583.6300 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1223
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0609
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 2756.95,                last time consumption/overall running time: 411.4315s / 117995.0615 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1239
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0636
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 2861.3,                last time consumption/overall running time: 423.0788s / 118418.1403 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.1225
env0_second_0:                 episode reward: 2.6500,                 loss: -0.0789
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 2956.1,                last time consumption/overall running time: 436.3494s / 118854.4897 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0952
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0599
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 3155.75,                last time consumption/overall running time: 469.4144s / 119323.9041 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0952
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0609
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 2955.7,                last time consumption/overall running time: 434.3152s / 119758.2193 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1028
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0693
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 2500.95,                last time consumption/overall running time: 377.8724s / 120136.0917 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1058
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0622
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 2577.25,                last time consumption/overall running time: 388.3478s / 120524.4395 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.1169
env0_second_0:                 episode reward: 6.3000,                 loss: -0.0869
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 3039.6,                last time consumption/overall running time: 449.3137s / 120973.7532 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.1050
env0_second_0:                 episode reward: 2.8000,                 loss: -0.0419
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 2437.05,                last time consumption/overall running time: 360.2503s / 121334.0035 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1098
env0_second_0:                 episode reward: 6.1500,                 loss: -0.0835
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 2889.25,                last time consumption/overall running time: 428.6149s / 121762.6185 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.1297
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0866
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 2518.55,                last time consumption/overall running time: 371.4181s / 122134.0366 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.1199
env0_second_0:                 episode reward: 5.9000,                 loss: -0.0785
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 2702.7,                last time consumption/overall running time: 400.5008s / 122534.5374 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.1356
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0867
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 2714.75,                last time consumption/overall running time: 398.9379s / 122933.4753 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1230
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0793
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 2626.2,                last time consumption/overall running time: 396.5382s / 123330.0134 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.1375
env0_second_0:                 episode reward: 5.5500,                 loss: -0.0902
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 2714.95,                last time consumption/overall running time: 406.1578s / 123736.1712 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.1406
env0_second_0:                 episode reward: 3.6500,                 loss: -0.0878
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 2528.6,                last time consumption/overall running time: 371.7029s / 124107.8741 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.1293
env0_second_0:                 episode reward: 4.4500,                 loss: -0.0882
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 2662.7,                last time consumption/overall running time: 392.0883s / 124499.9624 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1434
env0_second_0:                 episode reward: 4.4000,                 loss: -0.0868
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 2977.4,                last time consumption/overall running time: 436.7592s / 124936.7216 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.1229
env0_second_0:                 episode reward: 2.0000,                 loss: -0.0653
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 2726.9,                last time consumption/overall running time: 404.1543s / 125340.8759 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1244
env0_second_0:                 episode reward: 4.4000,                 loss: -0.0713
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 2852.5,                last time consumption/overall running time: 427.5934s / 125768.4692 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1250
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0597
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 2542.05,                last time consumption/overall running time: 375.9531s / 126144.4224 s
env0_first_0:                 episode reward: -4.5500,                 loss: -0.1422
env0_second_0:                 episode reward: 4.5500,                 loss: -0.0731
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 2583.15,                last time consumption/overall running time: 385.1624s / 126529.5847 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.1365
env0_second_0:                 episode reward: 5.8500,                 loss: -0.0801
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 2680.9,                last time consumption/overall running time: 399.3132s / 126928.8979 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1341
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0778
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 2603.0,                last time consumption/overall running time: 389.8525s / 127318.7504 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1419
env0_second_0:                 episode reward: 4.2000,                 loss: -0.0898
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 2261.4,                last time consumption/overall running time: 340.8145s / 127659.5649 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.1474
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0966
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 2677.85,                last time consumption/overall running time: 402.1440s / 128061.7089 s
env0_first_0:                 episode reward: -5.6000,                 loss: -0.1458
env0_second_0:                 episode reward: 5.6000,                 loss: -0.0957
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 2710.75,                last time consumption/overall running time: 404.8615s / 128466.5704 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.1400
env0_second_0:                 episode reward: 4.7000,                 loss: -0.0910
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 2959.15,                last time consumption/overall running time: 440.5110s / 128907.0814 s
env0_first_0:                 episode reward: -5.1500,                 loss: -0.1388
env0_second_0:                 episode reward: 5.1500,                 loss: -0.0916
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 3157.6,                last time consumption/overall running time: 469.4334s / 129376.5148 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.1304
env0_second_0:                 episode reward: 3.9500,                 loss: -0.0619
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 3048.25,                last time consumption/overall running time: 458.2370s / 129834.7518 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.1423
env0_second_0:                 episode reward: 2.7000,                 loss: -0.0858
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 3038.1,                last time consumption/overall running time: 452.7189s / 130287.4707 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.1438
env0_second_0:                 episode reward: 2.6000,                 loss: -0.0917
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 2700.65,                last time consumption/overall running time: 411.3566s / 130698.8273 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.1579
env0_second_0:                 episode reward: 3.7000,                 loss: -0.0936
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 2833.55,                last time consumption/overall running time: 427.8969s / 131126.7242 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.1521
env0_second_0:                 episode reward: 3.7000,                 loss: -0.0852
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 2978.5,                last time consumption/overall running time: 439.3378s / 131566.0621 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.1480
env0_second_0:                 episode reward: 2.2500,                 loss: -0.0697
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 2973.75,                last time consumption/overall running time: 437.2948s / 132003.3569 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.1482
env0_second_0:                 episode reward: 1.7500,                 loss: -0.0735
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 3470.8,                last time consumption/overall running time: 474.2618s / 132477.6187 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1303
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0447
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 3157.6,                last time consumption/overall running time: 425.2831s / 132902.9017 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1445
env0_second_0:                 episode reward: 3.7500,                 loss: -0.0707
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 3432.25,                last time consumption/overall running time: 461.9690s / 133364.8707 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.1336
env0_second_0:                 episode reward: 4.1000,                 loss: -0.0702
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 3588.2,                last time consumption/overall running time: 484.0514s / 133848.9221 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1310
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0661
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 3159.1,                last time consumption/overall running time: 431.3566s / 134280.2787 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.1403
env0_second_0:                 episode reward: 2.8500,                 loss: -0.0594
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 2965.95,                last time consumption/overall running time: 409.5535s / 134689.8322 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1274
env0_second_0:                 episode reward: 2.4500,                 loss: -0.0158
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 2862.5,                last time consumption/overall running time: 389.7262s / 135079.5584 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.1402
env0_second_0:                 episode reward: 5.4000,                 loss: -0.0436
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 2787.6,                last time consumption/overall running time: 381.9322s / 135461.4906 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.1469
env0_second_0:                 episode reward: 5.0500,                 loss: -0.0318
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 2503.1,                last time consumption/overall running time: 339.6832s / 135801.1739 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1475
env0_second_0:                 episode reward: 6.1500,                 loss: -0.0404
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 2646.4,                last time consumption/overall running time: 352.0967s / 136153.2706 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.1446
env0_second_0:                 episode reward: 3.9500,                 loss: -0.0438
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 2672.6,                last time consumption/overall running time: 377.4346s / 136530.7052 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1719
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0976
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 2702.55,                last time consumption/overall running time: 372.7829s / 136903.4881 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.1746
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0902
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 2712.5,                last time consumption/overall running time: 365.1594s / 137268.6475 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1614
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0857
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 2689.05,                last time consumption/overall running time: 366.6244s / 137635.2719 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.1667
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0905
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 2708.85,                last time consumption/overall running time: 359.9363s / 137995.2082 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.1566
env0_second_0:                 episode reward: 6.7500,                 loss: -0.0841
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 2686.55,                last time consumption/overall running time: 364.0399s / 138359.2480 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.1598
env0_second_0:                 episode reward: 8.6500,                 loss: -0.0840
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 2699.1,                last time consumption/overall running time: 361.2915s / 138720.5396 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.1676
env0_second_0:                 episode reward: 7.7000,                 loss: -0.1005
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 2974.6,                last time consumption/overall running time: 385.8383s / 139106.3778 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1606
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0951
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 3412.0,                last time consumption/overall running time: 456.1424s / 139562.5202 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1508
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0785
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 2946.6,                last time consumption/overall running time: 396.2735s / 139958.7937 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.1462
env0_second_0:                 episode reward: 6.2000,                 loss: -0.0841
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 2817.05,                last time consumption/overall running time: 374.3000s / 140333.0937 s
env0_first_0:                 episode reward: -6.9000,                 loss: -0.1596
env0_second_0:                 episode reward: 6.9000,                 loss: -0.0994
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 3374.0,                last time consumption/overall running time: 448.3506s / 140781.4443 s
env0_first_0:                 episode reward: -5.6500,                 loss: -0.1457
env0_second_0:                 episode reward: 5.6500,                 loss: -0.0860
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 2955.3,                last time consumption/overall running time: 365.7091s / 141147.1534 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.1394
env0_second_0:                 episode reward: 6.2500,                 loss: -0.0882
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 3479.95,                last time consumption/overall running time: 426.7460s / 141573.8994 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.1339
env0_second_0:                 episode reward: 5.7500,                 loss: -0.0394
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 3620.75,                last time consumption/overall running time: 442.0515s / 142015.9509 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1398
env0_second_0:                 episode reward: 3.7500,                 loss: -0.0599
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 3303.1,                last time consumption/overall running time: 407.7316s / 142423.6825 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.1559
env0_second_0:                 episode reward: 6.3000,                 loss: -0.0821
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 3525.65,                last time consumption/overall running time: 437.1882s / 142860.8706 s
env0_first_0:                 episode reward: -4.8000,                 loss: -0.1436
env0_second_0:                 episode reward: 4.8000,                 loss: -0.0715
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 3738.45,                last time consumption/overall running time: 464.1759s / 143325.0465 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1462
env0_second_0:                 episode reward: 4.2000,                 loss: -0.0857
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 3400.35,                last time consumption/overall running time: 422.5325s / 143747.5790 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1435
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0878
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 3627.35,                last time consumption/overall running time: 442.4089s / 144189.9879 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1445
env0_second_0:                 episode reward: 4.2000,                 loss: -0.0900
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 3640.9,                last time consumption/overall running time: 448.8103s / 144638.7982 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.1512
env0_second_0:                 episode reward: 2.3000,                 loss: -0.1004
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 3310.45,                last time consumption/overall running time: 408.7034s / 145047.5016 s
env0_first_0:                 episode reward: -6.5500,                 loss: -0.1569
env0_second_0:                 episode reward: 6.5500,                 loss: -0.1051
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 3814.45,                last time consumption/overall running time: 476.9856s / 145524.4872 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1446
env0_second_0:                 episode reward: 4.4000,                 loss: -0.0752
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 3474.1,                last time consumption/overall running time: 429.6745s / 145954.1618 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.1538
env0_second_0:                 episode reward: 5.9000,                 loss: -0.1027
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 3289.3,                last time consumption/overall running time: 427.9529s / 146382.1146 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.1542
env0_second_0:                 episode reward: 7.3500,                 loss: -0.1001
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 3178.7,                last time consumption/overall running time: 393.6113s / 146775.7259 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1557
env0_second_0:                 episode reward: 7.1000,                 loss: -0.0957
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 3732.75,                last time consumption/overall running time: 461.9515s / 147237.6774 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1341
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0681
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 2791.7,                last time consumption/overall running time: 346.4831s / 147584.1605 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.1465
env0_second_0:                 episode reward: 8.3000,                 loss: -0.0818
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 2617.9,                last time consumption/overall running time: 331.0263s / 147915.1869 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.1351
env0_second_0:                 episode reward: 7.4000,                 loss: -0.0387
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 2991.35,                last time consumption/overall running time: 375.5443s / 148290.7312 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.1336
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0608
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 2665.65,                last time consumption/overall running time: 334.3423s / 148625.0735 s
env0_first_0:                 episode reward: -7.3000,                 loss: -0.1440
env0_second_0:                 episode reward: 7.3000,                 loss: -0.0638
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 2886.8,                last time consumption/overall running time: 361.6886s / 148986.7621 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.1462
env0_second_0:                 episode reward: 6.1000,                 loss: -0.0526
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 3570.35,                last time consumption/overall running time: 446.3098s / 149433.0719 s
env0_first_0:                 episode reward: -4.9000,                 loss: -0.1181
env0_second_0:                 episode reward: 4.9000,                 loss: -0.0342
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 3389.95,                last time consumption/overall running time: 410.7311s / 149843.8029 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1161
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0628
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 2752.6,                last time consumption/overall running time: 341.9163s / 150185.7192 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.1414
env0_second_0:                 episode reward: 8.2500,                 loss: -0.0755
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 3284.45,                last time consumption/overall running time: 405.0924s / 150590.8117 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.1344
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0762
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 3593.8,                last time consumption/overall running time: 441.4028s / 151032.2145 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1364
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0796
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 3440.1,                last time consumption/overall running time: 423.9148s / 151456.1293 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1378
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0667
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 3631.05,                last time consumption/overall running time: 448.5680s / 151904.6973 s
env0_first_0:                 episode reward: -6.5500,                 loss: -0.1279
env0_second_0:                 episode reward: 6.5500,                 loss: -0.0821
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 3462.4,                last time consumption/overall running time: 425.6295s / 152330.3268 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.1421
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0936
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 3692.75,                last time consumption/overall running time: 448.0588s / 152778.3855 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1269
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0798
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 3000.35,                last time consumption/overall running time: 373.1726s / 153151.5581 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1398
env0_second_0:                 episode reward: 7.1000,                 loss: -0.0841
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 3110.35,                last time consumption/overall running time: 388.4507s / 153540.0089 s
env0_first_0:                 episode reward: -7.3000,                 loss: -0.1317
env0_second_0:                 episode reward: 7.3000,                 loss: -0.0571
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 3805.0,                last time consumption/overall running time: 468.7362s / 154008.7450 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.1353
env0_second_0:                 episode reward: 6.0000,                 loss: -0.0785
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 2839.8,                last time consumption/overall running time: 346.0860s / 154354.8310 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.1457
env0_second_0:                 episode reward: 7.4000,                 loss: -0.0875
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 2787.55,                last time consumption/overall running time: 341.6417s / 154696.4727 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.1494
env0_second_0:                 episode reward: 8.6500,                 loss: -0.0875
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 3587.7,                last time consumption/overall running time: 439.8253s / 155136.2980 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.1359
env0_second_0:                 episode reward: 6.6000,                 loss: -0.0773
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 3167.05,                last time consumption/overall running time: 403.5052s / 155539.8033 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.1473
env0_second_0:                 episode reward: 8.7500,                 loss: -0.1060
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 3386.8,                last time consumption/overall running time: 420.2977s / 155960.1009 s
env0_first_0:                 episode reward: -7.5000,                 loss: -0.1476
env0_second_0:                 episode reward: 7.5000,                 loss: -0.1001
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 3230.5,                last time consumption/overall running time: 403.6341s / 156363.7350 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.1379
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0808
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 3147.5,                last time consumption/overall running time: 388.0851s / 156751.8201 s
env0_first_0:                 episode reward: -7.0500,                 loss: -0.1677
env0_second_0:                 episode reward: 7.0500,                 loss: -0.1094
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 3356.8,                last time consumption/overall running time: 416.6447s / 157168.4649 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1502
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0929
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 3281.85,                last time consumption/overall running time: 409.3638s / 157577.8287 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.1497
env0_second_0:                 episode reward: 6.2000,                 loss: -0.0889
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 3652.35,                last time consumption/overall running time: 449.4486s / 158027.2773 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1459
env0_second_0:                 episode reward: 5.4500,                 loss: -0.0859
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 3347.75,                last time consumption/overall running time: 414.9225s / 158442.1998 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.1510
env0_second_0:                 episode reward: 6.2500,                 loss: -0.0895
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 3123.4,                last time consumption/overall running time: 387.0227s / 158829.2225 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1590
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0956
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 3479.15,                last time consumption/overall running time: 438.0441s / 159267.2666 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.1426
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0939
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 3619.6,                last time consumption/overall running time: 450.9365s / 159718.2030 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.1446
env0_second_0:                 episode reward: 4.0000,                 loss: -0.0881
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 3579.25,                last time consumption/overall running time: 454.8464s / 160173.0494 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.1290
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0613
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 2414.2,                last time consumption/overall running time: 304.2283s / 160477.2777 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.1331
env0_second_0:                 episode reward: 7.8500,                 loss: -0.0746
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 3508.9,                last time consumption/overall running time: 443.6940s / 160920.9717 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.1317
env0_second_0:                 episode reward: 4.7000,                 loss: -0.0728
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 3662.15,                last time consumption/overall running time: 463.8153s / 161384.7870 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.1297
env0_second_0:                 episode reward: 5.5000,                 loss: -0.0675
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 3259.25,                last time consumption/overall running time: 407.3255s / 161792.1125 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1296
env0_second_0:                 episode reward: 6.1500,                 loss: -0.0757
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 3068.55,                last time consumption/overall running time: 379.4170s / 162171.5295 s
env0_first_0:                 episode reward: -6.4000,                 loss: -0.1317
env0_second_0:                 episode reward: 6.4000,                 loss: -0.0760
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 3485.45,                last time consumption/overall running time: 431.2098s / 162602.7393 s
env0_first_0:                 episode reward: -4.2500,                 loss: -0.1039
env0_second_0:                 episode reward: 4.2500,                 loss: -0.0565
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 3298.25,                last time consumption/overall running time: 408.3242s / 163011.0635 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1119
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0706
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 3135.85,                last time consumption/overall running time: 391.6795s / 163402.7429 s
env0_first_0:                 episode reward: -7.9500,                 loss: -0.1265
env0_second_0:                 episode reward: 7.9500,                 loss: -0.0777
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 3051.2,                last time consumption/overall running time: 372.5311s / 163775.2740 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.1356
env0_second_0:                 episode reward: 7.6000,                 loss: -0.1026
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 2934.55,                last time consumption/overall running time: 362.1969s / 164137.4709 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.1371
env0_second_0:                 episode reward: 7.6000,                 loss: -0.0834
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 3411.3,                last time consumption/overall running time: 441.1580s / 164578.6289 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.1190
env0_second_0:                 episode reward: 5.5500,                 loss: -0.0694
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 3722.0,                last time consumption/overall running time: 668.0771s / 165246.7060 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.1135
env0_second_0:                 episode reward: 4.3000,                 loss: -0.0615
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 3530.15,                last time consumption/overall running time: 767.3680s / 166014.0740 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.1379
env0_second_0:                 episode reward: 7.0000,                 loss: -0.0704
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 3589.75,                last time consumption/overall running time: 779.7774s / 166793.8514 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.1231
env0_second_0:                 episode reward: 6.2500,                 loss: -0.0688
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 3616.25,                last time consumption/overall running time: 782.6259s / 167576.4773 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.1402
env0_second_0:                 episode reward: 7.0000,                 loss: -0.0838
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 3615.2,                last time consumption/overall running time: 787.9375s / 168364.4147 s
env0_first_0:                 episode reward: -7.0500,                 loss: -0.1483
env0_second_0:                 episode reward: 7.0500,                 loss: -0.0850
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 3578.8,                last time consumption/overall running time: 773.8881s / 169138.3029 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.1466
env0_second_0:                 episode reward: 5.9000,                 loss: -0.0865
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 3652.75,                last time consumption/overall running time: 789.5299s / 169927.8328 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.1480
env0_second_0:                 episode reward: 6.1000,                 loss: -0.0944
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 3224.5,                last time consumption/overall running time: 699.2054s / 170627.0382 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.1428
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0825
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 3231.7,                last time consumption/overall running time: 697.4907s / 171324.5289 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.1491
env0_second_0:                 episode reward: 6.6000,                 loss: -0.1066
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 3309.75,                last time consumption/overall running time: 718.4354s / 172042.9644 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.1434
env0_second_0:                 episode reward: 7.5500,                 loss: -0.0836
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 3724.35,                last time consumption/overall running time: 805.3049s / 172848.2693 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.1387
env0_second_0:                 episode reward: 6.3000,                 loss: -0.0869
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 4214.0,                last time consumption/overall running time: 906.6484s / 173754.9177 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.1357
env0_second_0:                 episode reward: 3.3000,                 loss: -0.0803
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 3672.2,                last time consumption/overall running time: 792.7668s / 174547.6845 s
env0_first_0:                 episode reward: -4.7500,                 loss: -0.1384
env0_second_0:                 episode reward: 4.7500,                 loss: -0.0697
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 3695.8,                last time consumption/overall running time: 795.9676s / 175343.6521 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.1386
env0_second_0:                 episode reward: 6.2500,                 loss: -0.0881
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 3406.75,                last time consumption/overall running time: 761.3147s / 176104.9668 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.1503
env0_second_0:                 episode reward: 5.5500,                 loss: -0.0915
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 3477.65,                last time consumption/overall running time: 834.4747s / 176939.4415 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1545
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0969
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 3355.65,                last time consumption/overall running time: 810.1518s / 177749.5933 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.1512
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0894
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 3358.4,                last time consumption/overall running time: 807.6794s / 178557.2727 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.1624
env0_second_0:                 episode reward: 6.0000,                 loss: -0.0966
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 3284.1,                last time consumption/overall running time: 847.7166s / 179404.9894 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1611
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0878
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 3547.15,                last time consumption/overall running time: 915.3513s / 180320.3407 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.1423
env0_second_0:                 episode reward: 5.5500,                 loss: -0.0747
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 3714.25,                last time consumption/overall running time: 957.5118s / 181277.8525 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.1385
env0_second_0:                 episode reward: 5.8500,                 loss: -0.0871
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 2676.4,                last time consumption/overall running time: 694.7624s / 181972.6149 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.1519
env0_second_0:                 episode reward: 9.0500,                 loss: -0.0910
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 3063.45,                last time consumption/overall running time: 788.8297s / 182761.4446 s
env0_first_0:                 episode reward: -7.0500,                 loss: -0.1320
env0_second_0:                 episode reward: 7.0500,                 loss: -0.0524
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 3583.6,                last time consumption/overall running time: 920.1372s / 183681.5818 s
env0_first_0:                 episode reward: -6.6500,                 loss: -0.1402
env0_second_0:                 episode reward: 6.6500,                 loss: -0.0770
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 3074.2,                last time consumption/overall running time: 790.1296s / 184471.7114 s
env0_first_0:                 episode reward: -7.0500,                 loss: -0.1562
env0_second_0:                 episode reward: 7.0500,                 loss: -0.0989
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 3702.4,                last time consumption/overall running time: 949.6146s / 185421.3260 s
env0_first_0:                 episode reward: -4.8000,                 loss: -0.1429
env0_second_0:                 episode reward: 4.8000,                 loss: -0.0794
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 2850.0,                last time consumption/overall running time: 726.4111s / 186147.7371 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.1368
env0_second_0:                 episode reward: 6.7500,                 loss: -0.0819
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 2994.3,                last time consumption/overall running time: 712.5819s / 186860.3190 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.1302
env0_second_0:                 episode reward: 6.5000,                 loss: -0.0635
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 3333.35,                last time consumption/overall running time: 793.2170s / 187653.5360 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.1339
env0_second_0:                 episode reward: 5.4000,                 loss: -0.0854
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 3801.85,                last time consumption/overall running time: 898.0017s / 188551.5377 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.1390
env0_second_0:                 episode reward: 3.5500,                 loss: -0.0725
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 3664.15,                last time consumption/overall running time: 863.3875s / 189414.9252 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1423
env0_second_0:                 episode reward: 5.4500,                 loss: -0.0858
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 3295.95,                last time consumption/overall running time: 779.9271s / 190194.8523 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.1346
env0_second_0:                 episode reward: 6.3000,                 loss: -0.0696
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 3895.45,                last time consumption/overall running time: 920.9453s / 191115.7976 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.1268
env0_second_0:                 episode reward: 5.5500,                 loss: -0.0651
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 4031.0,                last time consumption/overall running time: 952.6677s / 192068.4653 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.1155
env0_second_0:                 episode reward: 5.5500,                 loss: -0.0628
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 3870.85,                last time consumption/overall running time: 915.4440s / 192983.9093 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1343
env0_second_0:                 episode reward: 5.4500,                 loss: -0.0653
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 3660.05,                last time consumption/overall running time: 862.7222s / 193846.6315 s
env0_first_0:                 episode reward: -5.1500,                 loss: -0.1345
env0_second_0:                 episode reward: 5.1500,                 loss: -0.0617
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 2846.95,                last time consumption/overall running time: 681.8993s / 194528.5308 s
env0_first_0:                 episode reward: -7.9000,                 loss: -0.1524
env0_second_0:                 episode reward: 7.9000,                 loss: -0.0867
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 3220.1,                last time consumption/overall running time: 767.1723s / 195295.7031 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1362
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0573
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 3562.6,                last time consumption/overall running time: 846.5745s / 196142.2776 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.1279
env0_second_0:                 episode reward: 5.1000,                 loss: -0.0413
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 3871.55,                last time consumption/overall running time: 921.2259s / 197063.5035 s
env0_first_0:                 episode reward: -4.8000,                 loss: -0.1308
env0_second_0:                 episode reward: 4.8000,                 loss: -0.0667
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 2882.85,                last time consumption/overall running time: 687.9592s / 197751.4627 s
env0_first_0:                 episode reward: -6.6500,                 loss: -0.1447
env0_second_0:                 episode reward: 6.6500,                 loss: -0.0592
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 3453.55,                last time consumption/overall running time: 822.1050s / 198573.5677 s
env0_first_0:                 episode reward: -5.2000,                 loss: -0.1406
env0_second_0:                 episode reward: 5.2000,                 loss: -0.0675
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 3461.95,                last time consumption/overall running time: 821.4057s / 199394.9734 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.1324
env0_second_0:                 episode reward: 3.2000,                 loss: -0.0606
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 3517.25,                last time consumption/overall running time: 834.3035s / 200229.2768 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.1440
env0_second_0:                 episode reward: 3.9500,                 loss: -0.0800
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 3750.1,                last time consumption/overall running time: 887.8537s / 201117.1306 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.1269
env0_second_0:                 episode reward: 4.3500,                 loss: -0.0623
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 4283.25,                last time consumption/overall running time: 1006.6190s / 202123.7495 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.1179
env0_second_0:                 episode reward: 2.0000,                 loss: -0.0614
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 4285.0,                last time consumption/overall running time: 1010.6561s / 203134.4056 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1250
env0_second_0:                 episode reward: 1.3000,                 loss: -0.0695
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 3776.3,                last time consumption/overall running time: 892.5875s / 204026.9931 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.1457
env0_second_0:                 episode reward: 3.7000,                 loss: -0.0806
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 3435.8,                last time consumption/overall running time: 812.0862s / 204839.0793 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.1551
env0_second_0:                 episode reward: 5.5000,                 loss: -0.0878
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 3549.15,                last time consumption/overall running time: 834.0660s / 205673.1453 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.1589
env0_second_0:                 episode reward: 2.8000,                 loss: -0.0962
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 3469.75,                last time consumption/overall running time: 819.2458s / 206492.3911 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.1559
env0_second_0:                 episode reward: 3.6000,                 loss: -0.0841
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 3677.55,                last time consumption/overall running time: 868.4107s / 207360.8018 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.1503
env0_second_0:                 episode reward: 2.7000,                 loss: -0.0690
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 3624.55,                last time consumption/overall running time: 852.5322s / 208213.3339 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.1502
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0738
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 3636.5,                last time consumption/overall running time: 859.2767s / 209072.6106 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.1530
env0_second_0:                 episode reward: 4.8500,                 loss: -0.0789
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 3759.25,                last time consumption/overall running time: 885.0607s / 209957.6713 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.1518
env0_second_0:                 episode reward: 4.9500,                 loss: -0.0746
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 3993.05,                last time consumption/overall running time: 937.2516s / 210894.9229 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.1477
env0_second_0:                 episode reward: 3.4000,                 loss: -0.0741
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 3740.0,                last time consumption/overall running time: 882.2029s / 211777.1258 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1443
env0_second_0:                 episode reward: 5.4500,                 loss: -0.0757
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 3267.45,                last time consumption/overall running time: 772.7048s / 212549.8306 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.1578
env0_second_0:                 episode reward: 3.0500,                 loss: -0.0879
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 3519.6,                last time consumption/overall running time: 828.4616s / 213378.2923 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.1536
env0_second_0:                 episode reward: 5.2500,                 loss: -0.0814
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 2590.1,                last time consumption/overall running time: 616.4397s / 213994.7319 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.1576
env0_second_0:                 episode reward: 7.4000,                 loss: -0.0495
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 2866.45,                last time consumption/overall running time: 676.1267s / 214670.8586 s
env0_first_0:                 episode reward: -5.6000,                 loss: -0.1414
env0_second_0:                 episode reward: 5.6000,                 loss: -0.0607
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 3482.3,                last time consumption/overall running time: 822.7521s / 215493.6107 s
env0_first_0:                 episode reward: -5.2000,                 loss: -0.1523
env0_second_0:                 episode reward: 5.2000,                 loss: -0.0667
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 3512.35,                last time consumption/overall running time: 809.0912s / 216302.7019 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.1504
env0_second_0:                 episode reward: 3.3500,                 loss: -0.0753
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 3656.15,                last time consumption/overall running time: 808.5139s / 217111.2158 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.1426
env0_second_0:                 episode reward: 4.3500,                 loss: -0.0625
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 3329.75,                last time consumption/overall running time: 734.5606s / 217845.7764 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1553
env0_second_0:                 episode reward: 5.4500,                 loss: -0.0732
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 2274.8,                last time consumption/overall running time: 507.9506s / 218353.7270 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.1518
env0_second_0:                 episode reward: 5.9000,                 loss: -0.0730
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 2366.2,                last time consumption/overall running time: 526.6160s / 218880.3430 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.1488
env0_second_0:                 episode reward: 4.3000,                 loss: -0.0856
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 2131.95,                last time consumption/overall running time: 477.4541s / 219357.7971 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.1412
env0_second_0:                 episode reward: 5.7500,                 loss: -0.0912
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 1973.65,                last time consumption/overall running time: 441.1025s / 219798.8997 s
env0_first_0:                 episode reward: -6.4500,                 loss: -0.1661
env0_second_0:                 episode reward: 6.4500,                 loss: -0.1098
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 2264.05,                last time consumption/overall running time: 505.3827s / 220304.2824 s
env0_first_0:                 episode reward: -6.4500,                 loss: -0.1629
env0_second_0:                 episode reward: 6.4500,                 loss: -0.0850
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 1790.9,                last time consumption/overall running time: 401.8502s / 220706.1327 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.1527
env0_second_0:                 episode reward: 8.5500,                 loss: -0.0705
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 2247.9,                last time consumption/overall running time: 500.4793s / 221206.6119 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.1484
env0_second_0:                 episode reward: 6.7500,                 loss: -0.0624
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 1987.15,                last time consumption/overall running time: 443.4115s / 221650.0234 s
env0_first_0:                 episode reward: -7.9500,                 loss: -0.1522
env0_second_0:                 episode reward: 7.9500,                 loss: -0.0694
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 2226.65,                last time consumption/overall running time: 496.1368s / 222146.1602 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1541
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0754
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 2105.15,                last time consumption/overall running time: 469.6054s / 222615.7656 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1555
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0647
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 2058.55,                last time consumption/overall running time: 460.8223s / 223076.5879 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.1378
env0_second_0:                 episode reward: 5.7500,                 loss: -0.0590
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 2367.75,                last time consumption/overall running time: 526.2639s / 223602.8518 s
env0_first_0:                 episode reward: -7.9000,                 loss: -0.1590
env0_second_0:                 episode reward: 7.9000,                 loss: -0.0664
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 2179.95,                last time consumption/overall running time: 486.6187s / 224089.4705 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.1557
env0_second_0:                 episode reward: 8.2000,                 loss: -0.0624
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 1907.45,                last time consumption/overall running time: 429.0368s / 224518.5073 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.1534
env0_second_0:                 episode reward: 8.5500,                 loss: -0.0872
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 2186.75,                last time consumption/overall running time: 488.6668s / 225007.1741 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1655
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0906
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 2187.8,                last time consumption/overall running time: 488.7925s / 225495.9666 s
env0_first_0:                 episode reward: -7.3000,                 loss: -0.1540
env0_second_0:                 episode reward: 7.3000,                 loss: -0.0818
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 2186.3,                last time consumption/overall running time: 488.8920s / 225984.8586 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.1635
env0_second_0:                 episode reward: 7.7500,                 loss: -0.0933
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 1696.85,                last time consumption/overall running time: 383.3882s / 226368.2467 s
env0_first_0:                 episode reward: -9.1500,                 loss: -0.1467
env0_second_0:                 episode reward: 9.1500,                 loss: -0.0919
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 1569.5,                last time consumption/overall running time: 355.7423s / 226723.9891 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.1443
env0_second_0:                 episode reward: 8.6500,                 loss: -0.0910
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 2057.95,                last time consumption/overall running time: 462.3227s / 227186.3117 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.1478
env0_second_0:                 episode reward: 7.3500,                 loss: -0.0772
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 2102.25,                last time consumption/overall running time: 472.1644s / 227658.4761 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.1329
env0_second_0:                 episode reward: 5.8500,                 loss: -0.0692
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 1965.05,                last time consumption/overall running time: 442.0796s / 228100.5558 s
env0_first_0:                 episode reward: -4.9000,                 loss: -0.1164
env0_second_0:                 episode reward: 4.9000,                 loss: -0.0667
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 2242.0,                last time consumption/overall running time: 500.9110s / 228601.4668 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.1304
env0_second_0:                 episode reward: 6.1000,                 loss: -0.0698
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 2387.75,                last time consumption/overall running time: 533.4038s / 229134.8705 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.1585
env0_second_0:                 episode reward: 7.7500,                 loss: 0.0369
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 2377.1,                last time consumption/overall running time: 529.3849s / 229664.2554 s
env0_first_0:                 episode reward: -7.5000,                 loss: -0.1578
env0_second_0:                 episode reward: 7.5000,                 loss: -0.0461
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 2264.65,                last time consumption/overall running time: 505.8700s / 230170.1254 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.1667
env0_second_0:                 episode reward: 8.0000,                 loss: -0.0673
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 2364.05,                last time consumption/overall running time: 527.2352s / 230697.3606 s
env0_first_0:                 episode reward: -7.9000,                 loss: -0.1741
env0_second_0:                 episode reward: 7.9000,                 loss: -0.0887
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 2487.2,                last time consumption/overall running time: 554.6218s / 231251.9824 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.1608
env0_second_0:                 episode reward: 8.3000,                 loss: -0.0534
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 2739.55,                last time consumption/overall running time: 610.6060s / 231862.5883 s
env0_first_0:                 episode reward: -6.4500,                 loss: -0.1502
env0_second_0:                 episode reward: 6.4500,                 loss: -0.0468
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 2709.75,                last time consumption/overall running time: 580.7381s / 232443.3264 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.1593
env0_second_0:                 episode reward: 5.3000,                 loss: -0.0662
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 2287.85,                last time consumption/overall running time: 473.4568s / 232916.7832 s
env0_first_0:                 episode reward: -8.1500,                 loss: -0.1713
env0_second_0:                 episode reward: 8.1500,                 loss: -0.0883
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 2229.4,                last time consumption/overall running time: 464.4441s / 233381.2273 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.1440
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0726
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 2052.0,                last time consumption/overall running time: 426.1584s / 233807.3857 s
env0_first_0:                 episode reward: -7.1500,                 loss: -0.1378
env0_second_0:                 episode reward: 7.1500,                 loss: -0.0800
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 2277.2,                last time consumption/overall running time: 472.0656s / 234279.4513 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.1483
env0_second_0:                 episode reward: 8.5500,                 loss: -0.0560
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 2222.9,                last time consumption/overall running time: 460.9738s / 234740.4251 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1553
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0781
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1872.6,                last time consumption/overall running time: 392.2179s / 235132.6430 s
env0_first_0:                 episode reward: -7.9000,                 loss: -0.1549
env0_second_0:                 episode reward: 7.9000,                 loss: -0.0877
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 1938.3,                last time consumption/overall running time: 403.0869s / 235535.7299 s
env0_first_0:                 episode reward: -6.5500,                 loss: -0.1538
env0_second_0:                 episode reward: 6.5500,                 loss: -0.0961
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 2112.15,                last time consumption/overall running time: 437.3505s / 235973.0804 s
env0_first_0:                 episode reward: -8.1500,                 loss: -0.1609
env0_second_0:                 episode reward: 8.1500,                 loss: -0.0879
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 2193.25,                last time consumption/overall running time: 455.9042s / 236428.9846 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.1496
env0_second_0:                 episode reward: 7.0000,                 loss: -0.0825
env1_first_0:                 episode reward: -8.2500,                 loss: nan
env1_second_0:                 episode reward: 8.2500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 2255.7,                last time consumption/overall running time: 467.6461s / 236896.6307 s
env0_first_0:                 episode reward: -7.5000,                 loss: -0.1563
env0_second_0:                 episode reward: 7.5000,                 loss: -0.0939
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 2088.15,                last time consumption/overall running time: 432.6350s / 237329.2657 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.1713
env0_second_0:                 episode reward: 8.2500,                 loss: -0.1139
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 2198.25,                last time consumption/overall running time: 455.3120s / 237784.5777 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.1787
env0_second_0:                 episode reward: 8.3500,                 loss: -0.1158
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 1899.35,                last time consumption/overall running time: 396.0392s / 238180.6169 s
env0_first_0:                 episode reward: -7.9000,                 loss: -0.1500
env0_second_0:                 episode reward: 7.9000,                 loss: -0.0987
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 1778.35,                last time consumption/overall running time: 369.9369s / 238550.5538 s
env0_first_0:                 episode reward: -9.1000,                 loss: -0.1620
env0_second_0:                 episode reward: 9.1000,                 loss: -0.1022
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 1692.5,                last time consumption/overall running time: 354.2817s / 238904.8354 s
env0_first_0:                 episode reward: -8.1500,                 loss: -0.1627
env0_second_0:                 episode reward: 8.1500,                 loss: -0.0947
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 1905.95,                last time consumption/overall running time: 396.6126s / 239301.4480 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.1491
env0_second_0:                 episode reward: 7.3500,                 loss: -0.0717
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 1939.3,                last time consumption/overall running time: 404.0879s / 239705.5359 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.1675
env0_second_0:                 episode reward: 8.6000,                 loss: -0.0910
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 1949.95,                last time consumption/overall running time: 406.6594s / 240112.1953 s
env0_first_0:                 episode reward: -7.0500,                 loss: -0.1434
env0_second_0:                 episode reward: 7.0500,                 loss: -0.0651
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 1529.65,                last time consumption/overall running time: 321.8508s / 240434.0461 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.1460
env0_second_0:                 episode reward: 9.0000,                 loss: -0.0786
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 1775.0,                last time consumption/overall running time: 369.5054s / 240803.5515 s
env0_first_0:                 episode reward: -8.5000,                 loss: -0.1510
env0_second_0:                 episode reward: 8.5000,                 loss: -0.0924
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 2095.5,                last time consumption/overall running time: 433.7729s / 241237.3244 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1427
env0_second_0:                 episode reward: 6.1500,                 loss: -0.0451
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 3073.4,                last time consumption/overall running time: 632.2533s / 241869.5777 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.1310
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0062
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 2063.75,                last time consumption/overall running time: 428.9875s / 242298.5652 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.1501
env0_second_0:                 episode reward: 8.3500,                 loss: -0.0708
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 1744.4,                last time consumption/overall running time: 364.1931s / 242662.7583 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.1464
env0_second_0:                 episode reward: 8.3000,                 loss: -0.0925
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 1881.6,                last time consumption/overall running time: 391.3498s / 243054.1081 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.1431
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0875
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 1432.2,                last time consumption/overall running time: 301.5932s / 243355.7013 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1585
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0869
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 1815.25,                last time consumption/overall running time: 378.3238s / 243734.0251 s
env0_first_0:                 episode reward: -7.9000,                 loss: -0.1285
env0_second_0:                 episode reward: 7.9000,                 loss: -0.0408
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 2132.15,                last time consumption/overall running time: 443.2559s / 244177.2810 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.1356
env0_second_0:                 episode reward: 7.8500,                 loss: -0.0263
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 1844.4,                last time consumption/overall running time: 384.2388s / 244561.5198 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.1444
env0_second_0:                 episode reward: 8.6500,                 loss: -0.0483
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 2246.15,                last time consumption/overall running time: 464.2155s / 245025.7353 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.1237
env0_second_0:                 episode reward: 6.3000,                 loss: -0.0049
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 2311.1,                last time consumption/overall running time: 478.0893s / 245503.8246 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.1119
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0126
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 1621.6,                last time consumption/overall running time: 341.7079s / 245845.5325 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.1304
env0_second_0:                 episode reward: 9.0500,                 loss: -0.0483
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 1722.65,                last time consumption/overall running time: 359.9365s / 246205.4690 s
env0_first_0:                 episode reward: -8.9500,                 loss: -0.1340
env0_second_0:                 episode reward: 8.9500,                 loss: -0.0635
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 1534.65,                last time consumption/overall running time: 322.4513s / 246527.9202 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.1500
env0_second_0:                 episode reward: 9.7500,                 loss: -0.0788
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 2140.0,                last time consumption/overall running time: 445.5909s / 246973.5112 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.1273
env0_second_0:                 episode reward: 8.3500,                 loss: -0.0330
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 1497.3,                last time consumption/overall running time: 314.5858s / 247288.0969 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.1402
env0_second_0:                 episode reward: 9.5000,                 loss: 0.0011
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 1122.7,                last time consumption/overall running time: 239.4606s / 247527.5575 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1354
env0_second_0:                 episode reward: 9.7000,                 loss: -0.0664
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 927.1,                last time consumption/overall running time: 201.1998s / 247728.7573 s
env0_first_0:                 episode reward: -9.9500,                 loss: -0.1461
env0_second_0:                 episode reward: 9.9500,                 loss: -0.0911
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 901.75,                last time consumption/overall running time: 194.2849s / 247923.0423 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1629
env0_second_0:                 episode reward: 9.7000,                 loss: -0.0976
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 916.8,                last time consumption/overall running time: 198.7170s / 248121.7592 s
env0_first_0:                 episode reward: -9.8500,                 loss: -0.1717
env0_second_0:                 episode reward: 9.8500,                 loss: -0.1119
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 1254.4,                last time consumption/overall running time: 266.0907s / 248387.8500 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.1481
env0_second_0:                 episode reward: 9.6500,                 loss: -0.0684
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 2119.65,                last time consumption/overall running time: 436.7901s / 248824.6401 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1223
env0_second_0:                 episode reward: 7.1000,                 loss: -0.0252
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 2804.4,                last time consumption/overall running time: 577.7914s / 249402.4314 s
env0_first_0:                 episode reward: -5.2000,                 loss: -0.1382
env0_second_0:                 episode reward: 5.2000,                 loss: -0.0427
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 2690.55,                last time consumption/overall running time: 553.8122s / 249956.2436 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1481
env0_second_0:                 episode reward: 7.1000,                 loss: -0.0038
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 2577.35,                last time consumption/overall running time: 528.1303s / 250484.3739 s
env0_first_0:                 episode reward: -7.4500,                 loss: -0.1553
env0_second_0:                 episode reward: 7.4500,                 loss: -0.0754
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 2690.55,                last time consumption/overall running time: 552.9801s / 251037.3540 s
env0_first_0:                 episode reward: -6.7000,                 loss: -0.1644
env0_second_0:                 episode reward: 6.7000,                 loss: -0.0803
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 2843.2,                last time consumption/overall running time: 581.8798s / 251619.2338 s
env0_first_0:                 episode reward: -5.6000,                 loss: -0.1569
env0_second_0:                 episode reward: 5.6000,                 loss: -0.0787
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 3176.1,                last time consumption/overall running time: 651.2779s / 252270.5117 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.1504
env0_second_0:                 episode reward: 5.3500,                 loss: -0.0639
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 3686.45,                last time consumption/overall running time: 753.9119s / 253024.4235 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.1392
env0_second_0:                 episode reward: 3.8000,                 loss: -0.0672
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 3432.95,                last time consumption/overall running time: 700.5328s / 253724.9563 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.1528
env0_second_0:                 episode reward: 4.5000,                 loss: -0.0909
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 3381.1,                last time consumption/overall running time: 691.4798s / 254416.4361 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.1369
env0_second_0:                 episode reward: 3.8000,                 loss: -0.0798
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 3942.05,                last time consumption/overall running time: 800.3635s / 255216.7996 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.1331
env0_second_0:                 episode reward: 3.0500,                 loss: -0.0763
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 4030.9,                last time consumption/overall running time: 788.5870s / 256005.3866 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.1326
env0_second_0:                 episode reward: 2.9500,                 loss: -0.0806
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 4428.45,                last time consumption/overall running time: 841.0581s / 256846.4447 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1313
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0630Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
/home/zihan/research/MARS/mars/rl/agents/nash_ppo.py:181: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  s,a,r,s_prime,prob_a,done_mask =    torch.tensor(s_lst, dtype=torch.float).to(self.device), torch.tensor(a_lst).to(self.device), \
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 2871.55,                last time consumption/overall running time: 551.3890s / 257397.8337 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1562
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0770
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 2058.05,                last time consumption/overall running time: 397.6343s / 257795.4680 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1371
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0638
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 2497.55,                last time consumption/overall running time: 480.7606s / 258276.2286 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.1498
env0_second_0:                 episode reward: 4.3000,                 loss: -0.0708
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 2582.55,                last time consumption/overall running time: 493.4664s / 258769.6950 s
env0_first_0:                 episode reward: -4.9000,                 loss: -0.1273
env0_second_0:                 episode reward: 4.9000,                 loss: -0.0605
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 3186.35,                last time consumption/overall running time: 569.1748s / 259338.8698 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.1636
env0_second_0:                 episode reward: 3.4000,                 loss: -0.0914
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 2980.55,                last time consumption/overall running time: 534.9567s / 259873.8265 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.1660
env0_second_0:                 episode reward: 5.5000,                 loss: -0.0954
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 2818.6,                last time consumption/overall running time: 506.3310s / 260380.1574 s
env0_first_0:                 episode reward: -5.2000,                 loss: -0.1632
env0_second_0:                 episode reward: 5.2000,                 loss: -0.0935
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 2625.85,                last time consumption/overall running time: 469.9676s / 260850.1251 s
env0_first_0:                 episode reward: -6.4000,                 loss: -0.1648
env0_second_0:                 episode reward: 6.4000,                 loss: -0.0933
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 1913.55,                last time consumption/overall running time: 346.2672s / 261196.3923 s
env0_first_0:                 episode reward: -4.7500,                 loss: -0.1289
env0_second_0:                 episode reward: 4.7500,                 loss: -0.0714
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 2181.45,                last time consumption/overall running time: 392.2003s / 261588.5926 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.1414
env0_second_0:                 episode reward: 3.8500,                 loss: -0.0645
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
