pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
double_dunk_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'double_dunk_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220119_1543/pettingzoo_double_dunk_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_1543/pettingzoo_double_dunk_v2_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 4189.0,                last time consumption/overall running time: 73.7810s / 73.7810 s
env0_first_0:                 episode reward: -36.0000,                 loss: -0.0809
env0_second_0:                 episode reward: 36.0000,                 loss: -0.0907
env1_first_0:                 episode reward: -38.0000,                 loss: nan
env1_second_0:                 episode reward: 38.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 3815.05,                last time consumption/overall running time: 1440.8919s / 1514.6729 s
env0_first_0:                 episode reward: -22.4500,                 loss: -0.0530
env0_second_0:                 episode reward: 22.4500,                 loss: -0.0508
env1_first_0:                 episode reward: -25.8500,                 loss: nan
env1_second_0:                 episode reward: 25.8500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 3821.3,                last time consumption/overall running time: 1459.6578s / 2974.3307 s
env0_first_0:                 episode reward: -23.2500,                 loss: 0.0143
env0_second_0:                 episode reward: 23.2500,                 loss: 0.0119
env1_first_0:                 episode reward: -23.3000,                 loss: nan
env1_second_0:                 episode reward: 23.3000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 4208.55,                last time consumption/overall running time: 1605.0778s / 4579.4085 s
env0_first_0:                 episode reward: -30.0500,                 loss: 0.0247
env0_second_0:                 episode reward: 30.0500,                 loss: 0.0230
env1_first_0:                 episode reward: -27.9000,                 loss: nan
env1_second_0:                 episode reward: 27.9000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 3790.9,                last time consumption/overall running time: 1451.5053s / 6030.9139 s
env0_first_0:                 episode reward: -22.9000,                 loss: 0.0603
env0_second_0:                 episode reward: 22.9000,                 loss: 0.0562
env1_first_0:                 episode reward: -21.3500,                 loss: nan
env1_second_0:                 episode reward: 21.3500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 3989.25,                last time consumption/overall running time: 1519.2427s / 7550.1566 s
env0_first_0:                 episode reward: -24.1500,                 loss: 0.0517
env0_second_0:                 episode reward: 24.1500,                 loss: 0.0607
env1_first_0:                 episode reward: -28.3000,                 loss: nan
env1_second_0:                 episode reward: 28.3000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 3912.6,                last time consumption/overall running time: 1496.7725s / 9046.9291 s
env0_first_0:                 episode reward: -23.8500,                 loss: 0.0688
env0_second_0:                 episode reward: 23.8500,                 loss: 0.0768
env1_first_0:                 episode reward: -23.8000,                 loss: nan
env1_second_0:                 episode reward: 23.8000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 4067.2,                last time consumption/overall running time: 1556.5775s / 10603.5066 s
env0_first_0:                 episode reward: -24.8500,                 loss: 0.0622
env0_second_0:                 episode reward: 24.8500,                 loss: 0.0668
env1_first_0:                 episode reward: -24.2000,                 loss: nan
env1_second_0:                 episode reward: 24.2000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 3704.1,                last time consumption/overall running time: 1415.2508s / 12018.7574 s
env0_first_0:                 episode reward: -22.8500,                 loss: 0.0737
env0_second_0:                 episode reward: 22.8500,                 loss: 0.0777
env1_first_0:                 episode reward: -23.0000,                 loss: nan
env1_second_0:                 episode reward: 23.0000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 4069.4,                last time consumption/overall running time: 1555.7926s / 13574.5501 s
env0_first_0:                 episode reward: -21.7000,                 loss: 0.0552
env0_second_0:                 episode reward: 21.7000,                 loss: 0.0547
env1_first_0:                 episode reward: -24.7500,                 loss: nan
env1_second_0:                 episode reward: 24.7500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 3866.4,                last time consumption/overall running time: 1477.5377s / 15052.0877 s
env0_first_0:                 episode reward: -24.8500,                 loss: 0.0530
env0_second_0:                 episode reward: 24.8500,                 loss: 0.0556
env1_first_0:                 episode reward: -28.0500,                 loss: nan
env1_second_0:                 episode reward: 28.0500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 3981.1,                last time consumption/overall running time: 1519.4253s / 16571.5130 s
env0_first_0:                 episode reward: -23.9500,                 loss: 0.0461
env0_second_0:                 episode reward: 23.9500,                 loss: 0.0479
env1_first_0:                 episode reward: -25.9500,                 loss: nan
env1_second_0:                 episode reward: 25.9500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 3933.4,                last time consumption/overall running time: 1504.9235s / 18076.4365 s
env0_first_0:                 episode reward: -27.0000,                 loss: 0.0489
env0_second_0:                 episode reward: 27.0000,                 loss: 0.0550
env1_first_0:                 episode reward: -23.0500,                 loss: nan
env1_second_0:                 episode reward: 23.0500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 4249.5,                last time consumption/overall running time: 1624.1940s / 19700.6305 s
env0_first_0:                 episode reward: -29.0000,                 loss: 0.0469
env0_second_0:                 episode reward: 29.0000,                 loss: 0.0508
env1_first_0:                 episode reward: -28.2500,                 loss: nan
env1_second_0:                 episode reward: 28.2500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 4769.5,                last time consumption/overall running time: 1818.3650s / 21518.9955 s
env0_first_0:                 episode reward: -34.2000,                 loss: 0.0340
env0_second_0:                 episode reward: 34.2000,                 loss: 0.0397
env1_first_0:                 episode reward: -33.0000,                 loss: nan
env1_second_0:                 episode reward: 33.0000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 4298.65,                last time consumption/overall running time: 1640.2107s / 23159.2063 s
env0_first_0:                 episode reward: -27.3500,                 loss: 0.0359
env0_second_0:                 episode reward: 27.3500,                 loss: 0.0376
env1_first_0:                 episode reward: -28.6000,                 loss: nan
env1_second_0:                 episode reward: 28.6000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 4583.55,                last time consumption/overall running time: 1750.4830s / 24909.6893 s
env0_first_0:                 episode reward: -32.3000,                 loss: 0.0256
env0_second_0:                 episode reward: 32.3000,                 loss: 0.0238
env1_first_0:                 episode reward: -31.6000,                 loss: nan
env1_second_0:                 episode reward: 31.6000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 4647.4,                last time consumption/overall running time: 1771.3922s / 26681.0815 s
env0_first_0:                 episode reward: -33.9500,                 loss: 0.0041
env0_second_0:                 episode reward: 33.9500,                 loss: 0.0072
env1_first_0:                 episode reward: -36.6000,                 loss: nan
env1_second_0:                 episode reward: 36.6000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 4757.6,                last time consumption/overall running time: 1817.9292s / 28499.0107 s
env0_first_0:                 episode reward: -35.7500,                 loss: 0.0017
env0_second_0:                 episode reward: 35.7500,                 loss: 0.0017
env1_first_0:                 episode reward: -33.0500,                 loss: nan
env1_second_0:                 episode reward: 33.0500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 4771.25,                last time consumption/overall running time: 1822.1537s / 30321.1644 s
env0_first_0:                 episode reward: -36.1500,                 loss: -0.0005
env0_second_0:                 episode reward: 36.1500,                 loss: -0.0002
env1_first_0:                 episode reward: -36.2500,                 loss: nan
env1_second_0:                 episode reward: 36.2500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 5135.65,                last time consumption/overall running time: 1960.1036s / 32281.2679 s
env0_first_0:                 episode reward: -39.2500,                 loss: -0.0114
env0_second_0:                 episode reward: 39.2500,                 loss: -0.0126
env1_first_0:                 episode reward: -38.8500,                 loss: nan
env1_second_0:                 episode reward: 38.8500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 4929.5,                last time consumption/overall running time: 1880.4358s / 34161.7038 s
env0_first_0:                 episode reward: -37.8000,                 loss: -0.0281
env0_second_0:                 episode reward: 37.8000,                 loss: -0.0295
env1_first_0:                 episode reward: -35.7000,                 loss: nan
env1_second_0:                 episode reward: 35.7000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 4899.7,                last time consumption/overall running time: 1865.5981s / 36027.3018 s
env0_first_0:                 episode reward: -37.3000,                 loss: -0.0315
env0_second_0:                 episode reward: 37.3000,                 loss: -0.0377
env1_first_0:                 episode reward: -37.2000,                 loss: nan
env1_second_0:                 episode reward: 37.2000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 4806.65,                last time consumption/overall running time: 1828.6169s / 37855.9188 s
env0_first_0:                 episode reward: -38.2000,                 loss: -0.0399
env0_second_0:                 episode reward: 38.2000,                 loss: -0.0411
env1_first_0:                 episode reward: -35.4500,                 loss: nan
env1_second_0:                 episode reward: 35.4500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 4482.85,                last time consumption/overall running time: 1714.6675s / 39570.5862 s
env0_first_0:                 episode reward: -33.5000,                 loss: -0.0674
env0_second_0:                 episode reward: 33.5000,                 loss: -0.0677
env1_first_0:                 episode reward: -31.4000,                 loss: nan
env1_second_0:                 episode reward: 31.4000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 4270.25,                last time consumption/overall running time: 1625.7132s / 41196.2995 s
env0_first_0:                 episode reward: -27.0500,                 loss: -0.1063
env0_second_0:                 episode reward: 27.0500,                 loss: -0.1048
env1_first_0:                 episode reward: -25.2500,                 loss: nan
env1_second_0:                 episode reward: 25.2500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 4319.65,                last time consumption/overall running time: 1641.9453s / 42838.2447 s
env0_first_0:                 episode reward: -22.6500,                 loss: -0.1209
env0_second_0:                 episode reward: 22.6500,                 loss: -0.1131
env1_first_0:                 episode reward: -24.5000,                 loss: nan
env1_second_0:                 episode reward: 24.5000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 4140.0,                last time consumption/overall running time: 1578.1032s / 44416.3479 s
env0_first_0:                 episode reward: -15.1500,                 loss: -0.1917
env0_second_0:                 episode reward: 15.1500,                 loss: -0.1868
env1_first_0:                 episode reward: -16.2500,                 loss: nan
env1_second_0:                 episode reward: 16.2500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 3851.75,                last time consumption/overall running time: 1467.5946s / 45883.9426 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.2378
env0_second_0:                 episode reward: 7.3500,                 loss: -0.2349
env1_first_0:                 episode reward: -10.1500,                 loss: nan
env1_second_0:                 episode reward: 10.1500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 4210.25,                last time consumption/overall running time: 1603.1675s / 47487.1101 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.2634
env0_second_0:                 episode reward: 6.5000,                 loss: -0.2591
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 4150.95,                last time consumption/overall running time: 1582.2060s / 49069.3161 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.2699
env0_second_0:                 episode reward: 6.2500,                 loss: -0.2663
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 4205.55,                last time consumption/overall running time: 1608.2911s / 50677.6071 s
env0_first_0:                 episode reward: -3.1500,                 loss: -0.2811
env0_second_0:                 episode reward: 3.1500,                 loss: -0.2796
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 4056.4,                last time consumption/overall running time: 1555.8892s / 52233.4964 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.2960
env0_second_0:                 episode reward: 3.3500,                 loss: -0.2934
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 4042.85,                last time consumption/overall running time: 1551.5885s / 53785.0849 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3018
env0_second_0:                 episode reward: 1.3000,                 loss: -0.3004
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 3523.25,                last time consumption/overall running time: 1351.4585s / 55136.5434 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3123
env0_second_0:                 episode reward: 1.3500,                 loss: -0.3219
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 3710.9,                last time consumption/overall running time: 1430.5635s / 56567.1069 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3272
env0_second_0:                 episode reward: 0.4000,                 loss: -0.3221
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 4058.9,                last time consumption/overall running time: 1560.3850s / 58127.4919 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3342
env0_second_0:                 episode reward: -1.1000,                 loss: -0.3327
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 3990.8,                last time consumption/overall running time: 1451.8946s / 59579.3866 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3374
env0_second_0:                 episode reward: -0.2500,                 loss: -0.3317
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 3750.45,                last time consumption/overall running time: 1253.8800s / 60833.2666 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3431
env0_second_0:                 episode reward: -0.6500,                 loss: -0.3454
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 3501.65,                last time consumption/overall running time: 1093.0299s / 61926.2965 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3495
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3484
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 3713.85,                last time consumption/overall running time: 1162.3897s / 63088.6862 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3675
env0_second_0:                 episode reward: -0.9500,                 loss: -0.3624
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 3620.4,                last time consumption/overall running time: 1136.0358s / 64224.7220 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3513
env0_second_0:                 episode reward: -0.4500,                 loss: -0.3435
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 3638.9,                last time consumption/overall running time: 1143.0516s / 65367.7736 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3537
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3523
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 3750.95,                last time consumption/overall running time: 1175.3062s / 66543.0798 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3531
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3525
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 3238.0,                last time consumption/overall running time: 1015.2996s / 67558.3795 s
env0_first_0:                 episode reward: 9.3500,                 loss: -0.2913
env0_second_0:                 episode reward: -9.3500,                 loss: -0.2899
env1_first_0:                 episode reward: 10.2000,                 loss: nan
env1_second_0:                 episode reward: -10.2000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2159.45,                last time consumption/overall running time: 686.4944s / 68244.8739 s
env0_first_0:                 episode reward: 16.5000,                 loss: -0.2378
env0_second_0:                 episode reward: -16.5000,                 loss: -0.2402
env1_first_0:                 episode reward: 15.5500,                 loss: nan
env1_second_0:                 episode reward: -15.5500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1983.15,                last time consumption/overall running time: 634.1570s / 68879.0309 s
env0_first_0:                 episode reward: 15.8000,                 loss: -0.2430
env0_second_0:                 episode reward: -15.8000,                 loss: -0.2405
env1_first_0:                 episode reward: 16.9000,                 loss: nan
env1_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2018.8,                last time consumption/overall running time: 641.6151s / 69520.6460 s
env0_first_0:                 episode reward: 15.1000,                 loss: -0.2466
env0_second_0:                 episode reward: -15.1000,                 loss: -0.2505
env1_first_0:                 episode reward: 15.8500,                 loss: nan
env1_second_0:                 episode reward: -15.8500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2105.2,                last time consumption/overall running time: 669.8012s / 70190.4472 s
env0_first_0:                 episode reward: 17.3500,                 loss: -0.2494
env0_second_0:                 episode reward: -17.3500,                 loss: -0.2437
env1_first_0:                 episode reward: 14.6000,                 loss: nan
env1_second_0:                 episode reward: -14.6000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1962.25,                last time consumption/overall running time: 628.7106s / 70819.1578 s
env0_first_0:                 episode reward: 15.5000,                 loss: -0.2508
env0_second_0:                 episode reward: -15.5000,                 loss: -0.2456
env1_first_0:                 episode reward: 17.0500,                 loss: nan
env1_second_0:                 episode reward: -17.0500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1973.15,                last time consumption/overall running time: 629.9476s / 71449.1054 s
env0_first_0:                 episode reward: 16.6500,                 loss: -0.2428
env0_second_0:                 episode reward: -16.6500,                 loss: -0.2406
env1_first_0:                 episode reward: 15.8000,                 loss: nan
env1_second_0:                 episode reward: -15.8000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1861.9,                last time consumption/overall running time: 592.8821s / 72041.9875 s
env0_first_0:                 episode reward: 16.8500,                 loss: -0.2450
env0_second_0:                 episode reward: -16.8500,                 loss: -0.2430
env1_first_0:                 episode reward: 14.9500,                 loss: nan
env1_second_0:                 episode reward: -14.9500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1859.3,                last time consumption/overall running time: 598.0680s / 72640.0555 s
env0_first_0:                 episode reward: 15.2000,                 loss: -0.2500
env0_second_0:                 episode reward: -15.2000,                 loss: -0.2524
env1_first_0:                 episode reward: 16.1500,                 loss: nan
env1_second_0:                 episode reward: -16.1500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1953.5,                last time consumption/overall running time: 625.4859s / 73265.5413 s
env0_first_0:                 episode reward: 14.6500,                 loss: -0.2507
env0_second_0:                 episode reward: -14.6500,                 loss: -0.2479
env1_first_0:                 episode reward: 16.3500,                 loss: nan
env1_second_0:                 episode reward: -16.3500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1909.8,                last time consumption/overall running time: 610.6417s / 73876.1830 s
env0_first_0:                 episode reward: 16.3000,                 loss: -0.2599
env0_second_0:                 episode reward: -16.3000,                 loss: -0.2523
env1_first_0:                 episode reward: 15.1500,                 loss: nan
env1_second_0:                 episode reward: -15.1500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1925.1,                last time consumption/overall running time: 616.1527s / 74492.3357 s
env0_first_0:                 episode reward: 14.4500,                 loss: -0.2564
env0_second_0:                 episode reward: -14.4500,                 loss: -0.2531
env1_first_0:                 episode reward: 15.6000,                 loss: nan
env1_second_0:                 episode reward: -15.6000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1980.25,                last time consumption/overall running time: 630.6885s / 75123.0242 s
env0_first_0:                 episode reward: 16.2000,                 loss: -0.2608
env0_second_0:                 episode reward: -16.2000,                 loss: -0.2444
env1_first_0:                 episode reward: 15.9000,                 loss: nan
env1_second_0:                 episode reward: -15.9000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1965.0,                last time consumption/overall running time: 631.5321s / 75754.5563 s
env0_first_0:                 episode reward: 16.3500,                 loss: -0.2508
env0_second_0:                 episode reward: -16.3500,                 loss: -0.2487
env1_first_0:                 episode reward: 14.9500,                 loss: nan
env1_second_0:                 episode reward: -14.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1932.05,                last time consumption/overall running time: 617.1747s / 76371.7310 s
env0_first_0:                 episode reward: 16.0500,                 loss: -0.2558
env0_second_0:                 episode reward: -16.0500,                 loss: -0.2544
env1_first_0:                 episode reward: 13.7500,                 loss: nan
env1_second_0:                 episode reward: -13.7500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1945.6,                last time consumption/overall running time: 618.2301s / 76989.9610 s
env0_first_0:                 episode reward: 15.8000,                 loss: -0.2582
env0_second_0:                 episode reward: -15.8000,                 loss: -0.2559
env1_first_0:                 episode reward: 13.7500,                 loss: nan
env1_second_0:                 episode reward: -13.7500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2004.8,                last time consumption/overall running time: 641.8974s / 77631.8584 s
env0_first_0:                 episode reward: 15.0500,                 loss: -0.2504
env0_second_0:                 episode reward: -15.0500,                 loss: -0.2506
env1_first_0:                 episode reward: 13.9500,                 loss: nan
env1_second_0:                 episode reward: -13.9500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1888.7,                last time consumption/overall running time: 610.0840s / 78241.9424 s
env0_first_0:                 episode reward: 14.6500,                 loss: -0.2656
env0_second_0:                 episode reward: -14.6500,                 loss: -0.2639
env1_first_0:                 episode reward: 15.1000,                 loss: nan
env1_second_0:                 episode reward: -15.1000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1971.7,                last time consumption/overall running time: 633.0362s / 78874.9786 s
env0_first_0:                 episode reward: 14.4500,                 loss: -0.2468
env0_second_0:                 episode reward: -14.4500,                 loss: -0.2448
env1_first_0:                 episode reward: 13.3000,                 loss: nan
env1_second_0:                 episode reward: -13.3000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1894.45,                last time consumption/overall running time: 611.9670s / 79486.9456 s
env0_first_0:                 episode reward: 12.6500,                 loss: -0.2514
env0_second_0:                 episode reward: -12.6500,                 loss: -0.2515
env1_first_0:                 episode reward: 13.1500,                 loss: nan
env1_second_0:                 episode reward: -13.1500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1943.45,                last time consumption/overall running time: 625.9300s / 80112.8757 s
env0_first_0:                 episode reward: 13.4500,                 loss: -0.2640
env0_second_0:                 episode reward: -13.4500,                 loss: -0.2536
env1_first_0:                 episode reward: 14.1500,                 loss: nan
env1_second_0:                 episode reward: -14.1500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1942.45,                last time consumption/overall running time: 626.0835s / 80738.9591 s
env0_first_0:                 episode reward: 12.8500,                 loss: -0.2522
env0_second_0:                 episode reward: -12.8500,                 loss: -0.2423
env1_first_0:                 episode reward: 11.0000,                 loss: nan
env1_second_0:                 episode reward: -11.0000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1878.2,                last time consumption/overall running time: 601.1281s / 81340.0872 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.2524
env0_second_0:                 episode reward: -9.9000,                 loss: -0.2485
env1_first_0:                 episode reward: 10.2500,                 loss: nan
env1_second_0:                 episode reward: -10.2500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1935.4,                last time consumption/overall running time: 614.7652s / 81954.8524 s
env0_first_0:                 episode reward: 6.2000,                 loss: -0.2570
env0_second_0:                 episode reward: -6.2000,                 loss: -0.2554
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1724.45,                last time consumption/overall running time: 552.3064s / 82507.1588 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.2734
env0_second_0:                 episode reward: -7.0000,                 loss: -0.2674
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1788.05,                last time consumption/overall running time: 571.1873s / 83078.3461 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.2669
env0_second_0:                 episode reward: -5.6000,                 loss: -0.2545
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1851.95,                last time consumption/overall running time: 593.3115s / 83671.6576 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.2637
env0_second_0:                 episode reward: -3.8500,                 loss: -0.2540
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1738.15,                last time consumption/overall running time: 558.5393s / 84230.1969 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.3018
env0_second_0:                 episode reward: -3.5000,                 loss: -0.2902
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2034.1,                last time consumption/overall running time: 649.3224s / 84879.5194 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.2693
env0_second_0:                 episode reward: -5.4500,                 loss: -0.2645
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1658.15,                last time consumption/overall running time: 533.6584s / 85413.1778 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2914
env0_second_0:                 episode reward: -2.7000,                 loss: -0.2836
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 2041.65,                last time consumption/overall running time: 649.3581s / 86062.5359 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.2866
env0_second_0:                 episode reward: -3.6500,                 loss: -0.2787
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1785.85,                last time consumption/overall running time: 570.1408s / 86632.6767 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3016
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3013
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1904.55,                last time consumption/overall running time: 608.5291s / 87241.2059 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2997
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2989
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1889.35,                last time consumption/overall running time: 602.3941s / 87843.6000 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.2796
env0_second_0:                 episode reward: -3.0500,                 loss: -0.2778
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1933.45,                last time consumption/overall running time: 617.6756s / 88461.2756 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.2971
env0_second_0:                 episode reward: -3.0000,                 loss: -0.2924
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1735.1,                last time consumption/overall running time: 556.9482s / 89018.2238 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3041
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2976
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1897.2,                last time consumption/overall running time: 609.1242s / 89627.3479 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.3135
env0_second_0:                 episode reward: -2.2000,                 loss: -0.3101
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1763.95,                last time consumption/overall running time: 564.2433s / 90191.5912 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3162
env0_second_0:                 episode reward: -1.9000,                 loss: -0.3092
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1650.55,                last time consumption/overall running time: 527.5114s / 90719.1027 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.3197
env0_second_0:                 episode reward: -0.8000,                 loss: -0.3089
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1801.55,                last time consumption/overall running time: 577.8379s / 91296.9406 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.3218
env0_second_0:                 episode reward: -0.8500,                 loss: -0.3177
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1641.5,                last time consumption/overall running time: 524.9153s / 91821.8558 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3126
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3015
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1926.45,                last time consumption/overall running time: 614.9371s / 92436.7929 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3127
env0_second_0:                 episode reward: -1.4500,                 loss: -0.3054
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1636.15,                last time consumption/overall running time: 524.8763s / 92961.6692 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3261
env0_second_0:                 episode reward: -0.5500,                 loss: -0.3223
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1688.6,                last time consumption/overall running time: 542.3582s / 93504.0274 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.3448
env0_second_0:                 episode reward: -2.2000,                 loss: -0.3380
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1588.9,                last time consumption/overall running time: 510.4596s / 94014.4870 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3410
env0_second_0:                 episode reward: -1.1000,                 loss: -0.3318
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1591.5,                last time consumption/overall running time: 511.6436s / 94526.1307 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3203
env0_second_0:                 episode reward: 0.0000,                 loss: -0.3044
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1685.5,                last time consumption/overall running time: 543.6294s / 95069.7600 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3122
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3015
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1737.05,                last time consumption/overall running time: 561.6800s / 95631.4401 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3112
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3019
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1628.85,                last time consumption/overall running time: 528.9500s / 96160.3901 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3237
env0_second_0:                 episode reward: -2.1500,                 loss: -0.3185
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1741.75,                last time consumption/overall running time: 558.7244s / 96719.1145 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.3170
env0_second_0:                 episode reward: -2.0500,                 loss: -0.3032
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1587.5,                last time consumption/overall running time: 508.9774s / 97228.0918 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3185
env0_second_0:                 episode reward: -1.4000,                 loss: -0.3105
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1557.55,                last time consumption/overall running time: 495.9998s / 97724.0916 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3080
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2877
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1584.65,                last time consumption/overall running time: 509.8593s / 98233.9509 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3183
env0_second_0:                 episode reward: -0.3000,                 loss: -0.3026
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1511.35,                last time consumption/overall running time: 492.7796s / 98726.7305 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3351
env0_second_0:                 episode reward: -0.2500,                 loss: -0.3224
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1570.0,                last time consumption/overall running time: 507.3518s / 99234.0823 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3313
env0_second_0:                 episode reward: 0.6000,                 loss: -0.3242
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1534.3,                last time consumption/overall running time: 499.7110s / 99733.7933 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3320
env0_second_0:                 episode reward: -0.1000,                 loss: -0.3210
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1592.7,                last time consumption/overall running time: 516.3836s / 100250.1770 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3336
env0_second_0:                 episode reward: -0.7500,                 loss: -0.3262
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1532.05,                last time consumption/overall running time: 493.5871s / 100743.7640 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3569
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3370
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1540.7,                last time consumption/overall running time: 494.8432s / 101238.6073 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3304
env0_second_0:                 episode reward: -0.2500,                 loss: -0.3217
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1519.0,                last time consumption/overall running time: 495.9702s / 101734.5775 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3437
env0_second_0:                 episode reward: -0.9500,                 loss: -0.3294
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1688.2,                last time consumption/overall running time: 544.7544s / 102279.3319 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3336
env0_second_0:                 episode reward: -0.9500,                 loss: -0.3201
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1763.05,                last time consumption/overall running time: 565.7466s / 102845.0785 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.3099
env0_second_0:                 episode reward: -0.8000,                 loss: -0.3022
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1622.1,                last time consumption/overall running time: 527.6516s / 103372.7301 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3310
env0_second_0:                 episode reward: -1.1500,                 loss: -0.3199
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1515.75,                last time consumption/overall running time: 485.6201s / 103858.3502 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3234
env0_second_0:                 episode reward: -0.9500,                 loss: -0.3113
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1648.5,                last time consumption/overall running time: 526.8789s / 104385.2291 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3168
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3036
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1914.05,                last time consumption/overall running time: 612.3593s / 104997.5884 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3128
env0_second_0:                 episode reward: -2.7500,                 loss: -0.3017
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1942.5,                last time consumption/overall running time: 627.4862s / 105625.0746 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.3071
env0_second_0:                 episode reward: -3.6500,                 loss: -0.2950
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1933.75,                last time consumption/overall running time: 618.6292s / 106243.7038 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.2920
env0_second_0:                 episode reward: -4.1500,                 loss: -0.2814
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 2089.3,                last time consumption/overall running time: 666.7401s / 106910.4439 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.2755
env0_second_0:                 episode reward: -4.9500,                 loss: -0.2582
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 2107.85,                last time consumption/overall running time: 673.2820s / 107583.7259 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.2646
env0_second_0:                 episode reward: -4.8000,                 loss: -0.2559
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 2133.05,                last time consumption/overall running time: 681.8019s / 108265.5277 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.2739
env0_second_0:                 episode reward: -4.9000,                 loss: -0.2636
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1970.15,                last time consumption/overall running time: 630.3750s / 108895.9027 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.2718
env0_second_0:                 episode reward: -3.0000,                 loss: -0.2623
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 2189.95,                last time consumption/overall running time: 702.6691s / 109598.5718 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.2501
env0_second_0:                 episode reward: -4.2000,                 loss: -0.2420
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 2251.35,                last time consumption/overall running time: 715.8492s / 110314.4210 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.2734
env0_second_0:                 episode reward: -4.6500,                 loss: -0.2643
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1975.65,                last time consumption/overall running time: 629.2826s / 110943.7036 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.2540
env0_second_0:                 episode reward: -7.3000,                 loss: -0.2356
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1963.65,                last time consumption/overall running time: 628.5005s / 111572.2041 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.2740
env0_second_0:                 episode reward: -4.9500,                 loss: -0.2599
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 2052.8,                last time consumption/overall running time: 659.6723s / 112231.8764 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2749
env0_second_0:                 episode reward: -4.3000,                 loss: -0.2629
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 2155.85,                last time consumption/overall running time: 692.1994s / 112924.0758 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.2522
env0_second_0:                 episode reward: -5.1000,                 loss: -0.2415
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 2033.7,                last time consumption/overall running time: 650.0032s / 113574.0789 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.2539
env0_second_0:                 episode reward: -3.9000,                 loss: -0.2400
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1778.8,                last time consumption/overall running time: 568.8953s / 114142.9742 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3068
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2938
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 2026.9,                last time consumption/overall running time: 648.3811s / 114791.3553 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.2844
env0_second_0:                 episode reward: -6.0000,                 loss: -0.2665
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 2164.65,                last time consumption/overall running time: 691.0603s / 115482.4157 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.2698
env0_second_0:                 episode reward: -6.0500,                 loss: -0.2634
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 2217.15,                last time consumption/overall running time: 713.8898s / 116196.3054 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.2581
env0_second_0:                 episode reward: -6.7000,                 loss: -0.2541
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 2092.05,                last time consumption/overall running time: 665.3440s / 116861.6494 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2577
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2461
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1859.85,                last time consumption/overall running time: 594.1017s / 117455.7511 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2991
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2728
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1529.8,                last time consumption/overall running time: 492.6274s / 117948.3785 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3501
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3348
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1509.35,                last time consumption/overall running time: 487.3353s / 118435.7138 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3444
env0_second_0:                 episode reward: -0.7500,                 loss: -0.3347
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1674.65,                last time consumption/overall running time: 540.3938s / 118976.1076 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3266
env0_second_0:                 episode reward: -2.5500,                 loss: -0.3181
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1559.0,                last time consumption/overall running time: 504.3090s / 119480.4166 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3574
env0_second_0:                 episode reward: -0.9500,                 loss: -0.3387
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1493.3,                last time consumption/overall running time: 484.8627s / 119965.2793 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3490
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3345
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1526.15,                last time consumption/overall running time: 495.3245s / 120460.6038 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3556
env0_second_0:                 episode reward: -1.3000,                 loss: -0.3453
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1556.45,                last time consumption/overall running time: 504.4401s / 120965.0439 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3609
env0_second_0:                 episode reward: -2.3000,                 loss: -0.3572
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1545.95,                last time consumption/overall running time: 497.4848s / 121462.5287 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3413
env0_second_0:                 episode reward: -1.2500,                 loss: -0.3308
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1492.8,                last time consumption/overall running time: 482.7254s / 121945.2541 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3642
env0_second_0:                 episode reward: -1.6000,                 loss: -0.3537
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1777.8,                last time consumption/overall running time: 570.0375s / 122515.2916 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3090
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2929
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 2047.75,                last time consumption/overall running time: 658.9674s / 123174.2590 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.2727
env0_second_0:                 episode reward: -4.3500,                 loss: -0.2613
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 2322.2,                last time consumption/overall running time: 738.2805s / 123912.5395 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.2426
env0_second_0:                 episode reward: -5.8000,                 loss: -0.2313
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 2196.85,                last time consumption/overall running time: 698.7429s / 124611.2824 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.2652
env0_second_0:                 episode reward: -5.3000,                 loss: -0.2531
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 2019.2,                last time consumption/overall running time: 640.5275s / 125251.8099 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.2823
env0_second_0:                 episode reward: -4.1500,                 loss: -0.2774
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 2044.8,                last time consumption/overall running time: 646.7833s / 125898.5933 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2751
env0_second_0:                 episode reward: -2.6500,                 loss: -0.2664
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1847.45,                last time consumption/overall running time: 593.0042s / 126491.5974 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2868
env0_second_0:                 episode reward: -2.0000,                 loss: -0.2764
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1791.3,                last time consumption/overall running time: 560.0011s / 127051.5985 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.2850
env0_second_0:                 episode reward: -0.7000,                 loss: -0.2673
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1686.15,                last time consumption/overall running time: 505.4912s / 127557.0897 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3102
env0_second_0:                 episode reward: -1.4000,                 loss: -0.3069
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1625.15,                last time consumption/overall running time: 491.0714s / 128048.1611 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.3207
env0_second_0:                 episode reward: -1.9500,                 loss: -0.3143
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1629.8,                last time consumption/overall running time: 490.9791s / 128539.1402 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3429
env0_second_0:                 episode reward: -0.5000,                 loss: -0.3393
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1532.95,                last time consumption/overall running time: 462.5150s / 129001.6552 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3412
env0_second_0:                 episode reward: -0.4500,                 loss: -0.3283
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1662.1,                last time consumption/overall running time: 501.1477s / 129502.8028 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3383
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3065
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1836.55,                last time consumption/overall running time: 549.9707s / 130052.7736 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3027
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2914
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1878.95,                last time consumption/overall running time: 560.3171s / 130613.0907 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2863
env0_second_0:                 episode reward: -1.3000,                 loss: -0.2725
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1878.1,                last time consumption/overall running time: 563.3009s / 131176.3916 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.2906
env0_second_0:                 episode reward: -0.9500,                 loss: -0.2899
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1724.85,                last time consumption/overall running time: 519.8589s / 131696.2504 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3220
env0_second_0:                 episode reward: -2.1500,                 loss: -0.3100
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 2083.75,                last time consumption/overall running time: 620.7596s / 132317.0101 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.2756
env0_second_0:                 episode reward: 0.9000,                 loss: -0.2622
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 2070.45,                last time consumption/overall running time: 617.3741s / 132934.3842 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.2679
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2509
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1682.05,                last time consumption/overall running time: 507.5678s / 133441.9520 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3322
env0_second_0:                 episode reward: -0.6500,                 loss: -0.3237
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1580.4,                last time consumption/overall running time: 477.3036s / 133919.2556 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3418
env0_second_0:                 episode reward: -1.9000,                 loss: -0.3239
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1628.4,                last time consumption/overall running time: 490.6611s / 134409.9167 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3385
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3294
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1579.9,                last time consumption/overall running time: 475.8227s / 134885.7394 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3470
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3394
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1506.0,                last time consumption/overall running time: 453.0433s / 135338.7827 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3556
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3470
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1538.45,                last time consumption/overall running time: 462.8082s / 135801.5909 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3546
env0_second_0:                 episode reward: -1.9000,                 loss: -0.3377
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1499.65,                last time consumption/overall running time: 450.9508s / 136252.5417 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3605
env0_second_0:                 episode reward: -0.9000,                 loss: -0.3431
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1495.7,                last time consumption/overall running time: 449.8790s / 136702.4207 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3691
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3632
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1723.1,                last time consumption/overall running time: 517.6105s / 137220.0312 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.3243
env0_second_0:                 episode reward: -2.6500,                 loss: -0.3151
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1994.85,                last time consumption/overall running time: 597.2015s / 137817.2328 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2986
env0_second_0:                 episode reward: -4.3000,                 loss: -0.2854
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 2199.75,                last time consumption/overall running time: 651.6439s / 138468.8766 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2742
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2663
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 2084.75,                last time consumption/overall running time: 624.0590s / 139092.9357 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2869
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2680
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 2041.5,                last time consumption/overall running time: 610.0171s / 139702.9528 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2837
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2697
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1639.9,                last time consumption/overall running time: 491.5004s / 140194.4532 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2874
env0_second_0:                 episode reward: -0.5500,                 loss: -0.2720
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1621.65,                last time consumption/overall running time: 474.4167s / 140668.8699 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3406
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3294
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1589.2,                last time consumption/overall running time: 449.6614s / 141118.5313 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3385
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3185
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1540.75,                last time consumption/overall running time: 432.9538s / 141551.4851 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3594
env0_second_0:                 episode reward: -1.4000,                 loss: -0.3374
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1648.35,                last time consumption/overall running time: 463.9263s / 142015.4114 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.3218
env0_second_0:                 episode reward: -0.3500,                 loss: -0.3165
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1634.2,                last time consumption/overall running time: 462.6903s / 142478.1017 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3260
env0_second_0:                 episode reward: -1.6000,                 loss: -0.3155
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1727.0,                last time consumption/overall running time: 485.5085s / 142963.6102 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.3123
env0_second_0:                 episode reward: -3.0500,                 loss: -0.2958
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1904.85,                last time consumption/overall running time: 533.6678s / 143497.2781 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2919
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2840
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1857.75,                last time consumption/overall running time: 520.2037s / 144017.4817 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.2710
env0_second_0:                 episode reward: -3.5000,                 loss: -0.2590
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 2042.2,                last time consumption/overall running time: 570.4441s / 144587.9259 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.2664
env0_second_0:                 episode reward: -4.1500,                 loss: -0.2528
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1913.5,                last time consumption/overall running time: 539.9738s / 145127.8997 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2687
env0_second_0:                 episode reward: -1.8500,                 loss: -0.2600
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1745.8,                last time consumption/overall running time: 494.1916s / 145622.0912 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2904
env0_second_0:                 episode reward: -4.3000,                 loss: -0.2959
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1642.5,                last time consumption/overall running time: 466.3300s / 146088.4212 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2881
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2741
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1575.95,                last time consumption/overall running time: 449.0365s / 146537.4577 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3141
env0_second_0:                 episode reward: -2.5500,                 loss: -0.3030
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1768.95,                last time consumption/overall running time: 501.0568s / 147038.5145 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.2965
env0_second_0:                 episode reward: -3.4500,                 loss: -0.2843
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1615.4,                last time consumption/overall running time: 455.8015s / 147494.3160 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2950
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2830
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1799.25,                last time consumption/overall running time: 506.9302s / 148001.2462 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.2845
env0_second_0:                 episode reward: -3.6500,                 loss: -0.2639
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1831.8,                last time consumption/overall running time: 517.3059s / 148518.5521 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.2765
env0_second_0:                 episode reward: -5.8500,                 loss: -0.2689
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1808.8,                last time consumption/overall running time: 506.8407s / 149025.3928 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.2532
env0_second_0:                 episode reward: -3.9500,                 loss: -0.2428
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1706.2,                last time consumption/overall running time: 479.0407s / 149504.4334 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.2651
env0_second_0:                 episode reward: -5.0000,                 loss: -0.2514
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1667.5,                last time consumption/overall running time: 469.2261s / 149973.6596 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.3131
env0_second_0:                 episode reward: -3.8500,                 loss: -0.3041
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1647.95,                last time consumption/overall running time: 466.4628s / 150440.1224 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.3141
env0_second_0:                 episode reward: -2.5000,                 loss: -0.2978
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1625.25,                last time consumption/overall running time: 459.0344s / 150899.1568 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.2981
env0_second_0:                 episode reward: -3.5000,                 loss: -0.2818
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1547.55,                last time consumption/overall running time: 436.4321s / 151335.5889 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3653
env0_second_0:                 episode reward: -1.6000,                 loss: -0.3489
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1549.05,                last time consumption/overall running time: 439.6094s / 151775.1983 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3543
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3436
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1520.85,                last time consumption/overall running time: 431.8968s / 152207.0951 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3519
env0_second_0:                 episode reward: -1.4000,                 loss: -0.3436
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1624.45,                last time consumption/overall running time: 457.1752s / 152664.2704 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3191
env0_second_0:                 episode reward: -1.0500,                 loss: -0.3021
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1613.85,                last time consumption/overall running time: 455.7356s / 153120.0059 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.3261
env0_second_0:                 episode reward: -2.5000,                 loss: -0.3159
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1729.8,                last time consumption/overall running time: 489.7524s / 153609.7583 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3214
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3099
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1816.4,                last time consumption/overall running time: 514.7370s / 154124.4952 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2999
env0_second_0:                 episode reward: -0.2500,                 loss: -0.3015
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1623.45,                last time consumption/overall running time: 457.2093s / 154581.7045 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3345
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2974
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1536.65,                last time consumption/overall running time: 433.1133s / 155014.8178 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3428
env0_second_0:                 episode reward: -0.7500,                 loss: -0.3295
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1771.4,                last time consumption/overall running time: 499.6287s / 155514.4464 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.2685
env0_second_0:                 episode reward: -4.2500,                 loss: -0.2586
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1599.9,                last time consumption/overall running time: 454.4923s / 155968.9387 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2997
env0_second_0:                 episode reward: -2.7000,                 loss: -0.2936
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1626.0,                last time consumption/overall running time: 460.1803s / 156429.1191 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.2608
env0_second_0:                 episode reward: -2.8000,                 loss: -0.2558
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1685.2,                last time consumption/overall running time: 479.0303s / 156908.1493 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.2935
env0_second_0:                 episode reward: -3.3000,                 loss: -0.2780
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1629.85,                last time consumption/overall running time: 462.6583s / 157370.8077 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.3038
env0_second_0:                 episode reward: -3.2500,                 loss: -0.2937
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1613.9,                last time consumption/overall running time: 457.1206s / 157827.9282 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.3355
env0_second_0:                 episode reward: -0.8500,                 loss: -0.3201
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1522.8,                last time consumption/overall running time: 432.6589s / 158260.5871 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3420
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3356
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1488.1,                last time consumption/overall running time: 422.0778s / 158682.6649 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.3560
env0_second_0:                 episode reward: -1.9500,                 loss: -0.3453
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1513.65,                last time consumption/overall running time: 431.3806s / 159114.0455 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3511
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3448
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1669.6,                last time consumption/overall running time: 472.8502s / 159586.8957 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2930
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2787
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1503.85,                last time consumption/overall running time: 428.7294s / 160015.6252 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3645
env0_second_0:                 episode reward: -1.6000,                 loss: -0.3542
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1458.2,                last time consumption/overall running time: 416.2140s / 160431.8392 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3521
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3286
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1509.35,                last time consumption/overall running time: 429.0089s / 160860.8481 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3447
env0_second_0:                 episode reward: -2.2500,                 loss: -0.3215
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1544.15,                last time consumption/overall running time: 438.6002s / 161299.4482 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3359
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3170
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1531.55,                last time consumption/overall running time: 433.0311s / 161732.4793 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3113
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2887
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1578.3,                last time consumption/overall running time: 445.4616s / 162177.9409 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3329
env0_second_0:                 episode reward: -1.2500,                 loss: -0.3202
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1589.5,                last time consumption/overall running time: 445.9637s / 162623.9046 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3369
env0_second_0:                 episode reward: -2.3000,                 loss: -0.3293
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1550.25,                last time consumption/overall running time: 436.7672s / 163060.6717 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3449
env0_second_0:                 episode reward: -1.0500,                 loss: -0.3416
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1530.85,                last time consumption/overall running time: 430.6739s / 163491.3456 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3462
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3320
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1553.15,                last time consumption/overall running time: 435.4906s / 163926.8362 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3214
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3060
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1539.0,                last time consumption/overall running time: 432.8358s / 164359.6720 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.3428
env0_second_0:                 episode reward: -2.3500,                 loss: -0.3306
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1558.85,                last time consumption/overall running time: 440.2948s / 164799.9667 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3444
env0_second_0:                 episode reward: -2.5500,                 loss: -0.3192
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1630.4,                last time consumption/overall running time: 462.1229s / 165262.0896 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.3334
env0_second_0:                 episode reward: -2.1000,                 loss: -0.3240
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1494.9,                last time consumption/overall running time: 421.8828s / 165683.9725 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3584
env0_second_0:                 episode reward: -2.1500,                 loss: -0.3531
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1678.35,                last time consumption/overall running time: 474.1716s / 166158.1441 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.3027
env0_second_0:                 episode reward: -2.9500,                 loss: -0.2821
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1650.35,                last time consumption/overall running time: 461.9771s / 166620.1212 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2976
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2750
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1775.5,                last time consumption/overall running time: 498.4965s / 167118.6177 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2909
env0_second_0:                 episode reward: -1.8500,                 loss: -0.2758
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1561.7,                last time consumption/overall running time: 439.3525s / 167557.9702 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3309
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3251
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1580.8,                last time consumption/overall running time: 444.9898s / 168002.9600 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3327
env0_second_0:                 episode reward: -1.7500,                 loss: -0.3119
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1602.0,                last time consumption/overall running time: 449.7306s / 168452.6906 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3122
env0_second_0:                 episode reward: -0.4500,                 loss: -0.3042
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1818.1,                last time consumption/overall running time: 505.9641s / 168958.6546 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2731
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2700
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1776.55,                last time consumption/overall running time: 497.2740s / 169455.9287 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2941
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2780
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1586.1,                last time consumption/overall running time: 447.3960s / 169903.3246 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3150
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3050
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1525.65,                last time consumption/overall running time: 429.6241s / 170332.9488 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.3299
env0_second_0:                 episode reward: -2.6000,                 loss: -0.3071
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1525.3,                last time consumption/overall running time: 428.2595s / 170761.2083 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3105
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2980
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1546.55,                last time consumption/overall running time: 434.7594s / 171195.9677 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3259
env0_second_0:                 episode reward: -2.3000,                 loss: -0.3050
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1554.75,                last time consumption/overall running time: 438.8060s / 171634.7737 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3301
env0_second_0:                 episode reward: -1.4500,                 loss: -0.3187
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1542.45,                last time consumption/overall running time: 436.3157s / 172071.0895 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3357
env0_second_0:                 episode reward: -1.1000,                 loss: -0.3246
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1631.15,                last time consumption/overall running time: 457.2926s / 172528.3820 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.3292
env0_second_0:                 episode reward: -2.9000,                 loss: -0.3065
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1522.65,                last time consumption/overall running time: 429.4197s / 172957.8017 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.3560
env0_second_0:                 episode reward: -0.3500,                 loss: -0.3399
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1639.5,                last time consumption/overall running time: 460.5445s / 173418.3463 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3121
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2938
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1549.35,                last time consumption/overall running time: 437.6161s / 173855.9623 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3191
env0_second_0:                 episode reward: -0.4500,                 loss: -0.3015
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1533.7,                last time consumption/overall running time: 431.9081s / 174287.8704 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3303
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3100
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1527.35,                last time consumption/overall running time: 432.4329s / 174720.3033 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3527
env0_second_0:                 episode reward: -0.9000,                 loss: -0.3421
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 1551.6,                last time consumption/overall running time: 438.2088s / 175158.5121 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3289
env0_second_0:                 episode reward: -2.4000,                 loss: -0.3132
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1637.95,                last time consumption/overall running time: 459.3869s / 175617.8990 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3153
env0_second_0:                 episode reward: -2.8500,                 loss: -0.3007
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1565.95,                last time consumption/overall running time: 441.4944s / 176059.3934 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3098
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2990
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1525.15,                last time consumption/overall running time: 428.8964s / 176488.2898 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.3472
env0_second_0:                 episode reward: -2.6000,                 loss: -0.3315
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1619.55,                last time consumption/overall running time: 453.8875s / 176942.1774 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.3114
env0_second_0:                 episode reward: -2.8000,                 loss: -0.2993
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1561.1,                last time consumption/overall running time: 436.1202s / 177378.2976 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.3261
env0_second_0:                 episode reward: -3.4000,                 loss: -0.3101
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1629.25,                last time consumption/overall running time: 459.3873s / 177837.6849 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.3103
env0_second_0:                 episode reward: -3.1000,                 loss: -0.2875
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1599.85,                last time consumption/overall running time: 447.5283s / 178285.2132 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2896
env0_second_0:                 episode reward: -2.3500,                 loss: -0.2863
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1544.0,                last time consumption/overall running time: 433.0661s / 178718.2793 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3246
env0_second_0:                 episode reward: -2.0000,                 loss: -0.3151
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1541.15,                last time consumption/overall running time: 434.3822s / 179152.6615 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3101
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2794
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1470.05,                last time consumption/overall running time: 409.8665s / 179562.5280 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3079
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2875
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1562.55,                last time consumption/overall running time: 411.1271s / 179973.6551 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2817
env0_second_0:                 episode reward: -1.8500,                 loss: -0.2619
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1503.65,                last time consumption/overall running time: 396.3563s / 180370.0114 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2985
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2844
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1536.35,                last time consumption/overall running time: 403.9015s / 180773.9129 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2955
env0_second_0:                 episode reward: -2.1500,                 loss: -0.2812
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1667.1,                last time consumption/overall running time: 439.3900s / 181213.3029 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.3007
env0_second_0:                 episode reward: -3.1000,                 loss: -0.2826
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1569.55,                last time consumption/overall running time: 414.9563s / 181628.2592 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2894
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2584
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1539.65,                last time consumption/overall running time: 407.3266s / 182035.5858 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3323
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3202
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1520.9,                last time consumption/overall running time: 400.3944s / 182435.9801 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3264
env0_second_0:                 episode reward: -2.4000,                 loss: -0.3132
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1533.7,                last time consumption/overall running time: 404.5396s / 182840.5197 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3395
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3269
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1590.1,                last time consumption/overall running time: 416.0458s / 183256.5656 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3306
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3125
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1576.75,                last time consumption/overall running time: 412.8042s / 183669.3698 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.3020
env0_second_0:                 episode reward: -3.9500,                 loss: -0.2890
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1601.15,                last time consumption/overall running time: 420.8211s / 184090.1908 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2980
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2836
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1567.8,                last time consumption/overall running time: 413.7326s / 184503.9234 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3034
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2934
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1482.15,                last time consumption/overall running time: 392.6962s / 184896.6196 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.3346
env0_second_0:                 episode reward: -2.6000,                 loss: -0.3144
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1484.4,                last time consumption/overall running time: 392.1032s / 185288.7227 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.2913
env0_second_0:                 episode reward: -3.9000,                 loss: -0.2856
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1539.85,                last time consumption/overall running time: 407.5058s / 185696.2285 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3132
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2996
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1554.75,                last time consumption/overall running time: 408.9769s / 186105.2054 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.3251
env0_second_0:                 episode reward: -2.6500,                 loss: -0.3030
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 1610.95,                last time consumption/overall running time: 426.4536s / 186531.6591 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2807
env0_second_0:                 episode reward: -2.1500,                 loss: -0.2645
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1487.5,                last time consumption/overall running time: 391.0709s / 186922.7300 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3447
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3294
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1516.6,                last time consumption/overall running time: 398.7077s / 187321.4377 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3230
env0_second_0:                 episode reward: -0.9000,                 loss: -0.3032
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 1494.5,                last time consumption/overall running time: 396.6974s / 187718.1351 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.3509
env0_second_0:                 episode reward: -0.3500,                 loss: -0.3372
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1516.45,                last time consumption/overall running time: 399.0239s / 188117.1590 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3395
env0_second_0:                 episode reward: -0.4500,                 loss: -0.3170
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1664.4,                last time consumption/overall running time: 437.9782s / 188555.1372 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2993
env0_second_0:                 episode reward: -1.2500,                 loss: -0.2828
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1568.5,                last time consumption/overall running time: 413.9947s / 188969.1319 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3199
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3043
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1465.4,                last time consumption/overall running time: 385.7558s / 189354.8877 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3478
env0_second_0:                 episode reward: -1.6500,                 loss: -0.3293
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1486.8,                last time consumption/overall running time: 390.6985s / 189745.5862 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3237
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3132
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1464.15,                last time consumption/overall running time: 386.8369s / 190132.4231 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3368
env0_second_0:                 episode reward: -1.2500,                 loss: -0.3230
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1515.4,                last time consumption/overall running time: 399.1632s / 190531.5863 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3319
env0_second_0:                 episode reward: -1.4500,                 loss: -0.3172
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1615.75,                last time consumption/overall running time: 425.2352s / 190956.8215 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.3224
env0_second_0:                 episode reward: -2.1000,                 loss: -0.3049
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1521.0,                last time consumption/overall running time: 399.7074s / 191356.5289 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3340
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3225
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 1506.3,                last time consumption/overall running time: 400.1234s / 191756.6523 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3139
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2951
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 1491.15,                last time consumption/overall running time: 395.3976s / 192152.0498 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3388
env0_second_0:                 episode reward: -1.3000,                 loss: -0.3209
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 1624.4,                last time consumption/overall running time: 429.3412s / 192581.3910 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.3056
env0_second_0:                 episode reward: -3.9500,                 loss: -0.2851
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1531.7,                last time consumption/overall running time: 403.0920s / 192984.4830 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3123
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2959
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 1652.7,                last time consumption/overall running time: 434.1020s / 193418.5850 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.2831
env0_second_0:                 episode reward: -3.4000,                 loss: -0.2698
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1857.85,                last time consumption/overall running time: 485.3730s / 193903.9579 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.2610
env0_second_0:                 episode reward: -3.9500,                 loss: -0.2424
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1651.85,                last time consumption/overall running time: 434.7065s / 194338.6645 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2703
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2480
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 1654.95,                last time consumption/overall running time: 431.0590s / 194769.7235 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2958
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2776
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1519.35,                last time consumption/overall running time: 399.3740s / 195169.0975 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3197
env0_second_0:                 episode reward: -1.6000,                 loss: -0.3085
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1563.95,                last time consumption/overall running time: 413.8245s / 195582.9220 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.2995
env0_second_0:                 episode reward: -0.9500,                 loss: -0.2791
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 1634.2,                last time consumption/overall running time: 427.6105s / 196010.5325 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.2716
env0_second_0:                 episode reward: -1.2000,                 loss: -0.2457
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 1560.0,                last time consumption/overall running time: 410.4747s / 196421.0072 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3056
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2890
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 1582.05,                last time consumption/overall running time: 414.6947s / 196835.7018 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.2787
env0_second_0:                 episode reward: -2.2000,                 loss: -0.2584
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 1595.3,                last time consumption/overall running time: 422.1155s / 197257.8173 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2871
env0_second_0:                 episode reward: -2.3500,                 loss: -0.2727
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 1729.3,                last time consumption/overall running time: 451.9591s / 197709.7764 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.2359
env0_second_0:                 episode reward: -4.8500,                 loss: -0.2190
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 1636.7,                last time consumption/overall running time: 428.5596s / 198138.3360 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.2352
env0_second_0:                 episode reward: -6.5500,                 loss: -0.2225
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 1810.05,                last time consumption/overall running time: 473.5287s / 198611.8647 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1781
env0_second_0:                 episode reward: -2.2000,                 loss: -0.1617
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 1770.65,                last time consumption/overall running time: 465.0975s / 199076.9622 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.2286
env0_second_0:                 episode reward: -3.2000,                 loss: -0.2123
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1534.7,                last time consumption/overall running time: 404.3137s / 199481.2759 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3167
env0_second_0:                 episode reward: -2.3000,                 loss: -0.2950
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 1515.1,                last time consumption/overall running time: 400.2551s / 199881.5310 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3126
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2888
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1572.45,                last time consumption/overall running time: 414.3383s / 200295.8692 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.2943
env0_second_0:                 episode reward: -2.2000,                 loss: -0.2734
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1505.9,                last time consumption/overall running time: 396.6824s / 200692.5516 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3130
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2966
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1519.1,                last time consumption/overall running time: 400.7276s / 201093.2792 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3145
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2945
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 1633.95,                last time consumption/overall running time: 427.9605s / 201521.2398 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.2743
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2551
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 1566.2,                last time consumption/overall running time: 412.4701s / 201933.7099 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3125
env0_second_0:                 episode reward: -2.1500,                 loss: -0.2810
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1551.4,                last time consumption/overall running time: 404.4432s / 202338.1531 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3014
env0_second_0:                 episode reward: -1.2500,                 loss: -0.2838
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 1574.45,                last time consumption/overall running time: 412.5254s / 202750.6785 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3146
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2971
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 1638.2,                last time consumption/overall running time: 432.5550s / 203183.2335 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2956
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2716
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 1563.65,                last time consumption/overall running time: 411.1140s / 203594.3476 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3107
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2880
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1566.6,                last time consumption/overall running time: 409.6550s / 204004.0025 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3006
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2885
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1575.3,                last time consumption/overall running time: 415.2860s / 204419.2885 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2965
env0_second_0:                 episode reward: -1.3000,                 loss: -0.2787
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 1514.5,                last time consumption/overall running time: 398.7802s / 204818.0687 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.3153
env0_second_0:                 episode reward: -2.5000,                 loss: -0.2964
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1527.85,                last time consumption/overall running time: 402.9256s / 205220.9943 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2818
env0_second_0:                 episode reward: -2.1500,                 loss: -0.2601
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 1581.75,                last time consumption/overall running time: 415.7034s / 205636.6978 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3073
env0_second_0:                 episode reward: -2.3000,                 loss: -0.2887
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 1579.65,                last time consumption/overall running time: 414.9361s / 206051.6339 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2926
env0_second_0:                 episode reward: -1.9500,                 loss: -0.2726
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 1533.5,                last time consumption/overall running time: 400.8762s / 206452.5101 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3259
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3103
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 1587.85,                last time consumption/overall running time: 417.5744s / 206870.0845 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3020
env0_second_0:                 episode reward: -1.8500,                 loss: -0.2758
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 1626.5,                last time consumption/overall running time: 426.3423s / 207296.4269 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2833
env0_second_0:                 episode reward: -1.8500,                 loss: -0.2694
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 1500.1,                last time consumption/overall running time: 394.0399s / 207690.4668 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3334
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3199
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 1565.45,                last time consumption/overall running time: 409.1726s / 208099.6394 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.3056
env0_second_0:                 episode reward: -2.3500,                 loss: -0.2864
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 1555.55,                last time consumption/overall running time: 409.6243s / 208509.2636 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3255
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2973
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 1584.4,                last time consumption/overall running time: 413.9563s / 208923.2200 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2870
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2839
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 1527.95,                last time consumption/overall running time: 398.9386s / 209322.1586 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3122
env0_second_0:                 episode reward: -1.8500,                 loss: -0.2946
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 1669.0,                last time consumption/overall running time: 437.5404s / 209759.6990 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3027
env0_second_0:                 episode reward: -1.2500,                 loss: -0.2806
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1573.35,                last time consumption/overall running time: 412.9435s / 210172.6425 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3315
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3158
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 1718.85,                last time consumption/overall running time: 446.0804s / 210618.7228 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2976
env0_second_0:                 episode reward: -2.7000,                 loss: -0.2710
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 1582.35,                last time consumption/overall running time: 416.3946s / 211035.1175 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3210
env0_second_0:                 episode reward: -0.5000,                 loss: -0.3021
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 1483.1,                last time consumption/overall running time: 386.3514s / 211421.4689 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3444
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3206
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 1524.05,                last time consumption/overall running time: 398.2893s / 211819.7582 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.3345
env0_second_0:                 episode reward: -2.0500,                 loss: -0.3209
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 1502.45,                last time consumption/overall running time: 394.2172s / 212213.9754 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3283
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3117
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 1625.2,                last time consumption/overall running time: 422.7182s / 212636.6936 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2952
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2637
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 1484.95,                last time consumption/overall running time: 388.2897s / 213024.9833 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.3327
env0_second_0:                 episode reward: -2.2000,                 loss: -0.3199
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 1656.8,                last time consumption/overall running time: 433.3729s / 213458.3562 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3193
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3034
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 2061.3,                last time consumption/overall running time: 534.2802s / 213992.6364 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.2696
env0_second_0:                 episode reward: -5.4500,                 loss: -0.2433
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1822.25,                last time consumption/overall running time: 472.2805s / 214464.9169 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.2694
env0_second_0:                 episode reward: -4.9000,                 loss: -0.2418
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 1608.3,                last time consumption/overall running time: 418.8012s / 214883.7181 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3036
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2693
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 1485.3,                last time consumption/overall running time: 388.9334s / 215272.6515 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3150
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2793
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 1529.7,                last time consumption/overall running time: 399.9344s / 215672.5858 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3229
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3026
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 1543.2,                last time consumption/overall running time: 405.3686s / 216077.9544 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.3179
env0_second_0:                 episode reward: -2.3500,                 loss: -0.2868
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 1475.4,                last time consumption/overall running time: 386.4916s / 216464.4460 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.3439
env0_second_0:                 episode reward: -1.9500,                 loss: -0.3291
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 1534.0,                last time consumption/overall running time: 400.6409s / 216865.0869 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3342
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3130
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 1520.75,                last time consumption/overall running time: 396.9129s / 217261.9997 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3350
env0_second_0:                 episode reward: -2.5500,                 loss: -0.3206
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 1554.25,                last time consumption/overall running time: 409.1382s / 217671.1380 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3152
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2973
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 1584.75,                last time consumption/overall running time: 412.6660s / 218083.8040 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.3130
env0_second_0:                 episode reward: -2.1000,                 loss: -0.2853
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 1511.55,                last time consumption/overall running time: 397.6035s / 218481.4074 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3358
env0_second_0:                 episode reward: -1.4000,                 loss: -0.3176
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 1530.65,                last time consumption/overall running time: 402.0031s / 218883.4105 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3228
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2810
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 1670.25,                last time consumption/overall running time: 433.2487s / 219316.6592 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.2964
env0_second_0:                 episode reward: -2.9000,                 loss: -0.2713
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 1703.55,                last time consumption/overall running time: 442.9460s / 219759.6052 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2973
env0_second_0:                 episode reward: -2.1000,                 loss: -0.2703
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 1624.85,                last time consumption/overall running time: 423.3650s / 220182.9702 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3120
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2979
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 1563.45,                last time consumption/overall running time: 407.5481s / 220590.5183 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3266
env0_second_0:                 episode reward: -0.3000,                 loss: -0.3038
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 1529.25,                last time consumption/overall running time: 400.7887s / 220991.3070 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3523
env0_second_0:                 episode reward: -1.4500,                 loss: -0.3313
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 1546.2,                last time consumption/overall running time: 404.8590s / 221396.1660 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3494
env0_second_0:                 episode reward: -1.1000,                 loss: -0.3263
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1508.7,                last time consumption/overall running time: 395.7215s / 221791.8874 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3393
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3114
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 1771.3,                last time consumption/overall running time: 462.3837s / 222254.2711 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.2887
env0_second_0:                 episode reward: -3.6000,                 loss: -0.2469
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 1592.5,                last time consumption/overall running time: 416.2995s / 222670.5706 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3055
env0_second_0:                 episode reward: -1.7500,                 loss: -0.2674
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1513.65,                last time consumption/overall running time: 397.4048s / 223067.9755 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3332
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3061
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 1550.9,                last time consumption/overall running time: 407.9612s / 223475.9366 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3274
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3058
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 1507.0,                last time consumption/overall running time: 395.9074s / 223871.8441 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3410
env0_second_0:                 episode reward: -1.6500,                 loss: -0.3161
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 1508.25,                last time consumption/overall running time: 396.7171s / 224268.5612 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.3414
env0_second_0:                 episode reward: -2.0500,                 loss: -0.3210
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 1476.65,                last time consumption/overall running time: 387.7442s / 224656.3053 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.3504
env0_second_0:                 episode reward: -2.9000,                 loss: -0.3248
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 1494.9,                last time consumption/overall running time: 392.8978s / 225049.2031 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3635
env0_second_0:                 episode reward: -1.6500,                 loss: -0.3383
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 1452.95,                last time consumption/overall running time: 382.7779s / 225431.9809 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.3351
env0_second_0:                 episode reward: -3.5000,                 loss: -0.3194
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 1497.8,                last time consumption/overall running time: 395.2073s / 225827.1882 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3298
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3061
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1500.65,                last time consumption/overall running time: 394.2604s / 226221.4487 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3104
env0_second_0:                 episode reward: -2.3000,                 loss: -0.2820
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1568.3,                last time consumption/overall running time: 407.7902s / 226629.2388 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2710
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2463
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1485.7,                last time consumption/overall running time: 388.7154s / 227017.9542 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3049
env0_second_0:                 episode reward: -2.5500,                 loss: -0.2812
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 1475.8,                last time consumption/overall running time: 388.7831s / 227406.7374 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3030
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2893
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 1499.4,                last time consumption/overall running time: 394.7120s / 227801.4494 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.3098
env0_second_0:                 episode reward: -3.3000,                 loss: -0.2799
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 1652.15,                last time consumption/overall running time: 429.4540s / 228230.9034 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2876
env0_second_0:                 episode reward: -3.3500,                 loss: -0.2434
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 1655.8,                last time consumption/overall running time: 435.0077s / 228665.9111 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3000
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2714
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 1567.75,                last time consumption/overall running time: 409.8913s / 229075.8024 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2743
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2482
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 1489.75,                last time consumption/overall running time: 389.2363s / 229465.0387 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3208
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2884
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 1567.55,                last time consumption/overall running time: 407.8527s / 229872.8914 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.2977
env0_second_0:                 episode reward: -1.2000,                 loss: -0.2651
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 1527.15,                last time consumption/overall running time: 399.2164s / 230272.1077 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3063
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2804
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 1677.85,                last time consumption/overall running time: 439.6549s / 230711.7627 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2874
env0_second_0:                 episode reward: -1.9500,                 loss: -0.2487
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 1631.05,                last time consumption/overall running time: 424.2486s / 231136.0112 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2866
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2594
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 1661.8,                last time consumption/overall running time: 432.7424s / 231568.7536 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.2680
env0_second_0:                 episode reward: -4.5000,                 loss: -0.2394
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1640.1,                last time consumption/overall running time: 428.5736s / 231997.3272 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2526
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2185
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 1557.25,                last time consumption/overall running time: 408.1719s / 232405.4991 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.2966
env0_second_0:                 episode reward: -1.4000,                 loss: -0.2473
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 1487.65,                last time consumption/overall running time: 389.1709s / 232794.6700 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3160
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2762
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 1471.35,                last time consumption/overall running time: 385.4269s / 233180.0969 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.3017
env0_second_0:                 episode reward: -2.7000,                 loss: -0.2532
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 1457.15,                last time consumption/overall running time: 380.9484s / 233561.0453 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2751
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2445
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 1564.3,                last time consumption/overall running time: 411.0124s / 233972.0577 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2600
env0_second_0:                 episode reward: -2.3000,                 loss: -0.2292
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 1518.55,                last time consumption/overall running time: 393.5402s / 234365.5979 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.2654
env0_second_0:                 episode reward: -5.2000,                 loss: -0.2397
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 1542.55,                last time consumption/overall running time: 402.0095s / 234767.6074 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2642
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2302
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 1600.25,                last time consumption/overall running time: 415.3852s / 235182.9926 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2705
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2444
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 1485.45,                last time consumption/overall running time: 388.8271s / 235571.8198 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2893
env0_second_0:                 episode reward: -3.3500,                 loss: -0.2539
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 1660.35,                last time consumption/overall running time: 432.6810s / 236004.5007 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2811
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2465
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 1870.7,                last time consumption/overall running time: 482.1223s / 236486.6231 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.2742
env0_second_0:                 episode reward: -4.2500,                 loss: -0.2423
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 1777.55,                last time consumption/overall running time: 461.6561s / 236948.2791 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2811
env0_second_0:                 episode reward: -2.6000,                 loss: -0.2510
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 1529.8,                last time consumption/overall running time: 397.9785s / 237346.2576 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3132
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2847
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 1452.35,                last time consumption/overall running time: 379.2182s / 237725.4758 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.3092
env0_second_0:                 episode reward: -3.8500,                 loss: -0.2838
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 1505.0,                last time consumption/overall running time: 393.7965s / 238119.2722 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.3074
env0_second_0:                 episode reward: -3.2500,                 loss: -0.2792
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 1605.3,                last time consumption/overall running time: 421.3322s / 238540.6044 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.2890
env0_second_0:                 episode reward: -3.8500,                 loss: -0.2646
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 1947.7,                last time consumption/overall running time: 502.0848s / 239042.6892 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.2445
env0_second_0:                 episode reward: -4.3500,                 loss: -0.2182
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 1763.0,                last time consumption/overall running time: 456.9291s / 239499.6183 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2662
env0_second_0:                 episode reward: -3.3500,                 loss: -0.2316
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 1688.4,                last time consumption/overall running time: 439.5420s / 239939.1604 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.2842
env0_second_0:                 episode reward: -4.7500,                 loss: -0.2560
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 1757.85,                last time consumption/overall running time: 458.2062s / 240397.3665 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.2874
env0_second_0:                 episode reward: -3.1500,                 loss: -0.2603
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 1617.1,                last time consumption/overall running time: 422.2696s / 240819.6361 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.2987
env0_second_0:                 episode reward: -4.0500,                 loss: -0.2642
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 1556.2,                last time consumption/overall running time: 407.5046s / 241227.1407 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3305
env0_second_0:                 episode reward: -2.5500,                 loss: -0.2993
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 1573.85,                last time consumption/overall running time: 409.7038s / 241636.8445 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2911
env0_second_0:                 episode reward: -1.1500,                 loss: -0.2695
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 1645.15,                last time consumption/overall running time: 428.7134s / 242065.5578 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2965
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2726
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 1654.05,                last time consumption/overall running time: 430.5734s / 242496.1313 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.3197
env0_second_0:                 episode reward: -2.5000,                 loss: -0.2933
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 1689.0,                last time consumption/overall running time: 441.1155s / 242937.2468 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3134
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2854
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 1546.95,                last time consumption/overall running time: 402.7717s / 243340.0185 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3377
env0_second_0:                 episode reward: -1.7500,                 loss: -0.3104
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 1689.15,                last time consumption/overall running time: 437.7754s / 243777.7939 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3304
env0_second_0:                 episode reward: -1.3000,                 loss: -0.2989
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 1603.9,                last time consumption/overall running time: 418.4503s / 244196.2441 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3351
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2958
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 1747.6,                last time consumption/overall running time: 452.5228s / 244648.7669 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.3024
env0_second_0:                 episode reward: -0.4000,                 loss: -0.2647
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 1786.1,                last time consumption/overall running time: 461.2224s / 245109.9893 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.3062
env0_second_0:                 episode reward: -2.1000,                 loss: -0.2796
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 1899.2,                last time consumption/overall running time: 490.7692s / 245600.7585 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2909
env0_second_0:                 episode reward: -1.7500,                 loss: -0.2555
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 1976.15,                last time consumption/overall running time: 508.7837s / 246109.5422 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2614
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2339
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 1826.35,                last time consumption/overall running time: 473.4968s / 246583.0390 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2934
env0_second_0:                 episode reward: -2.1500,                 loss: -0.2689
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 1763.7,                last time consumption/overall running time: 457.3123s / 247040.3513 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2941
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2635
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 1545.5,                last time consumption/overall running time: 402.6085s / 247442.9598 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3419
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3143
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 1594.1,                last time consumption/overall running time: 416.1854s / 247859.1452 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3325
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3127
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 1783.15,                last time consumption/overall running time: 460.2395s / 248319.3848 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.2724
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2428
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 1597.25,                last time consumption/overall running time: 418.3349s / 248737.7196 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.3179
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2868
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 1680.15,                last time consumption/overall running time: 434.5084s / 249172.2280 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2940
env0_second_0:                 episode reward: -0.5500,                 loss: -0.2628
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 1594.05,                last time consumption/overall running time: 414.0067s / 249586.2347 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3179
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2938
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 1516.85,                last time consumption/overall running time: 398.7041s / 249984.9388 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.3562
env0_second_0:                 episode reward: -2.0500,                 loss: -0.3182
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 1655.15,                last time consumption/overall running time: 430.4172s / 250415.3561 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2877
env0_second_0:                 episode reward: 0.5000,                 loss: -0.2435
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 1799.25,                last time consumption/overall running time: 467.7248s / 250883.0809 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.2850
env0_second_0:                 episode reward: -0.8000,                 loss: -0.2433
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 2060.6,                last time consumption/overall running time: 531.7145s / 251414.7953 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2401
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2029
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 1964.05,                last time consumption/overall running time: 508.1572s / 251922.9526 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2281
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1838
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 1976.1,                last time consumption/overall running time: 510.3638s / 252433.3164 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2464
env0_second_0:                 episode reward: -1.3000,                 loss: -0.2116
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 1668.7,                last time consumption/overall running time: 433.8143s / 252867.1307 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3099
env0_second_0:                 episode reward: -2.5500,                 loss: -0.2747
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 1622.8,                last time consumption/overall running time: 409.7803s / 253276.9110 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3100
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2794
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 1533.05,                last time consumption/overall running time: 373.9538s / 253650.8647 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3265
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2945
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 1570.05,                last time consumption/overall running time: 380.7628s / 254031.6275 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3188
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2831
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 1642.9,                last time consumption/overall running time: 395.6876s / 254427.3152 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3077
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2760
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 1802.95,                last time consumption/overall running time: 434.6882s / 254862.0034 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2642
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2381
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 1659.0,                last time consumption/overall running time: 402.4992s / 255264.5026 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3128
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2862
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 1678.35,                last time consumption/overall running time: 405.3376s / 255669.8402 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.2946
env0_second_0:                 episode reward: -3.8000,                 loss: -0.2632
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 1636.75,                last time consumption/overall running time: 396.6104s / 256066.4506 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3174
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2858
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 1700.45,                last time consumption/overall running time: 409.2665s / 256475.7171 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3179
env0_second_0:                 episode reward: -2.0000,                 loss: -0.2862
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1622.25,                last time consumption/overall running time: 391.3380s / 256867.0551 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3175
env0_second_0:                 episode reward: -0.9500,                 loss: -0.3003
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 1516.75,                last time consumption/overall running time: 367.2916s / 257234.3467 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3372
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3073
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 1571.2,                last time consumption/overall running time: 379.0965s / 257613.4433 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3178
env0_second_0:                 episode reward: -1.4000,                 loss: -0.2838
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 1672.6,                last time consumption/overall running time: 401.3495s / 258014.7927 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3003
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2654
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 1555.2,                last time consumption/overall running time: 376.6177s / 258391.4105 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3187
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2951
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 1530.9,                last time consumption/overall running time: 368.6464s / 258760.0568 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3376
env0_second_0:                 episode reward: -0.7500,                 loss: -0.3066
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 1518.05,                last time consumption/overall running time: 366.1907s / 259126.2476 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3412
env0_second_0:                 episode reward: -1.0500,                 loss: -0.3188
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 1596.25,                last time consumption/overall running time: 387.6946s / 259513.9422 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.2886
env0_second_0:                 episode reward: -2.5500,                 loss: -0.2524
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 1535.35,                last time consumption/overall running time: 372.9026s / 259886.8448 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3404
env0_second_0:                 episode reward: -1.7500,                 loss: -0.3118
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 1612.7,                last time consumption/overall running time: 389.0790s / 260275.9237 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2961
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2789
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 1587.25,                last time consumption/overall running time: 381.9674s / 260657.8912 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.3194
env0_second_0:                 episode reward: -0.8000,                 loss: -0.2957
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 1494.15,                last time consumption/overall running time: 361.1386s / 261019.0298 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3625
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3274
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 1490.45,                last time consumption/overall running time: 359.4709s / 261378.5007 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3544
env0_second_0:                 episode reward: -1.3000,                 loss: -0.3287
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 1490.9,                last time consumption/overall running time: 360.4888s / 261738.9896 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.3491
env0_second_0:                 episode reward: -0.8500,                 loss: -0.3098
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 1567.0,                last time consumption/overall running time: 379.1183s / 262118.1079 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3383
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3078
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 1937.9,                last time consumption/overall running time: 462.4288s / 262580.5367 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2929
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2543
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 1752.25,                last time consumption/overall running time: 423.6339s / 263004.1706 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.3003
env0_second_0:                 episode reward: -2.5000,                 loss: -0.2630
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 1658.5,                last time consumption/overall running time: 400.4314s / 263404.6020 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3031
env0_second_0:                 episode reward: -1.7500,                 loss: -0.2876
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 1532.0,                last time consumption/overall running time: 374.4295s / 263779.0315 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3440
env0_second_0:                 episode reward: -1.3000,                 loss: -0.3180
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 1519.55,                last time consumption/overall running time: 365.7436s / 264144.7750 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.3338
env0_second_0:                 episode reward: -2.0500,                 loss: -0.2974
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 1561.15,                last time consumption/overall running time: 376.5756s / 264521.3506 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3264
env0_second_0:                 episode reward: -0.5500,                 loss: -0.3006
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 1555.1,                last time consumption/overall running time: 373.6522s / 264895.0028 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.3181
env0_second_0:                 episode reward: -0.8500,                 loss: -0.2939
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 1483.25,                last time consumption/overall running time: 360.9413s / 265255.9441 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3374
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3133
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 1492.35,                last time consumption/overall running time: 362.6893s / 265618.6335 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3489
env0_second_0:                 episode reward: -1.0500,                 loss: -0.3134
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 1531.7,                last time consumption/overall running time: 368.4220s / 265987.0555 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3424
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3136
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 1541.3,                last time consumption/overall running time: 373.6603s / 266360.7158 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3421
env0_second_0:                 episode reward: -1.4000,                 loss: -0.3187
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 1580.95,                last time consumption/overall running time: 369.8128s / 266730.5286 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3385
env0_second_0:                 episode reward: -2.1500,                 loss: -0.3174
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 1538.3,                last time consumption/overall running time: 345.5034s / 267076.0320 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.3558
env0_second_0:                 episode reward: -2.0500,                 loss: -0.3383
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 1565.4,                last time consumption/overall running time: 350.4327s / 267426.4647 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3479
env0_second_0:                 episode reward: -1.3000,                 loss: -0.3193
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 1548.45,                last time consumption/overall running time: 347.6964s / 267774.1611 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3470
env0_second_0:                 episode reward: -1.6000,                 loss: -0.3106
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 1537.45,                last time consumption/overall running time: 346.7919s / 268120.9530 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3382
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3052
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 1522.25,                last time consumption/overall running time: 344.2638s / 268465.2167 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3509
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3249
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 1525.75,                last time consumption/overall running time: 344.6789s / 268809.8956 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3417
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3234
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 1545.4,                last time consumption/overall running time: 347.2814s / 269157.1770 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3498
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3247
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 1490.1,                last time consumption/overall running time: 335.9989s / 269493.1759 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3492
env0_second_0:                 episode reward: -2.2500,                 loss: -0.3178
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 1505.05,                last time consumption/overall running time: 337.8307s / 269831.0066 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3444
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3095
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 1517.8,                last time consumption/overall running time: 343.0957s / 270174.1024 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3267
env0_second_0:                 episode reward: -1.4000,                 loss: -0.3121
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 1523.45,                last time consumption/overall running time: 344.6571s / 270518.7595 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3382
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3109
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 1522.75,                last time consumption/overall running time: 340.2301s / 270858.9896 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3522
env0_second_0:                 episode reward: -0.5000,                 loss: -0.3356
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 1510.85,                last time consumption/overall running time: 316.9801s / 271175.9697 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3270
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2998
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 1503.95,                last time consumption/overall running time: 315.3728s / 271491.3425 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3411
env0_second_0:                 episode reward: -2.3000,                 loss: -0.3005
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 1562.5,                last time consumption/overall running time: 325.5497s / 271816.8922 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3226
env0_second_0:                 episode reward: -1.1500,                 loss: -0.2900
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 1538.3,                last time consumption/overall running time: 324.0244s / 272140.9166 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3291
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2924
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 1471.45,                last time consumption/overall running time: 309.1929s / 272450.1095 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3602
env0_second_0:                 episode reward: -1.6000,                 loss: -0.3312
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 1506.4,                last time consumption/overall running time: 313.6969s / 272763.8063 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3456
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3195
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 1643.15,                last time consumption/overall running time: 342.5839s / 273106.3902 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2667
env0_second_0:                 episode reward: -0.5500,                 loss: -0.2189
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 1697.45,                last time consumption/overall running time: 353.3525s / 273459.7426 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2817
env0_second_0:                 episode reward: -1.9500,                 loss: -0.2354
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 1689.9,                last time consumption/overall running time: 351.5645s / 273811.3072 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2853
env0_second_0:                 episode reward: -2.7000,                 loss: -0.2495
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 1567.4,                last time consumption/overall running time: 327.2528s / 274138.5600 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3214Load double_dunk_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load double_dunk_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rl/agents/nash_ppo.py:181: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  s,a,r,s_prime,prob_a,done_mask =    torch.tensor(s_lst, dtype=torch.float).to(self.device), torch.tensor(a_lst).to(self.device), \
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_second_0:                 episode reward: -1.5000,                 loss: -0.2879
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 1578.3,                last time consumption/overall running time: 329.5504s / 274468.1103 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3364
env0_second_0:                 episode reward: -2.2500,                 loss: -0.3053
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 1669.1,                last time consumption/overall running time: 349.0538s / 274817.1642 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3185
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2884
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 1589.8,                last time consumption/overall running time: 332.1212s / 275149.2853 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3217
env0_second_0:                 episode reward: -2.0000,                 loss: -0.2965
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 1532.1,                last time consumption/overall running time: 322.0881s / 275471.3735 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3322
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2921
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 1458.95,                last time consumption/overall running time: 306.2379s / 275777.6114 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3624
env0_second_0:                 episode reward: -0.4500,                 loss: -0.3392
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 1487.35,                last time consumption/overall running time: 311.6240s / 276089.2354 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.3525
env0_second_0:                 episode reward: -1.9500,                 loss: -0.3200
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 1509.95,                last time consumption/overall running time: 317.6922s / 276406.9276 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3508
env0_second_0:                 episode reward: -1.4000,                 loss: -0.3197
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 1574.45,                last time consumption/overall running time: 327.7429s / 276734.6705 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3417
env0_second_0:                 episode reward: -1.6000,                 loss: -0.3143
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 1481.65,                last time consumption/overall running time: 307.8269s / 277042.4973 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.3550
env0_second_0:                 episode reward: -1.9500,                 loss: -0.3261
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 1454.85,                last time consumption/overall running time: 304.2614s / 277346.7587 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3465
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3101
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
