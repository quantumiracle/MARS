pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=1024, bias=True)
      (1): Tanh()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): Tanh()
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Tanh()
      (6): Linear(in_features=1024, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=1024, bias=True)
      (1): Tanh()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): Tanh()
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Tanh()
      (6): Linear(in_features=1024, out_features=6, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 32, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [1024, 1024, 1024], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 4, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2.
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 98
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=1024, bias=True)
      (1): Tanh()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): Tanh()
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Tanh()
      (6): Linear(in_features=1024, out_features=6, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=1024, bias=True)
      (1): Tanh()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): Tanh()
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Tanh()
      (6): Linear(in_features=1024, out_features=6, bias=True)
    )
  )
)
Episode: 1/10000 (0.0100%),                 avg. length: 425.0,                last time consumption/overall running time: 3.4713s / 3.4713 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0045
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 568.6,                last time consumption/overall running time: 64.7920s / 68.2633 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0080
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 600.85,                last time consumption/overall running time: 69.7127s / 137.9761 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0120
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 557.75,                last time consumption/overall running time: 104.8201s / 242.7962 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0119
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0063
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/49_0.
Episode: 81/10000 (0.8100%),                 avg. length: 570.7,                last time consumption/overall running time: 174.8762s / 417.6724 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0093
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 623.1,                last time consumption/overall running time: 230.3181s / 647.9904 s
env0_first_0:                 episode reward: 1.0500,                 loss: nan
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0106
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 589.6,                last time consumption/overall running time: 218.7444s / 866.7348 s
env0_first_0:                 episode reward: 0.8000,                 loss: nan
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0112
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 570.45,                last time consumption/overall running time: 213.1088s / 1079.8436 s
env0_first_0:                 episode reward: 0.6000,                 loss: nan
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0120
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 558.45,                last time consumption/overall running time: 208.2784s / 1288.1221 s
env0_first_0:                 episode reward: -0.1000,                 loss: nan
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0129
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 565.1,                last time consumption/overall running time: 211.3262s / 1499.4483 s
env0_first_0:                 episode reward: -0.2000,                 loss: nan
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0142
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 572.6,                last time consumption/overall running time: 213.3634s / 1712.8116 s
env0_first_0:                 episode reward: -0.9500,                 loss: nan
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0156
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 580.05,                last time consumption/overall running time: 218.1045s / 1930.9161 s
env0_first_0:                 episode reward: 0.7000,                 loss: nan
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0161
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 571.75,                last time consumption/overall running time: 214.4334s / 2145.3495 s
env0_first_0:                 episode reward: -1.0000,                 loss: nan
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0171
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 611.35,                last time consumption/overall running time: 238.8603s / 2384.2098 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0115
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0168
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/244_1.
Episode: 281/10000 (2.8100%),                 avg. length: 630.0,                last time consumption/overall running time: 234.1645s / 2618.3742 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0127
env0_second_0:                 episode reward: -0.8500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 574.15,                last time consumption/overall running time: 213.7512s / 2832.1254 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0139
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 604.35,                last time consumption/overall running time: 226.7332s / 3058.8586 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0154
env0_second_0:                 episode reward: -0.5500,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 608.2,                last time consumption/overall running time: 228.8694s / 3287.7280 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0171
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 588.75,                last time consumption/overall running time: 220.3254s / 3508.0534 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0163
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 604.1,                last time consumption/overall running time: 227.6204s / 3735.6738 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0167
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 599.5,                last time consumption/overall running time: 224.3806s / 3960.0544 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0176
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 615.15,                last time consumption/overall running time: 230.4173s / 4190.4717 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0188
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 605.65,                last time consumption/overall running time: 226.5696s / 4417.0413 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0194
env0_second_0:                 episode reward: -0.7000,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 598.85,                last time consumption/overall running time: 223.8742s / 4640.9155 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0205
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 571.7,                last time consumption/overall running time: 214.4766s / 4855.3921 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0199
env0_second_0:                 episode reward: -0.6000,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 627.0,                last time consumption/overall running time: 235.2202s / 5090.6123 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0207
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 551.35,                last time consumption/overall running time: 206.9409s / 5297.5531 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0199
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 586.7,                last time consumption/overall running time: 219.3349s / 5516.8881 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0187
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 611.35,                last time consumption/overall running time: 228.0026s / 5744.8907 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0199
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 590.85,                last time consumption/overall running time: 220.6814s / 5965.5720 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0207
env0_second_0:                 episode reward: -0.6000,                 loss: nan
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 598.45,                last time consumption/overall running time: 221.7729s / 6187.3450 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0213
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 582.35,                last time consumption/overall running time: 216.3240s / 6403.6690 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0213
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 590.25,                last time consumption/overall running time: 231.9161s / 6635.5850 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0209
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0182
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/626_0.
Episode: 661/10000 (6.6100%),                 avg. length: 625.35,                last time consumption/overall running time: 233.4955s / 6869.0806 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0190
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 645.15,                last time consumption/overall running time: 241.3287s / 7110.4092 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0186
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 600.4,                last time consumption/overall running time: 223.1676s / 7333.5769 s
env0_first_0:                 episode reward: -0.4000,                 loss: nan
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0197
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 589.8,                last time consumption/overall running time: 240.7673s / 7574.3442 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0209
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0195
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Score delta: 4.6, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/710_1.
Episode: 741/10000 (7.4100%),                 avg. length: 630.95,                last time consumption/overall running time: 235.5294s / 7809.8735 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0209
env0_second_0:                 episode reward: -0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 555.4,                last time consumption/overall running time: 208.2772s / 8018.1507 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0217
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 597.1,                last time consumption/overall running time: 221.2049s / 8239.3556 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0210
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 593.05,                last time consumption/overall running time: 219.3241s / 8458.6797 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0198
env0_second_0:                 episode reward: -1.2500,                 loss: nan
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 606.95,                last time consumption/overall running time: 225.0743s / 8683.7540 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0205
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 600.0,                last time consumption/overall running time: 223.5417s / 8907.2957 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0193
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 591.65,                last time consumption/overall running time: 220.2179s / 9127.5137 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0188
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 604.55,                last time consumption/overall running time: 224.3167s / 9351.8304 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0187
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 579.1,                last time consumption/overall running time: 215.3504s / 9567.1808 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0190
env0_second_0:                 episode reward: -0.8500,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 575.9,                last time consumption/overall running time: 215.1916s / 9782.3724 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0176
env0_second_0:                 episode reward: -0.5500,                 loss: nan
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 605.8,                last time consumption/overall running time: 225.6921s / 10008.0644 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0184
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 634.95,                last time consumption/overall running time: 235.4794s / 10243.5438 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0205
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 606.5,                last time consumption/overall running time: 225.9417s / 10469.4855 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0211
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 622.25,                last time consumption/overall running time: 234.0157s / 10703.5012 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0192
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 584.75,                last time consumption/overall running time: 217.9621s / 10921.4633 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0173
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 584.9,                last time consumption/overall running time: 217.8666s / 11139.3299 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0179
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 599.5,                last time consumption/overall running time: 223.7031s / 11363.0329 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0182
env0_second_0:                 episode reward: -1.2000,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 602.3,                last time consumption/overall running time: 223.2212s / 11586.2541 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0190
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 588.8,                last time consumption/overall running time: 242.3998s / 11828.6539 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0188
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0176
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Score delta: 4.8, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/1100_0.
Episode: 1121/10000 (11.2100%),                 avg. length: 585.9,                last time consumption/overall running time: 218.9206s / 12047.5744 s
env0_first_0:                 episode reward: 0.3500,                 loss: nan
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0186
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 580.7,                last time consumption/overall running time: 212.7745s / 12260.3489 s
env0_first_0:                 episode reward: 0.1500,                 loss: nan
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0184
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 629.25,                last time consumption/overall running time: 233.5619s / 12493.9108 s
env0_first_0:                 episode reward: -0.0500,                 loss: nan
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0195
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 616.0,                last time consumption/overall running time: 228.8716s / 12722.7825 s
env0_first_0:                 episode reward: 0.1500,                 loss: nan
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0195
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 601.0,                last time consumption/overall running time: 224.6788s / 12947.4613 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0199
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 594.2,                last time consumption/overall running time: 219.9258s / 13167.3871 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0208
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 577.9,                last time consumption/overall running time: 214.1770s / 13381.5641 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0195
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 639.7,                last time consumption/overall running time: 236.1430s / 13617.7071 s
env0_first_0:                 episode reward: 0.8500,                 loss: nan
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0194
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 592.2,                last time consumption/overall running time: 218.1417s / 13835.8488 s
env0_first_0:                 episode reward: 0.7500,                 loss: nan
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0207
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 630.95,                last time consumption/overall running time: 234.2221s / 14070.0709 s
env0_first_0:                 episode reward: -0.0500,                 loss: nan
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0196
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 577.4,                last time consumption/overall running time: 213.8964s / 14283.9672 s
env0_first_0:                 episode reward: 0.6500,                 loss: nan
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0199
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 619.55,                last time consumption/overall running time: 230.9106s / 14514.8778 s
env0_first_0:                 episode reward: -0.2000,                 loss: nan
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0211
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 634.15,                last time consumption/overall running time: 236.4171s / 14751.2950 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0204
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 578.25,                last time consumption/overall running time: 215.8658s / 14967.1608 s
env0_first_0:                 episode reward: 0.8000,                 loss: nan
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0205
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 607.5,                last time consumption/overall running time: 224.4185s / 15191.5793 s
env0_first_0:                 episode reward: -0.9000,                 loss: nan
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0200
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 640.55,                last time consumption/overall running time: 236.3653s / 15427.9446 s
env0_first_0:                 episode reward: -0.3000,                 loss: nan
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0202
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 616.2,                last time consumption/overall running time: 228.1349s / 15656.0795 s
env0_first_0:                 episode reward: 0.8000,                 loss: nan
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0195
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 615.0,                last time consumption/overall running time: 229.4420s / 15885.5215 s
env0_first_0:                 episode reward: -0.3000,                 loss: nan
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0188
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 582.0,                last time consumption/overall running time: 214.6239s / 16100.1454 s
env0_first_0:                 episode reward: 0.1000,                 loss: nan
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0190
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 601.3,                last time consumption/overall running time: 225.1178s / 16325.2632 s
env0_first_0:                 episode reward: -0.7000,                 loss: nan
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0189
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 607.95,                last time consumption/overall running time: 227.9852s / 16553.2484 s
env0_first_0:                 episode reward: 0.9500,                 loss: nan
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0180
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 655.35,                last time consumption/overall running time: 242.8957s / 16796.1441 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0189
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 627.25,                last time consumption/overall running time: 231.2074s / 17027.3515 s
env0_first_0:                 episode reward: 0.3500,                 loss: nan
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0182
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 612.35,                last time consumption/overall running time: 224.4368s / 17251.7883 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0188
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 642.65,                last time consumption/overall running time: 235.9633s / 17487.7516 s
env0_first_0:                 episode reward: -0.8000,                 loss: nan
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0184
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 603.0,                last time consumption/overall running time: 219.5528s / 17707.3043 s
env0_first_0:                 episode reward: 0.9500,                 loss: nan
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0176
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 607.0,                last time consumption/overall running time: 220.6113s / 17927.9156 s
env0_first_0:                 episode reward: -0.2000,                 loss: nan
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0179
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 636.95,                last time consumption/overall running time: 235.5877s / 18163.5033 s
env0_first_0:                 episode reward: -0.4500,                 loss: nan
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0182
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 667.9,                last time consumption/overall running time: 245.9430s / 18409.4463 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0188
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 619.8,                last time consumption/overall running time: 226.5287s / 18635.9750 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0178
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 640.6,                last time consumption/overall running time: 235.0955s / 18871.0705 s
env0_first_0:                 episode reward: -1.2500,                 loss: nan
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0175
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 677.35,                last time consumption/overall running time: 250.5070s / 19121.5774 s
env0_first_0:                 episode reward: -1.1000,                 loss: nan
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0172
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 596.9,                last time consumption/overall running time: 258.8218s / 19380.3992 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0175
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0194
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Score delta: 5.0, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/1742_1.
Episode: 1781/10000 (17.8100%),                 avg. length: 632.9,                last time consumption/overall running time: 234.6764s / 19615.0756 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0176
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 652.35,                last time consumption/overall running time: 243.3022s / 19858.3779 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0173
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 651.35,                last time consumption/overall running time: 240.3448s / 20098.7227 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0169
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 627.05,                last time consumption/overall running time: 231.3353s / 20330.0580 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0168
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 625.0,                last time consumption/overall running time: 229.4477s / 20559.5057 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0182
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 674.1,                last time consumption/overall running time: 244.2777s / 20803.7834 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0176
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 660.4,                last time consumption/overall running time: 242.9189s / 21046.7023 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0175
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 657.0,                last time consumption/overall running time: 242.7011s / 21289.4033 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0178
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 661.7,                last time consumption/overall running time: 244.8191s / 21534.2225 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0184
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 707.4,                last time consumption/overall running time: 259.3702s / 21793.5927 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0177
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 654.65,                last time consumption/overall running time: 245.2660s / 22038.8587 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0181
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 738.2,                last time consumption/overall running time: 274.1016s / 22312.9602 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0185
env0_second_0:                 episode reward: -0.5500,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 778.2,                last time consumption/overall running time: 286.7316s / 22599.6918 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0169
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 709.55,                last time consumption/overall running time: 328.2010s / 22927.8929 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0164
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0156
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/2035_0.
Episode: 2061/10000 (20.6100%),                 avg. length: 889.8,                last time consumption/overall running time: 326.3289s / 23254.2218 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0152
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 833.6,                last time consumption/overall running time: 305.0706s / 23559.2924 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0138
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 893.65,                last time consumption/overall running time: 329.9517s / 23889.2441 s
env0_first_0:                 episode reward: 1.9500,                 loss: nan
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0123
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 789.55,                last time consumption/overall running time: 288.7490s / 24177.9931 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0120
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 848.4,                last time consumption/overall running time: 313.9299s / 24491.9230 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0129
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 711.5,                last time consumption/overall running time: 267.2356s / 24759.1586 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0109
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 743.25,                last time consumption/overall running time: 276.5409s / 25035.6995 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0110
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 907.6,                last time consumption/overall running time: 328.7979s / 25364.4974 s
env0_first_0:                 episode reward: 2.2000,                 loss: nan
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0107
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 747.45,                last time consumption/overall running time: 274.4986s / 25638.9960 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0126
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 785.9,                last time consumption/overall running time: 290.7939s / 25929.7899 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0151
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 844.6,                last time consumption/overall running time: 313.9039s / 26243.6938 s
env0_first_0:                 episode reward: 1.5000,                 loss: nan
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0147
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 803.55,                last time consumption/overall running time: 295.2774s / 26538.9712 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0149
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 950.05,                last time consumption/overall running time: 350.6526s / 26889.6238 s
env0_first_0:                 episode reward: 1.7000,                 loss: nan
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0161
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 816.55,                last time consumption/overall running time: 302.9258s / 27192.5496 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0176
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 902.95,                last time consumption/overall running time: 337.2791s / 27529.8287 s
env0_first_0:                 episode reward: 2.1500,                 loss: nan
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0168
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 906.7,                last time consumption/overall running time: 335.6672s / 27865.4959 s
env0_first_0:                 episode reward: 1.9000,                 loss: nan
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0169
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 830.4,                last time consumption/overall running time: 306.6162s / 28172.1121 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0160
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 792.4,                last time consumption/overall running time: 289.4550s / 28461.5672 s
env0_first_0:                 episode reward: 1.9000,                 loss: nan
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0141
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 833.8,                last time consumption/overall running time: 307.8789s / 28769.4461 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0130
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 813.25,                last time consumption/overall running time: 298.2836s / 29067.7297 s
env0_first_0:                 episode reward: 1.4000,                 loss: nan
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0120
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 825.4,                last time consumption/overall running time: 304.8682s / 29372.5978 s
env0_first_0:                 episode reward: 2.1500,                 loss: nan
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0124
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 871.75,                last time consumption/overall running time: 324.6288s / 29697.2267 s
env0_first_0:                 episode reward: 2.2500,                 loss: nan
env0_second_0:                 episode reward: -2.2500,                 loss: 0.0126
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 792.0,                last time consumption/overall running time: 292.3544s / 29989.5810 s
env0_first_0:                 episode reward: 2.2000,                 loss: nan
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0130
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 895.4,                last time consumption/overall running time: 332.0395s / 30321.6205 s
env0_first_0:                 episode reward: 2.3000,                 loss: nan
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0129
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 984.35,                last time consumption/overall running time: 365.0639s / 30686.6844 s
env0_first_0:                 episode reward: 1.9000,                 loss: nan
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0119
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 893.4,                last time consumption/overall running time: 331.3078s / 31017.9922 s
env0_first_0:                 episode reward: 1.8500,                 loss: nan
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0118
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 918.25,                last time consumption/overall running time: 337.4044s / 31355.3966 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0111
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 991.9,                last time consumption/overall running time: 367.6964s / 31723.0930 s
env0_first_0:                 episode reward: 1.8000,                 loss: nan
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0118
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1028.0,                last time consumption/overall running time: 373.6930s / 32096.7860 s
env0_first_0:                 episode reward: 1.9000,                 loss: nan
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0123
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1046.9,                last time consumption/overall running time: 385.7553s / 32482.5413 s
env0_first_0:                 episode reward: 1.8500,                 loss: nan
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0115
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1040.95,                last time consumption/overall running time: 384.9333s / 32867.4746 s
env0_first_0:                 episode reward: 1.7000,                 loss: nan
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0099
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1194.8,                last time consumption/overall running time: 436.6954s / 33304.1700 s
env0_first_0:                 episode reward: 1.4000,                 loss: nan
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0101
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1220.3,                last time consumption/overall running time: 441.6695s / 33745.8395 s
env0_first_0:                 episode reward: 1.3000,                 loss: nan
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0099
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1186.35,                last time consumption/overall running time: 439.7961s / 34185.6355 s
env0_first_0:                 episode reward: 0.6500,                 loss: nan
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0089
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1377.1,                last time consumption/overall running time: 515.8160s / 34701.4516 s
env0_first_0:                 episode reward: -0.2000,                 loss: nan
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0089
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1321.4,                last time consumption/overall running time: 493.9743s / 35195.4259 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0082
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1340.0,                last time consumption/overall running time: 488.7673s / 35684.1932 s
env0_first_0:                 episode reward: 0.3000,                 loss: nan
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0080
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1407.7,                last time consumption/overall running time: 517.6764s / 36201.8697 s
env0_first_0:                 episode reward: -0.8000,                 loss: nan
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0070
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1251.9,                last time consumption/overall running time: 462.2598s / 36664.1295 s
env0_first_0:                 episode reward: 0.4000,                 loss: nan
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0067
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1254.65,                last time consumption/overall running time: 465.2151s / 37129.3446 s
env0_first_0:                 episode reward: 1.3500,                 loss: nan
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0079
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1206.6,                last time consumption/overall running time: 466.3019s / 37595.6465 s
env0_first_0:                 episode reward: -0.2500,                 loss: nan
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0081
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1425.4,                last time consumption/overall running time: 615.5625s / 38211.2090 s
env0_first_0:                 episode reward: -0.3000,                 loss: nan
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0078
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1436.45,                last time consumption/overall running time: 631.7538s / 38842.9628 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0074
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 943.55,                last time consumption/overall running time: 499.6969s / 39342.6597 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0168
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0073
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/2909_1.
Episode: 2941/10000 (29.4100%),                 avg. length: 701.45,                last time consumption/overall running time: 310.3109s / 39652.9706 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0156
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 747.1,                last time consumption/overall running time: 395.5103s / 40048.4809 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0154
env0_second_0:                 episode reward: -1.7000,                 loss: nan
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Score delta: 4.6, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/2961_0.
Episode: 2981/10000 (29.8100%),                 avg. length: 683.8,                last time consumption/overall running time: 303.0778s / 40351.5587 s
env0_first_0:                 episode reward: 1.9000,                 loss: nan
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0106
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 715.95,                last time consumption/overall running time: 317.7015s / 40669.2602 s
env0_first_0:                 episode reward: 1.2500,                 loss: nan
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0117
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 763.45,                last time consumption/overall running time: 335.2212s / 41004.4814 s
env0_first_0:                 episode reward: 0.8000,                 loss: nan
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0139
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 833.5,                last time consumption/overall running time: 369.0239s / 41373.5054 s
env0_first_0:                 episode reward: -0.1000,                 loss: nan
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0148
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 829.75,                last time consumption/overall running time: 365.6816s / 41739.1870 s
env0_first_0:                 episode reward: 0.4000,                 loss: nan
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0141
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 892.5,                last time consumption/overall running time: 392.1290s / 42131.3161 s
env0_first_0:                 episode reward: -1.1500,                 loss: nan
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0136
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 838.2,                last time consumption/overall running time: 476.1714s / 42607.4875 s
env0_first_0:                 episode reward: -1.5000,                 loss: nan
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0128
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Score delta: 4.6, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/3101_1.
Episode: 3121/10000 (31.2100%),                 avg. length: 941.8,                last time consumption/overall running time: 415.5727s / 43023.0602 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0142
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1060.0,                last time consumption/overall running time: 469.7224s / 43492.7826 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0126
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1144.75,                last time consumption/overall running time: 505.2237s / 43998.0063 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0112
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1109.8,                last time consumption/overall running time: 491.7523s / 44489.7586 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0108
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1080.65,                last time consumption/overall running time: 481.2333s / 44970.9919 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0103
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1133.8,                last time consumption/overall running time: 502.2947s / 45473.2866 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0093
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 989.65,                last time consumption/overall running time: 438.9386s / 45912.2251 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0097
env0_second_0:                 episode reward: 0.5000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 838.3,                last time consumption/overall running time: 373.6026s / 46285.8277 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0088
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 897.9,                last time consumption/overall running time: 398.6574s / 46684.4850 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0093
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1059.95,                last time consumption/overall running time: 467.7672s / 47152.2522 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0095
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1084.25,                last time consumption/overall running time: 476.0770s / 47628.3292 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0098
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1131.95,                last time consumption/overall running time: 500.6614s / 48128.9906 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0091
env0_second_0:                 episode reward: -0.9000,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1113.5,                last time consumption/overall running time: 489.7334s / 48618.7240 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0085
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1036.65,                last time consumption/overall running time: 460.3971s / 49079.1212 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0090
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1335.4,                last time consumption/overall running time: 595.0104s / 49674.1315 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0092
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1218.55,                last time consumption/overall running time: 541.7960s / 50215.9276 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0081
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1266.05,                last time consumption/overall running time: 561.8786s / 50777.8062 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0079
env0_second_0:                 episode reward: 0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1307.8,                last time consumption/overall running time: 576.6744s / 51354.4807 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0071
env0_second_0:                 episode reward: -1.0500,                 loss: nan
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1120.2,                last time consumption/overall running time: 492.5215s / 51847.0021 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0074
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1093.15,                last time consumption/overall running time: 484.4079s / 52331.4101 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0082
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1213.95,                last time consumption/overall running time: 537.7349s / 52869.1450 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0093
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1217.85,                last time consumption/overall running time: 538.0183s / 53407.1633 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0100
env0_second_0:                 episode reward: -1.2000,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1201.75,                last time consumption/overall running time: 532.1524s / 53939.3157 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0094
env0_second_0:                 episode reward: -1.1500,                 loss: nan
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1151.55,                last time consumption/overall running time: 507.3774s / 54446.6931 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0078
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1261.5,                last time consumption/overall running time: 556.2162s / 55002.9092 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0078
env0_second_0:                 episode reward: -1.2500,                 loss: nan
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1180.05,                last time consumption/overall running time: 524.3590s / 55527.2682 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0072
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1402.5,                last time consumption/overall running time: 621.5351s / 56148.8034 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0072
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1298.0,                last time consumption/overall running time: 568.2712s / 56717.0745 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0066
env0_second_0:                 episode reward: -1.3500,                 loss: nan
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1158.5,                last time consumption/overall running time: 615.5598s / 57332.6344 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0065
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0116
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/3675_0.
Episode: 3701/10000 (37.0100%),                 avg. length: 1250.55,                last time consumption/overall running time: 551.0661s / 57883.7005 s
env0_first_0:                 episode reward: 1.7000,                 loss: nan
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0108
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1080.3,                last time consumption/overall running time: 478.9728s / 58362.6733 s
env0_first_0:                 episode reward: 1.4500,                 loss: nan
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0088
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1054.55,                last time consumption/overall running time: 462.2371s / 58824.9104 s
env0_first_0:                 episode reward: 0.1000,                 loss: nan
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0091
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1115.25,                last time consumption/overall running time: 486.1084s / 59311.0188 s
env0_first_0:                 episode reward: 0.6000,                 loss: nan
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0098
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1078.0,                last time consumption/overall running time: 473.0771s / 59784.0959 s
env0_first_0:                 episode reward: 0.6500,                 loss: nan
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0097
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 901.7,                last time consumption/overall running time: 386.7954s / 60170.8913 s
env0_first_0:                 episode reward: 1.5000,                 loss: nan
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0090
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1040.6,                last time consumption/overall running time: 452.5678s / 60623.4591 s
env0_first_0:                 episode reward: 1.3500,                 loss: nan
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0103
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 973.05,                last time consumption/overall running time: 433.4424s / 61056.9015 s
env0_first_0:                 episode reward: 1.0000,                 loss: nan
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0115
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1133.8,                last time consumption/overall running time: 502.9566s / 61559.8582 s
env0_first_0:                 episode reward: 0.8500,                 loss: nan
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0111
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1106.85,                last time consumption/overall running time: 486.0908s / 62045.9490 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0101
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1109.25,                last time consumption/overall running time: 490.0978s / 62536.0468 s
env0_first_0:                 episode reward: 1.1500,                 loss: nan
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0091
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1281.1,                last time consumption/overall running time: 566.5644s / 63102.6112 s
env0_first_0:                 episode reward: 0.3500,                 loss: nan
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0079
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1113.9,                last time consumption/overall running time: 497.1962s / 63599.8073 s
env0_first_0:                 episode reward: 1.1000,                 loss: nan
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0079
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1448.5,                last time consumption/overall running time: 647.0206s / 64246.8279 s
env0_first_0:                 episode reward: 0.5500,                 loss: nan
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0078
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1169.15,                last time consumption/overall running time: 518.5832s / 64765.4111 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0071
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1133.05,                last time consumption/overall running time: 499.0464s / 65264.4575 s
env0_first_0:                 episode reward: 0.3500,                 loss: nan
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0076
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1234.8,                last time consumption/overall running time: 541.4074s / 65805.8648 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0079
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1270.15,                last time consumption/overall running time: 555.6679s / 66361.5327 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0073
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1307.8,                last time consumption/overall running time: 574.2205s / 66935.7532 s
env0_first_0:                 episode reward: -1.3000,                 loss: nan
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1321.5,                last time consumption/overall running time: 581.2037s / 67516.9569 s
env0_first_0:                 episode reward: -0.1000,                 loss: nan
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 943.35,                last time consumption/overall running time: 526.7465s / 68043.7035 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0070
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0068
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/4087_1.
Episode: 4121/10000 (41.2100%),                 avg. length: 1012.55,                last time consumption/overall running time: 566.1374s / 68609.8409 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0075
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0072
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Score delta: 4.6, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/4111_0.
Episode: 4141/10000 (41.4100%),                 avg. length: 1138.4,                last time consumption/overall running time: 510.1266s / 69119.9674 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0076
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1275.7,                last time consumption/overall running time: 564.2761s / 69684.2435 s
env0_first_0:                 episode reward: -1.0000,                 loss: nan
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0076
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1135.0,                last time consumption/overall running time: 504.5813s / 70188.8249 s
env0_first_0:                 episode reward: -1.1500,                 loss: nan
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0071
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1317.85,                last time consumption/overall running time: 588.0006s / 70776.8255 s
env0_first_0:                 episode reward: -0.7000,                 loss: nan
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0069
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1374.3,                last time consumption/overall running time: 608.1432s / 71384.9686 s
env0_first_0:                 episode reward: 0.5500,                 loss: nan
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1158.15,                last time consumption/overall running time: 508.4156s / 71893.3842 s
env0_first_0:                 episode reward: 0.5500,                 loss: nan
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0064
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1004.65,                last time consumption/overall running time: 433.5476s / 72326.9318 s
env0_first_0:                 episode reward: 0.4500,                 loss: nan
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0070
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1177.8,                last time consumption/overall running time: 512.2414s / 72839.1732 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0083
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1300.1,                last time consumption/overall running time: 570.2704s / 73409.4436 s
env0_first_0:                 episode reward: 0.1000,                 loss: nan
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0072
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1185.5,                last time consumption/overall running time: 517.0490s / 73926.4926 s
env0_first_0:                 episode reward: 0.0500,                 loss: nan
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0069
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1043.3,                last time consumption/overall running time: 583.9348s / 74510.4274 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0075
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0075
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/4335_1.
Episode: 4361/10000 (43.6100%),                 avg. length: 964.1,                last time consumption/overall running time: 429.2726s / 74939.7000 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0084
env0_second_0:                 episode reward: -0.8000,                 loss: nan
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 962.2,                last time consumption/overall running time: 570.1555s / 75509.8556 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0090
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0115
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/4378_0.
Episode: 4401/10000 (44.0100%),                 avg. length: 1218.4,                last time consumption/overall running time: 543.2375s / 76053.0931 s
env0_first_0:                 episode reward: 1.4000,                 loss: nan
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0089
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1180.65,                last time consumption/overall running time: 524.8059s / 76577.8990 s
env0_first_0:                 episode reward: 0.7000,                 loss: nan
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0087
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 944.8,                last time consumption/overall running time: 627.6246s / 77205.5236 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0088
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0094
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Score delta: 4.8, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/4434_1.
Episode: 4461/10000 (44.6100%),                 avg. length: 932.65,                last time consumption/overall running time: 580.3584s / 77785.8820 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0087
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0085
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Score delta: 5.0, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/4455_0.
Episode: 4481/10000 (44.8100%),                 avg. length: 1022.85,                last time consumption/overall running time: 455.1231s / 78241.0051 s
env0_first_0:                 episode reward: 1.3000,                 loss: nan
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0087
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1059.25,                last time consumption/overall running time: 466.6206s / 78707.6257 s
env0_first_0:                 episode reward: -0.0500,                 loss: nan
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0088
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1057.85,                last time consumption/overall running time: 467.9657s / 79175.5915 s
env0_first_0:                 episode reward: 0.8000,                 loss: nan
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0088
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1269.9,                last time consumption/overall running time: 845.7665s / 80021.3579 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0086
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0096
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Score delta: 4.6, save the model to .//data/model/20220119_0521/slimevolley_SlimeVolley-v0_nxdo2/4536_1.
Episode: 4561/10000 (45.6100%),                 avg. length: 1922.8,                last time consumption/overall running time: 854.1625s / 80875.5204 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0062
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1649.55,                last time consumption/overall running time: 728.0111s / 81603.5315 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0043
env0_second_0:                 episode reward: 2.1500,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1360.9,                last time consumption/overall running time: 596.2526s / 82199.7841 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0053
env0_second_0:                 episode reward: 2.7000,                 loss: nan
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1599.1,                last time consumption/overall running time: 707.9789s / 82907.7630 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0058
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1537.7,                last time consumption/overall running time: 682.0202s / 83589.7832 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0061
env0_second_0:                 episode reward: 2.5000,                 loss: nan
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1803.05,                last time consumption/overall running time: 791.2930s / 84381.0762 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0058
env0_second_0:                 episode reward: 0.8500,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1842.6,                last time consumption/overall running time: 796.2699s / 85177.3460 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0059
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1693.65,                last time consumption/overall running time: 742.2837s / 85919.6298 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0061
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1570.1,                last time consumption/overall running time: 697.9649s / 86617.5947 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0065
env0_second_0:                 episode reward: 2.1500,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1323.05,                last time consumption/overall running time: 585.6576s / 87203.2522 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0065
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1860.4,                last time consumption/overall running time: 826.5880s / 88029.8402 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0065
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1437.5,                last time consumption/overall running time: 639.1822s / 88669.0224 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0055
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1770.7,                last time consumption/overall running time: 783.1628s / 89452.1852 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0053
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1675.7,                last time consumption/overall running time: 743.8568s / 90196.0420 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0048
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1511.5,                last time consumption/overall running time: 664.9029s / 90860.9449 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0047
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1463.7,                last time consumption/overall running time: 637.2737s / 91498.2186 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0053
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1720.8,                last time consumption/overall running time: 760.3282s / 92258.5468 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0061
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1681.45,                last time consumption/overall running time: 745.2531s / 93003.7999 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0049
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 2129.5,                last time consumption/overall running time: 949.8108s / 93953.6107 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0053
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1645.5,                last time consumption/overall running time: 736.5204s / 94690.1311 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0045
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1585.8,                last time consumption/overall running time: 708.5719s / 95398.7030 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0050
env0_second_0:                 episode reward: 2.1500,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 2060.0,                last time consumption/overall running time: 908.2753s / 96306.9783 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0045
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1938.45,                last time consumption/overall running time: 859.8085s / 97166.7868 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0044
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1929.85,                last time consumption/overall running time: 841.1563s / 98007.9432 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0043
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1650.2,                last time consumption/overall running time: 711.3116s / 98719.2548 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0047
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 2028.65,                last time consumption/overall running time: 884.9807s / 99604.2355 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0047
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1574.1,                last time consumption/overall running time: 693.2389s / 100297.4744 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0046
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1861.85,                last time consumption/overall running time: 814.5665s / 101112.0410 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1835.5,                last time consumption/overall running time: 798.8543s / 101910.8952 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0047
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1662.2,                last time consumption/overall running time: 740.8616s / 102651.7568 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0050
env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1909.8,                last time consumption/overall running time: 851.9305s / 103503.6873 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0055
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 2017.55,                last time consumption/overall running time: 888.6262s / 104392.3135 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0048
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1659.4,                last time consumption/overall running time: 721.1500s / 105113.4636 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0048
env0_second_0:                 episode reward: 2.0500,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1671.45,                last time consumption/overall running time: 743.7419s / 105857.2054 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0053
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1567.9,                last time consumption/overall running time: 689.6342s / 106546.8396 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0054
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1618.4,                last time consumption/overall running time: 709.6372s / 107256.4768 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0058
env0_second_0:                 episode reward: 2.1500,                 loss: nan
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 2269.35,                last time consumption/overall running time: 997.3639s / 108253.8407 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0048
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1631.0,                last time consumption/overall running time: 724.9806s / 108978.8214 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0046
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1940.1,                last time consumption/overall running time: 859.7540s / 109838.5754 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0054
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 2215.7,                last time consumption/overall running time: 977.3080s / 110815.8834 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1911.65,                last time consumption/overall running time: 837.1794s / 111653.0628 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0045
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1995.7,                last time consumption/overall running time: 874.1659s / 112527.2287 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0047
env0_second_0:                 episode reward: 2.8000,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1943.4,                last time consumption/overall running time: 856.7361s / 113383.9648 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0047
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1946.7,                last time consumption/overall running time: 864.5888s / 114248.5536 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0045
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 2015.05,                last time consumption/overall running time: 894.2169s / 115142.7705 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0046
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 2367.95,                last time consumption/overall running time: 1055.6942s / 116198.4647 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0044
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1933.25,                last time consumption/overall running time: 855.1641s / 117053.6288 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0042
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1794.6,                last time consumption/overall running time: 786.7659s / 117840.3947 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 1683.3,                last time consumption/overall running time: 723.8473s / 118564.2420 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0049
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1684.9,                last time consumption/overall running time: 736.4847s / 119300.7267 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1766.35,                last time consumption/overall running time: 785.5393s / 120086.2659 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0052
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 2022.2,                last time consumption/overall running time: 898.4577s / 120984.7236 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0050
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 2388.5,                last time consumption/overall running time: 1053.2103s / 122037.9340 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0046
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1994.7,                last time consumption/overall running time: 879.2586s / 122917.1926 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0047
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1981.0,                last time consumption/overall running time: 880.1760s / 123797.3685 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 2065.5,                last time consumption/overall running time: 913.7861s / 124711.1547 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0046
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1878.95,                last time consumption/overall running time: 807.9847s / 125519.1394 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0050
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 2329.3,                last time consumption/overall running time: 1005.0107s / 126524.1501 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0047
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 1715.25,                last time consumption/overall running time: 758.8221s / 127282.9722 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0050
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 1975.4,                last time consumption/overall running time: 864.5464s / 128147.5185 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 2089.8,                last time consumption/overall running time: 912.7682s / 129060.2867 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0044
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1821.6,                last time consumption/overall running time: 801.8459s / 129862.1326 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0050
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 1620.15,                last time consumption/overall running time: 713.1970s / 130575.3296 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0054
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1566.65,                last time consumption/overall running time: 691.7681s / 131267.0977 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0062
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 2131.3,                last time consumption/overall running time: 927.4973s / 132194.5950 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0055
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 1781.5,                last time consumption/overall running time: 767.8735s / 132962.4684 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0054
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1736.55,                last time consumption/overall running time: 753.8505s / 133716.3189 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0052
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1983.35,                last time consumption/overall running time: 874.0477s / 134590.3666 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0051
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 1767.05,                last time consumption/overall running time: 783.5822s / 135373.9488 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0047
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 1723.9,                last time consumption/overall running time: 764.2951s / 136138.2439 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0054
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 1887.1,                last time consumption/overall running time: 835.1789s / 136973.4228 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0053
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 2064.5,                last time consumption/overall running time: 919.2665s / 137892.6893 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0052
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 2095.2,                last time consumption/overall running time: 920.8499s / 138813.5392 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0047
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 1864.1,                last time consumption/overall running time: 808.8301s / 139622.3692 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0048
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 2158.2,                last time consumption/overall running time: 939.4882s / 140561.8574 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 2048.45,                last time consumption/overall running time: 901.8736s / 141463.7310 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1787.45,                last time consumption/overall running time: 782.2468s / 142245.9779 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0052
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 2141.8,                last time consumption/overall running time: 934.2939s / 143180.2718 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0054
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1863.6,                last time consumption/overall running time: 823.2496s / 144003.5213 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0053
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1885.25,                last time consumption/overall running time: 840.5232s / 144844.0445 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0049
env0_second_0:                 episode reward: 2.6000,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1802.35,                last time consumption/overall running time: 795.9108s / 145639.9553 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0053
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 2123.3,                last time consumption/overall running time: 931.3460s / 146571.3013 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 2206.45,                last time consumption/overall running time: 962.0930s / 147533.3943 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0050
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1731.1,                last time consumption/overall running time: 755.3480s / 148288.7423 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 2134.05,                last time consumption/overall running time: 943.9047s / 149232.6470 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0055
env0_second_0:                 episode reward: 0.8500,                 loss: nan
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 1977.75,                last time consumption/overall running time: 868.3201s / 150100.9671 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0045
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 1900.3,                last time consumption/overall running time: 840.3036s / 150941.2707 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1841.15,                last time consumption/overall running time: 810.4605s / 151751.7312 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0054
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1951.2,                last time consumption/overall running time: 860.8678s / 152612.5990 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0053
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 2323.8,                last time consumption/overall running time: 1029.8273s / 153642.4264 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0052
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 2021.35,                last time consumption/overall running time: 880.5081s / 154522.9344 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 2229.5,                last time consumption/overall running time: 966.1594s / 155489.0938 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0046
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 2097.9,                last time consumption/overall running time: 919.7200s / 156408.8138 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0046
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 1661.75,                last time consumption/overall running time: 726.3834s / 157135.1972 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0051
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 2379.7,                last time consumption/overall running time: 1055.6341s / 158190.8313 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0049
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 1857.45,                last time consumption/overall running time: 817.3506s / 159008.1818 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 1981.6,                last time consumption/overall running time: 870.0423s / 159878.2241 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 1776.55,                last time consumption/overall running time: 787.6300s / 160665.8541 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 1938.0,                last time consumption/overall running time: 856.5157s / 161522.3699 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0054
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 2340.3,                last time consumption/overall running time: 1001.6900s / 162524.0599 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0049
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 2302.9,                last time consumption/overall running time: 990.1498s / 163514.2097 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0049
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 1967.35,                last time consumption/overall running time: 864.7253s / 164378.9350 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0052
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1950.35,                last time consumption/overall running time: 863.3067s / 165242.2417 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0056
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 2048.0,                last time consumption/overall running time: 908.0503s / 166150.2920 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0051
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 2002.05,                last time consumption/overall running time: 890.3722s / 167040.6643 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0048
env0_second_0:                 episode reward: 0.8500,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 1628.4,                last time consumption/overall running time: 723.9892s / 167764.6534 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0056
env0_second_0:                 episode reward: 2.1500,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 2118.35,                last time consumption/overall running time: 933.7204s / 168698.3738 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0058
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 2067.6,                last time consumption/overall running time: 904.2228s / 169602.5966 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 2106.85,                last time consumption/overall running time: 917.6037s / 170520.2003 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 1964.45,                last time consumption/overall running time: 854.6814s / 171374.8817 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0046
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 1825.65,                last time consumption/overall running time: 798.4356s / 172173.3172 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0055
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 2156.8,                last time consumption/overall running time: 946.2579s / 173119.5752 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0051
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1808.65,                last time consumption/overall running time: 802.0020s / 173921.5772 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0051
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 1649.1,                last time consumption/overall running time: 732.5737s / 174654.1509 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 2103.55,                last time consumption/overall running time: 933.4451s / 175587.5960 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0059
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 2029.9,                last time consumption/overall running time: 901.3827s / 176488.9787 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0052
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 2447.95,                last time consumption/overall running time: 1071.6880s / 177560.6667 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 1965.3,                last time consumption/overall running time: 849.2452s / 178409.9119 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0046
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 2240.3,                last time consumption/overall running time: 964.1066s / 179374.0186 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0051
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 2253.45,                last time consumption/overall running time: 985.0501s / 180359.0687 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0048
env0_second_0:                 episode reward: 2.0500,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 2121.3,                last time consumption/overall running time: 930.1567s / 181289.2254 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0055
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 2200.8,                last time consumption/overall running time: 962.7138s / 182251.9392 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0053
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 2180.85,                last time consumption/overall running time: 959.2326s / 183211.1718 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0051
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 2009.9,                last time consumption/overall running time: 894.7589s / 184105.9307 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0060
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 2195.25,                last time consumption/overall running time: 966.9235s / 185072.8541 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0050
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 1847.8,                last time consumption/overall running time: 810.0077s / 185882.8618 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0047
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 2016.0,                last time consumption/overall running time: 832.5724s / 186715.4342 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0050
env0_second_0:                 episode reward: 2.3500,                 loss: nan
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 2104.05,                last time consumption/overall running time: 872.1912s / 187587.6254 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0047
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 2318.65,                last time consumption/overall running time: 974.8970s / 188562.5224 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0048
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 2140.35,                last time consumption/overall running time: 904.3246s / 189466.8470 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0047
env0_second_0:                 episode reward: 2.1500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1991.6,                last time consumption/overall running time: 842.8075s / 190309.6545 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0047
env0_second_0:                 episode reward: 2.1500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 2378.55,                last time consumption/overall running time: 995.5005s / 191305.1550 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0056
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 2435.2,                last time consumption/overall running time: 1010.2844s / 192315.4394 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0049
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1992.2,                last time consumption/overall running time: 812.4961s / 193127.9355 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 1921.65,                last time consumption/overall running time: 785.7381s / 193913.6736 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 2289.45,                last time consumption/overall running time: 936.7585s / 194850.4321 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 1470.55,                last time consumption/overall running time: 606.9515s / 195457.3836 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0047
env0_second_0:                 episode reward: 2.9000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 2039.9,                last time consumption/overall running time: 846.2681s / 196303.6516 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0059
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 1903.5,                last time consumption/overall running time: 794.5433s / 197098.1949 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 1891.3,                last time consumption/overall running time: 782.1541s / 197880.3490 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0053
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 1802.65,                last time consumption/overall running time: 755.2508s / 198635.5998 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0053
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1749.35,                last time consumption/overall running time: 726.8349s / 199362.4347 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1780.15,                last time consumption/overall running time: 734.9589s / 200097.3936 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0054
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1923.5,                last time consumption/overall running time: 786.6940s / 200884.0875 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0054
env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 1718.7,                last time consumption/overall running time: 701.9498s / 201586.0373 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0056
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 2241.55,                last time consumption/overall running time: 916.8371s / 202502.8744 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0052
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 2001.65,                last time consumption/overall running time: 819.7105s / 203322.5849 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0046
env0_second_0:                 episode reward: 2.2500,                 loss: nan
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 2341.75,                last time consumption/overall running time: 976.0617s / 204298.6466 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0047
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 1835.9,                last time consumption/overall running time: 756.5502s / 205055.1969 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0042
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 1883.85,                last time consumption/overall running time: 774.4413s / 205829.6381 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0052
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 1569.25,                last time consumption/overall running time: 654.1708s / 206483.8089 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 1609.7,                last time consumption/overall running time: 674.5982s / 207158.4071 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0056
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 1584.75,                last time consumption/overall running time: 660.3058s / 207818.7129 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0055
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 2103.9,                last time consumption/overall running time: 875.8027s / 208694.5155 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0050
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 1972.65,                last time consumption/overall running time: 815.4200s / 209509.9355 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1846.45,                last time consumption/overall running time: 759.5468s / 210269.4823 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0052
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 2101.85,                last time consumption/overall running time: 862.3409s / 211131.8231 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0055
env0_second_0:                 episode reward: 0.5000,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 1977.2,                last time consumption/overall running time: 822.5397s / 211954.3629 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 1651.9,                last time consumption/overall running time: 687.1002s / 212641.4631 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 1943.3,                last time consumption/overall running time: 812.5405s / 213454.0036 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0053
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 1780.8,                last time consumption/overall running time: 749.8413s / 214203.8449 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0050
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 2092.9,                last time consumption/overall running time: 877.6367s / 215081.4817 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0053
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 2005.15,                last time consumption/overall running time: 841.5987s / 215923.0804 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0050
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 1681.75,                last time consumption/overall running time: 694.7883s / 216617.8687 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0055
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 2072.3,                last time consumption/overall running time: 836.9564s / 217454.8251 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0054
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 2094.35,                last time consumption/overall running time: 814.6828s / 218269.5080 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0048
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 1961.35,                last time consumption/overall running time: 767.4249s / 219036.9329 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0050
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 1583.8,                last time consumption/overall running time: 624.2794s / 219661.2124 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0049
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 2062.15,                last time consumption/overall running time: 817.5419s / 220478.7542 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0055
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 1725.05,                last time consumption/overall running time: 683.2058s / 221161.9601 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 1956.45,                last time consumption/overall running time: 774.9358s / 221936.8959 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0048
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 2097.35,                last time consumption/overall running time: 831.0317s / 222767.9276 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0051
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 2179.6,                last time consumption/overall running time: 853.3734s / 223621.3011 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0046
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 1858.35,                last time consumption/overall running time: 726.3435s / 224347.6445 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0050
env0_second_0:                 episode reward: 2.4000,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 2107.65,                last time consumption/overall running time: 814.6745s / 225162.3190 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 2106.55,                last time consumption/overall running time: 824.0011s / 225986.3201 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0053
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 1666.35,                last time consumption/overall running time: 652.7546s / 226639.0747 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 1938.75,                last time consumption/overall running time: 761.9353s / 227401.0099 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0055
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 2003.85,                last time consumption/overall running time: 791.3581s / 228192.3680 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0052
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 2218.35,                last time consumption/overall running time: 876.0309s / 229068.3989 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0046
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 1722.95,                last time consumption/overall running time: 676.8068s / 229745.2057 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0047
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 1928.7,                last time consumption/overall running time: 752.9012s / 230498.1069 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 1600.95,                last time consumption/overall running time: 620.8215s / 231118.9285 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0055
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 1652.3,                last time consumption/overall running time: 644.9365s / 231763.8650 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0056
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 1931.75,                last time consumption/overall running time: 737.4767s / 232501.3417 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 1718.75,                last time consumption/overall running time: 648.1123s / 233149.4540 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0050
env0_second_0:                 episode reward: 2.0500,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 1788.1,                last time consumption/overall running time: 684.7890s / 233834.2430 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0055
env0_second_0:                 episode reward: 2.3500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 2018.85,                last time consumption/overall running time: 775.6871s / 234609.9300 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0052
env0_second_0:                 episode reward: 2.5000,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 2352.45,                last time consumption/overall running time: 922.5897s / 235532.5197 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0053
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 2044.3,                last time consumption/overall running time: 813.7357s / 236346.2554 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0047
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 1942.4,                last time consumption/overall running time: 763.0140s / 237109.2694 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0055
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 2436.2,                last time consumption/overall running time: 938.5954s / 238047.8648 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 1616.2,                last time consumption/overall running time: 621.3799s / 238669.2447 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0044
env0_second_0:                 episode reward: 2.8000,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 2003.5,                last time consumption/overall running time: 772.6942s / 239441.9389 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 1961.3,                last time consumption/overall running time: 767.8555s / 240209.7944 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 1799.95,                last time consumption/overall running time: 706.5976s / 240916.3920 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0047
env0_second_0:                 episode reward: 2.1500,                 loss: nan
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 1974.35,                last time consumption/overall running time: 775.0767s / 241691.4688 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0050
env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 1964.6,                last time consumption/overall running time: 767.1245s / 242458.5933 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0051
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 2325.15,                last time consumption/overall running time: 917.6921s / 243376.2853 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 2221.7,                last time consumption/overall running time: 875.4130s / 244251.6983 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 2378.75,                last time consumption/overall running time: 930.0287s / 245181.7271 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0046
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 1549.25,                last time consumption/overall running time: 601.2745s / 245783.0016 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0043
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 1798.95,                last time consumption/overall running time: 701.8680s / 246484.8695 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0054
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 1724.8,                last time consumption/overall running time: 669.2986s / 247154.1682 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0056
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 2100.1,                last time consumption/overall running time: 818.3338s / 247972.5020 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0055
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 1779.05,                last time consumption/overall running time: 699.5970s / 248672.0990 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0059
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 2053.45,                last time consumption/overall running time: 805.2267s / 249477.3256 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0059
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 1661.6,                last time consumption/overall running time: 652.2583s / 250129.5840 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0053
env0_second_0:                 episode reward: 2.4000,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 1810.8,                last time consumption/overall running time: 707.6535s / 250837.2375 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0059
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 1867.9,                last time consumption/overall running time: 740.3011s / 251577.5386 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0053
env0_second_0:                 episode reward: 2.8500,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 1962.05,                last time consumption/overall running time: 779.4852s / 252357.0237 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 1631.55,                last time consumption/overall running time: 641.1427s / 252998.1664 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0051
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 1472.65,                last time consumption/overall running time: 576.0656s / 253574.2320 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0055
env0_second_0:                 episode reward: 2.4000,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1962.2,                last time consumption/overall running time: 766.5614s / 254340.7934 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0053
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 1978.85,                last time consumption/overall running time: 769.9239s / 255110.7173 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 2291.9,                last time consumption/overall running time: 896.7066s / 256007.4239 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0048
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 1609.15,                last time consumption/overall running time: 626.9543s / 256634.3782 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0045
env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 2283.0,                last time consumption/overall running time: 852.0654s / 257486.4436 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 1916.6,                last time consumption/overall running time: 691.8421s / 258178.2858 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0048
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 2020.35,                last time consumption/overall running time: 689.8548s / 258868.1406 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0047
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 1429.5,                last time consumption/overall running time: 490.4068s / 259358.5474 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0049
env0_second_0:                 episode reward: 3.2500,                 loss: nan
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 1919.45,                last time consumption/overall running time: 656.4100s / 260014.9574 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0057
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 1869.55,                last time consumption/overall running time: 632.4868s / 260647.4441 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 1768.2,                last time consumption/overall running time: 594.5256s / 261241.9697 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0051
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 1829.8,                last time consumption/overall running time: 618.9648s / 261860.9344 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 2008.5,                last time consumption/overall running time: 680.4900s / 262541.4245 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0057
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 2292.45,                last time consumption/overall running time: 771.8444s / 263313.2689 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0050
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 1824.65,                last time consumption/overall running time: 609.9136s / 263923.1825 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0050
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 1954.9,                last time consumption/overall running time: 656.1605s / 264579.3430 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0049
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 2000.65,                last time consumption/overall running time: 669.5430s / 265248.8860 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 1478.65,                last time consumption/overall running time: 498.5363s / 265747.4223 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0053
env0_second_0:                 episode reward: 2.2500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 2170.65,                last time consumption/overall running time: 728.2711s / 266475.6933 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0055
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 1758.05,                last time consumption/overall running time: 588.0192s / 267063.7125 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 2223.75,                last time consumption/overall running time: 742.6781s / 267806.3907 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 2099.3,                last time consumption/overall running time: 708.3847s / 268514.7754 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0047
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 1955.05,                last time consumption/overall running time: 667.3082s / 269182.0836 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0050
env0_second_0:                 episode reward: 2.2500,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 1764.6,                last time consumption/overall running time: 605.4256s / 269787.5092 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 1803.7,                last time consumption/overall running time: 620.7341s / 270408.2433 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0056
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 2259.9,                last time consumption/overall running time: 768.3069s / 271176.5502 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0053
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 2189.05,                last time consumption/overall running time: 740.4767s / 271917.0270 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0046
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 1696.45,                last time consumption/overall running time: 573.2853s / 272490.3123 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0047
env0_second_0:                 episode reward: 2.3500,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 1552.65,                last time consumption/overall running time: 521.5611s / 273011.8734 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0057
env0_second_0:                 episode reward: 2.6000,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 1982.95,                last time consumption/overall running time: 662.3474s / 273674.2207 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0056
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 2473.2,                last time consumption/overall running time: 833.8856s / 274508.1063 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0046
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 2231.5,                last time consumption/overall running time: 757.6348s / 275265.7411 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0048
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 2062.6,                last time consumption/overall running time: 702.0001s / 275967.7412 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0050
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 2148.7,                last time consumption/overall running time: 725.2780s / 276693.0191 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 1645.2,                last time consumption/overall running time: 560.5799s / 277253.5990 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0050
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 2029.0,                last time consumption/overall running time: 691.1214s / 277944.7204 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0053
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 1748.2,                last time consumption/overall running time: 588.0759s / 278532.7963 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0051
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 1994.25,                last time consumption/overall running time: 668.9965s / 279201.7928 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0053
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 1563.5,                last time consumption/overall running time: 529.3642s / 279731.1570 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0053
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 2231.0,                last time consumption/overall running time: 754.2451s / 280485.4021 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0053
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 1771.25,                last time consumption/overall running time: 597.7398s / 281083.1420 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0050
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 1796.85,                last time consumption/overall running time: 605.1689s / 281688.3109 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0052
env0_second_0:                 episode reward: 2.5000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 2312.8,                last time consumption/overall running time: 783.7376s / 282472.0485 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 1931.55,                last time consumption/overall running time: 648.9085s / 283120.9569 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0046
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 2087.9,                last time consumption/overall running time: 700.5261s / 283821.4830 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0050
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 1752.7,                last time consumption/overall running time: 591.9033s / 284413.3863 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0051
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 2052.7,                last time consumption/overall running time: 702.5255s / 285115.9118 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 1590.9,                last time consumption/overall running time: 543.3382s / 285659.2501 sLoad SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr = np.asanyarray(arr)

env0_first_0:                 episode reward: -2.5500,                 loss: 0.0047
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 1846.25,                last time consumption/overall running time: 624.9065s / 286284.1565 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0054
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 1953.0,                last time consumption/overall running time: 656.3772s / 286940.5337 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0051
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 1819.0,                last time consumption/overall running time: 605.3109s / 287545.8446 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0054
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 1788.1,                last time consumption/overall running time: 595.3497s / 288141.1943 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0053
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 2026.8,                last time consumption/overall running time: 666.8930s / 288808.0873 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0052
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 1971.9,                last time consumption/overall running time: 654.8123s / 289462.8996 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0050
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 1649.4,                last time consumption/overall running time: 556.8529s / 290019.7525 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0052
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 1945.45,                last time consumption/overall running time: 662.8595s / 290682.6119 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0050
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 1903.25,                last time consumption/overall running time: 647.7438s / 291330.3557 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0051
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 1788.4,                last time consumption/overall running time: 606.6154s / 291936.9712 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0050
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 1901.05,                last time consumption/overall running time: 641.6476s / 292578.6187 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0048
env0_second_0:                 episode reward: 2.5000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
