pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [8, 98]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220115_0159/pettingzoo_tennis_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220115_0159/pettingzoo_tennis_v2_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 78.7437s / 78.7437 s
env0_first_0:                 episode reward: -3.0000,                 loss: -0.0154
env0_second_0:                 episode reward: 3.0000,                 loss: -0.0178
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 9585.8,                last time consumption/overall running time: 1471.9687s / 1550.7124 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0437
env0_second_0:                 episode reward: -2.8500,                 loss: -0.0471
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 9309.65,                last time consumption/overall running time: 1423.6434s / 2974.3558 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.0596
env0_second_0:                 episode reward: -3.5500,                 loss: -0.0576
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1530.9088s / 4505.2646 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.0714
env0_second_0:                 episode reward: -4.1000,                 loss: -0.0721
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 9634.2,                last time consumption/overall running time: 1466.3494s / 5971.6140 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0782
env0_second_0:                 episode reward: 1.2500,                 loss: -0.0667
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1533.1083s / 7504.7223 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.0834
env0_second_0:                 episode reward: -3.0000,                 loss: -0.0784
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 9593.2,                last time consumption/overall running time: 1480.9546s / 8985.6769 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.0984
env0_second_0:                 episode reward: -3.5500,                 loss: -0.0915
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 9809.6,                last time consumption/overall running time: 1503.9142s / 10489.5911 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0841
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0785
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1535.6441s / 12025.2352 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.0968
env0_second_0:                 episode reward: 2.2000,                 loss: -0.0959
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 9411.75,                last time consumption/overall running time: 1436.2237s / 13461.4589 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1028
env0_second_0:                 episode reward: -5.0500,                 loss: -0.1052
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 9024.8,                last time consumption/overall running time: 1384.2291s / 14845.6880 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1064
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1151
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 9662.85,                last time consumption/overall running time: 1477.6466s / 16323.3346 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1249
env0_second_0:                 episode reward: -1.3500,                 loss: -0.1316
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 8708.75,                last time consumption/overall running time: 1325.2688s / 17648.6034 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1338
env0_second_0:                 episode reward: 0.6500,                 loss: -0.1444
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 9635.25,                last time consumption/overall running time: 1484.5864s / 19133.1898 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1457
env0_second_0:                 episode reward: -3.7500,                 loss: -0.1595
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 8725.05,                last time consumption/overall running time: 1338.7028s / 20471.8926 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1411
env0_second_0:                 episode reward: -1.6000,                 loss: -0.1532
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 9396.35,                last time consumption/overall running time: 1435.9840s / 21907.8766 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1627
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1714
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 9617.75,                last time consumption/overall running time: 1477.3348s / 23385.2113 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1627
env0_second_0:                 episode reward: -1.1500,                 loss: -0.1710
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 9903.2,                last time consumption/overall running time: 1527.7533s / 24912.9646 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1646
env0_second_0:                 episode reward: 2.1500,                 loss: -0.1766
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 8971.95,                last time consumption/overall running time: 1388.1664s / 26301.1310 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1641
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1758
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 8876.45,                last time consumption/overall running time: 1364.3125s / 27665.4435 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1646
env0_second_0:                 episode reward: -1.1500,                 loss: -0.1784
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 9402.75,                last time consumption/overall running time: 1440.7452s / 29106.1888 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.1709
env0_second_0:                 episode reward: 4.6500,                 loss: -0.1887
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 9773.25,                last time consumption/overall running time: 1505.0107s / 30611.1994 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.1876
env0_second_0:                 episode reward: -6.4500,                 loss: -0.1998
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 9611.45,                last time consumption/overall running time: 1471.6487s / 32082.8481 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1803
env0_second_0:                 episode reward: -2.0000,                 loss: -0.1969
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 9270.6,                last time consumption/overall running time: 1432.9255s / 33515.7736 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1834
env0_second_0:                 episode reward: -4.5500,                 loss: -0.1942
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 9648.45,                last time consumption/overall running time: 1488.8436s / 35004.6173 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1919
env0_second_0:                 episode reward: -2.8000,                 loss: -0.2038
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 9078.05,                last time consumption/overall running time: 1267.0804s / 36271.6976 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1986
env0_second_0:                 episode reward: -1.2500,                 loss: -0.2066
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 8553.3,                last time consumption/overall running time: 1162.3295s / 37434.0271 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1864
env0_second_0:                 episode reward: -5.1000,                 loss: -0.1943
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 9947.35,                last time consumption/overall running time: 1366.1353s / 38800.1625 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1969
env0_second_0:                 episode reward: -2.7000,                 loss: -0.2112
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1390.0115s / 40190.1739 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.2064
env0_second_0:                 episode reward: -4.8000,                 loss: -0.2171
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 9029.7,                last time consumption/overall running time: 1242.9434s / 41433.1173 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1986
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2073
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 9381.8,                last time consumption/overall running time: 1295.3694s / 42728.4867 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2114
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2178
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 9777.3,                last time consumption/overall running time: 1346.6387s / 44075.1253 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.2067
env0_second_0:                 episode reward: -3.4000,                 loss: -0.2179
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 9626.05,                last time consumption/overall running time: 1313.5726s / 45388.6980 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2246
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2333
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 9278.15,                last time consumption/overall running time: 1283.9285s / 46672.6264 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2000
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2056
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 8550.85,                last time consumption/overall running time: 1164.3493s / 47836.9758 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.2066
env0_second_0:                 episode reward: -4.6000,                 loss: -0.2156
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 9235.1,                last time consumption/overall running time: 1263.6305s / 49100.6062 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2216
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2296
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 8372.4,                last time consumption/overall running time: 1145.6553s / 50246.2615 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2024
env0_second_0:                 episode reward: -0.8500,                 loss: -0.2105
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 9785.5,                last time consumption/overall running time: 1341.4818s / 51587.7433 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2189
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2230
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 8114.45,                last time consumption/overall running time: 1122.2188s / 52709.9621 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2126
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2218
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 9632.05,                last time consumption/overall running time: 1315.4268s / 54025.3888 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2235
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2297
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 9543.7,                last time consumption/overall running time: 1301.9195s / 55327.3083 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2176
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2299
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 9953.25,                last time consumption/overall running time: 1362.6654s / 56689.9737 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2311
env0_second_0:                 episode reward: -2.6500,                 loss: -0.2398
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 9685.0,                last time consumption/overall running time: 1324.7863s / 58014.7600 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.2348
env0_second_0:                 episode reward: 3.3000,                 loss: -0.2449
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 8896.5,                last time consumption/overall running time: 1223.1278s / 59237.8878 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2147
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2242
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 8356.15,                last time consumption/overall running time: 1149.7623s / 60387.6502 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2171
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2284
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 9061.45,                last time consumption/overall running time: 1240.0500s / 61627.7002 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2233
env0_second_0:                 episode reward: -1.7500,                 loss: -0.2334
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 9384.35,                last time consumption/overall running time: 1275.6085s / 62903.3087 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2287
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2426
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 8910.75,                last time consumption/overall running time: 1238.2174s / 64141.5261 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2215
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2333
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 9612.15,                last time consumption/overall running time: 1339.2247s / 65480.7507 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.2336
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2429
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 8927.05,                last time consumption/overall running time: 1240.0578s / 66720.8085 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2222
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2334
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 8656.55,                last time consumption/overall running time: 1210.0655s / 67930.8740 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2273
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2344
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 9464.45,                last time consumption/overall running time: 1315.2356s / 69246.1096 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.2318
env0_second_0:                 episode reward: -1.4000,                 loss: -0.2412
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 8908.35,                last time consumption/overall running time: 1242.1656s / 70488.2752 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2591
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2661
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 9044.5,                last time consumption/overall running time: 1250.7030s / 71738.9783 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.2392
env0_second_0:                 episode reward: -3.4000,                 loss: -0.2443
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 9619.7,                last time consumption/overall running time: 1312.9550s / 73051.9333 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2366
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2486
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 9554.3,                last time consumption/overall running time: 1306.1912s / 74358.1245 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.2456
env0_second_0:                 episode reward: -2.9000,                 loss: -0.2509
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 9184.2,                last time consumption/overall running time: 1251.7765s / 75609.9009 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2352
env0_second_0:                 episode reward: -1.3000,                 loss: -0.2408
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 9319.15,                last time consumption/overall running time: 1287.5242s / 76897.4252 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2421
env0_second_0:                 episode reward: -0.5000,                 loss: -0.2465
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 9290.15,                last time consumption/overall running time: 1269.9124s / 78167.3375 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.2516
env0_second_0:                 episode reward: -0.8000,                 loss: -0.2592
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 9795.85,                last time consumption/overall running time: 1342.4276s / 79509.7651 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2491
env0_second_0:                 episode reward: -2.6500,                 loss: -0.2547
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 9254.8,                last time consumption/overall running time: 1269.2970s / 80779.0621 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.2511
env0_second_0:                 episode reward: -3.7000,                 loss: -0.2539
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 9626.9,                last time consumption/overall running time: 1333.5376s / 82112.5998 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.2457
env0_second_0:                 episode reward: -4.2000,                 loss: -0.2524
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 8313.55,                last time consumption/overall running time: 1161.5268s / 83274.1265 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2362
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2386
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 9179.05,                last time consumption/overall running time: 1241.7900s / 84515.9165 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2488
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2590
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 9607.7,                last time consumption/overall running time: 1322.6996s / 85838.6160 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.2630
env0_second_0:                 episode reward: -4.6500,                 loss: -0.2636
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 9565.9,                last time consumption/overall running time: 1314.4010s / 87153.0170 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2477
env0_second_0:                 episode reward: -2.7000,                 loss: -0.2522
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 8660.8,                last time consumption/overall running time: 1149.3849s / 88302.4020 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2453
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2500
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 9630.1,                last time consumption/overall running time: 1264.1775s / 89566.5794 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2442
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2543
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1286.6146s / 90853.1940 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.2612
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2680
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 9548.55,                last time consumption/overall running time: 1215.5423s / 92068.7363 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2732
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2793
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 9849.1,                last time consumption/overall running time: 1233.8286s / 93302.5649 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.2726
env0_second_0:                 episode reward: -7.6500,                 loss: -0.2742
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 8280.75,                last time consumption/overall running time: 1060.9366s / 94363.5015 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2662
env0_second_0:                 episode reward: -2.0000,                 loss: -0.2656
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 8675.4,                last time consumption/overall running time: 1103.2720s / 95466.7735 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2646
env0_second_0:                 episode reward: -2.6500,                 loss: -0.2693
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 9618.75,                last time consumption/overall running time: 1222.6137s / 96689.3873 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.2672
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2669
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 9015.75,                last time consumption/overall running time: 1149.3587s / 97838.7460 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2733
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2741
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 8725.35,                last time consumption/overall running time: 1117.4555s / 98956.2015 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.2547
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2599
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 9758.05,                last time consumption/overall running time: 1239.0449s / 100195.2464 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2727
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2757
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 9248.45,                last time consumption/overall running time: 1108.7128s / 101303.9591 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.2727
env0_second_0:                 episode reward: 0.4500,                 loss: -0.2716
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1173.1898s / 102477.1489 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.2839
env0_second_0:                 episode reward: -6.0500,                 loss: -0.2798
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 9234.7,                last time consumption/overall running time: 1105.7043s / 103582.8532 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.2698
env0_second_0:                 episode reward: 0.8500,                 loss: -0.2726
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 9255.7,                last time consumption/overall running time: 1072.5868s / 104655.4399 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2731
env0_second_0:                 episode reward: -4.3000,                 loss: -0.2725
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 9614.35,                last time consumption/overall running time: 1114.1109s / 105769.5508 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2814
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2822
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 8887.45,                last time consumption/overall running time: 1059.4542s / 106829.0051 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2698
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2728
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 8539.45,                last time consumption/overall running time: 997.7930s / 107826.7980 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.2875
env0_second_0:                 episode reward: -2.5000,                 loss: -0.2850
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 8640.35,                last time consumption/overall running time: 1016.0899s / 108842.8879 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.2816
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2795
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 9876.0,                last time consumption/overall running time: 1144.5839s / 109987.4718 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.2900
env0_second_0:                 episode reward: -4.2000,                 loss: -0.2853
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 8468.1,                last time consumption/overall running time: 995.9606s / 110983.4324 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2725
env0_second_0:                 episode reward: -1.7500,                 loss: -0.2682
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 9093.9,                last time consumption/overall running time: 1076.9560s / 112060.3884 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2908
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2895
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 9332.65,                last time consumption/overall running time: 1122.8920s / 113183.2805 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.2714
env0_second_0:                 episode reward: 0.7500,                 loss: -0.2713
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 9424.0,                last time consumption/overall running time: 1108.2695s / 114291.5500 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.2849
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2841
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 9524.6,                last time consumption/overall running time: 1126.1615s / 115417.7115 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2980
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2928
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 8414.0,                last time consumption/overall running time: 971.5447s / 116389.2562 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.2745
env0_second_0:                 episode reward: 2.7500,                 loss: -0.2719
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 9223.35,                last time consumption/overall running time: 1070.8562s / 117460.1124 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2825
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2778
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 9413.6,                last time consumption/overall running time: 1098.2227s / 118558.3351 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3075
env0_second_0:                 episode reward: -2.2500,                 loss: -0.3021
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 9418.9,                last time consumption/overall running time: 1081.9268s / 119640.2619 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3059
env0_second_0:                 episode reward: -2.1500,                 loss: -0.3014
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 9634.4,                last time consumption/overall running time: 1139.9453s / 120780.2072 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2976
env0_second_0:                 episode reward: -2.1000,                 loss: -0.2941
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 9270.5,                last time consumption/overall running time: 1101.9162s / 121882.1234 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.2902
env0_second_0:                 episode reward: 2.0000,                 loss: -0.2846
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 9784.55,                last time consumption/overall running time: 1124.3485s / 123006.4719 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.3042
env0_second_0:                 episode reward: 1.4000,                 loss: -0.2946
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 8848.35,                last time consumption/overall running time: 1034.9065s / 124041.3784 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2902
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2853
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 9265.0,                last time consumption/overall running time: 1124.2664s / 125165.6448 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.2905
env0_second_0:                 episode reward: 4.9500,                 loss: -0.2810
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 9630.6,                last time consumption/overall running time: 1133.0216s / 126298.6664 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3160
env0_second_0:                 episode reward: -1.1500,                 loss: -0.3144
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 8857.7,                last time consumption/overall running time: 1047.4075s / 127346.0739 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.3012
env0_second_0:                 episode reward: -2.5000,                 loss: -0.2928
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1179.3488s / 128525.4227 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.3124
env0_second_0:                 episode reward: -3.4000,                 loss: -0.3043
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 9557.3,                last time consumption/overall running time: 1088.7593s / 129614.1819 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3055
env0_second_0:                 episode reward: -1.1000,                 loss: -0.3017
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 9940.35,                last time consumption/overall running time: 1080.3143s / 130694.4962 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3198
env0_second_0:                 episode reward: -1.9000,                 loss: -0.3128
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 9506.3,                last time consumption/overall running time: 1051.8545s / 131746.3507 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.3137
env0_second_0:                 episode reward: -2.6000,                 loss: -0.3039
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 9030.1,                last time consumption/overall running time: 1003.9573s / 132750.3080 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3108
env0_second_0:                 episode reward: -1.9000,                 loss: -0.3040
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 9404.3,                last time consumption/overall running time: 1035.6153s / 133785.9233 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3184
env0_second_0:                 episode reward: -0.1000,                 loss: -0.3132
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1041.3291s / 134827.2524 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3302
env0_second_0:                 episode reward: -3.5500,                 loss: -0.3247
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 9532.55,                last time consumption/overall running time: 1061.2799s / 135888.5324 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.3134
env0_second_0:                 episode reward: -4.3000,                 loss: -0.3081
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 9390.25,                last time consumption/overall running time: 1031.7391s / 136920.2715 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.3079
env0_second_0:                 episode reward: 3.6500,                 loss: -0.3039
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 9450.95,                last time consumption/overall running time: 1065.4947s / 137985.7663 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3106
env0_second_0:                 episode reward: 0.8500,                 loss: -0.3072
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 8512.65,                last time consumption/overall running time: 933.8258s / 138919.5920 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3144
env0_second_0:                 episode reward: -1.4500,                 loss: -0.3093
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 9277.8,                last time consumption/overall running time: 1034.1899s / 139953.7820 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3147
env0_second_0:                 episode reward: -0.5500,                 loss: -0.3112
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 9293.05,                last time consumption/overall running time: 1065.3475s / 141019.1295 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.3124
env0_second_0:                 episode reward: 1.1000,                 loss: -0.3160
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 9247.95,                last time consumption/overall running time: 1012.1995s / 142031.3289 s
env0_first_0:                 episode reward: 7.9000,                 loss: -0.3117
env0_second_0:                 episode reward: -7.9000,                 loss: -0.3088
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 9146.85,                last time consumption/overall running time: 1011.7990s / 143043.1280 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3200
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3127
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 9202.1,                last time consumption/overall running time: 1000.5793s / 144043.7072 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3301
env0_second_0:                 episode reward: -1.0500,                 loss: -0.3235
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 9937.0,                last time consumption/overall running time: 1142.6760s / 145186.3832 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.3334
env0_second_0:                 episode reward: -4.6500,                 loss: -0.3266
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 8313.95,                last time consumption/overall running time: 994.3376s / 146180.7209 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3101
env0_second_0:                 episode reward: -2.4000,                 loss: -0.3044
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 9615.35,                last time consumption/overall running time: 1141.2572s / 147321.9781 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.3300
env0_second_0:                 episode reward: -3.9500,                 loss: -0.3217
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 9217.0,                last time consumption/overall running time: 1094.2472s / 148416.2253 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3135
env0_second_0:                 episode reward: -0.3000,                 loss: -0.3061
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 8892.55,                last time consumption/overall running time: 1067.1865s / 149483.4118 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.3082
env0_second_0:                 episode reward: -3.6500,                 loss: -0.3082
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 9598.15,                last time consumption/overall running time: 1143.4329s / 150626.8447 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.3255
env0_second_0:                 episode reward: -9.1500,                 loss: -0.3251
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 9397.5,                last time consumption/overall running time: 1096.2232s / 151723.0679 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.3383
env0_second_0:                 episode reward: -3.2500,                 loss: -0.3352
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 8121.25,                last time consumption/overall running time: 930.5864s / 152653.6544 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3161
env0_second_0:                 episode reward: -0.9500,                 loss: -0.3195
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 9033.15,                last time consumption/overall running time: 1079.3146s / 153732.9689 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.3214
env0_second_0:                 episode reward: -3.6000,                 loss: -0.3205
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 9821.05,                last time consumption/overall running time: 1182.1046s / 154915.0735 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.3367
env0_second_0:                 episode reward: 2.9000,                 loss: -0.3311
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 9612.4,                last time consumption/overall running time: 1144.4528s / 156059.5263 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.3312
env0_second_0:                 episode reward: 1.0500,                 loss: -0.3324
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 9374.5,                last time consumption/overall running time: 1072.2352s / 157131.7615 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.3284
env0_second_0:                 episode reward: -2.6000,                 loss: -0.3274
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 9279.35,                last time consumption/overall running time: 1085.9568s / 158217.7183 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.3309
env0_second_0:                 episode reward: -4.4500,                 loss: -0.3243
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 9229.65,                last time consumption/overall running time: 1081.8094s / 159299.5277 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.3287
env0_second_0:                 episode reward: 0.4500,                 loss: -0.3254
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 8590.05,                last time consumption/overall running time: 1006.6738s / 160306.2015 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3222
env0_second_0:                 episode reward: -1.1500,                 loss: -0.3263
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 9401.6,                last time consumption/overall running time: 1132.1376s / 161438.3391 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3506
env0_second_0:                 episode reward: -2.5500,                 loss: -0.3481
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 9637.6,                last time consumption/overall running time: 1148.5225s / 162586.8616 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.3531
env0_second_0:                 episode reward: -3.0000,                 loss: -0.3499
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 8863.65,                last time consumption/overall running time: 1080.1356s / 163666.9972 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.3298
env0_second_0:                 episode reward: -3.8000,                 loss: -0.3262
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 8824.0,                last time consumption/overall running time: 1056.6631s / 164723.6603 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.3334
env0_second_0:                 episode reward: 0.9500,                 loss: -0.3333
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 9261.8,                last time consumption/overall running time: 1101.8974s / 165825.5577 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3392
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3397
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 9687.1,                last time consumption/overall running time: 1170.2162s / 166995.7739 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3535
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3468
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 9339.25,                last time consumption/overall running time: 1122.1200s / 168117.8939 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3403
env0_second_0:                 episode reward: -0.6000,                 loss: -0.3406
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 8496.6,                last time consumption/overall running time: 1023.8313s / 169141.7252 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3217
env0_second_0:                 episode reward: -2.0000,                 loss: -0.3192
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 9798.65,                last time consumption/overall running time: 1180.0831s / 170321.8082 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3638
env0_second_0:                 episode reward: 0.6500,                 loss: -0.3604
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 9469.0,                last time consumption/overall running time: 1140.2250s / 171462.0333 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.3464
env0_second_0:                 episode reward: -3.8500,                 loss: -0.3454
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 8953.25,                last time consumption/overall running time: 1070.6296s / 172532.6629 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.3333
env0_second_0:                 episode reward: -2.6500,                 loss: -0.3357
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 8827.1,                last time consumption/overall running time: 1058.8556s / 173591.5185 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3419
env0_second_0:                 episode reward: -2.3000,                 loss: -0.3408
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 9618.75,                last time consumption/overall running time: 1145.5583s / 174737.0768 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.3282
env0_second_0:                 episode reward: -3.7000,                 loss: -0.3306
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 9617.3,                last time consumption/overall running time: 1137.5159s / 175874.5927 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.3456
env0_second_0:                 episode reward: -9.6500,                 loss: -0.3497
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 9351.95,                last time consumption/overall running time: 1128.0591s / 177002.6518 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.3638
env0_second_0:                 episode reward: -3.0500,                 loss: -0.3575
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 9170.15,                last time consumption/overall running time: 1087.7345s / 178090.3863 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.3561
env0_second_0:                 episode reward: -3.6500,                 loss: -0.3487
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 9611.4,                last time consumption/overall running time: 1098.8621s / 179189.2485 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.3538
env0_second_0:                 episode reward: -3.0500,                 loss: -0.3486
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 9299.25,                last time consumption/overall running time: 1105.5553s / 180294.8037 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3458
env0_second_0:                 episode reward: -0.1000,                 loss: -0.3493
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 8525.1,                last time consumption/overall running time: 1028.2338s / 181323.0375 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3405
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3487
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 9701.45,                last time consumption/overall running time: 1161.6158s / 182484.6533 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3536
env0_second_0:                 episode reward: -1.3000,                 loss: -0.3514
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 8886.6,                last time consumption/overall running time: 1065.1167s / 183549.7700 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3386
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3358
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 9573.4,                last time consumption/overall running time: 1137.8027s / 184687.5728 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.3394
env0_second_0:                 episode reward: 3.7500,                 loss: -0.3390
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 9022.95,                last time consumption/overall running time: 1073.2143s / 185760.7871 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3655
env0_second_0:                 episode reward: -1.0500,                 loss: -0.3666
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 9608.8,                last time consumption/overall running time: 1129.5603s / 186890.3474 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.3520
env0_second_0:                 episode reward: 1.7000,                 loss: -0.3564
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 9216.9,                last time consumption/overall running time: 1105.1883s / 187995.5357 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3426
env0_second_0:                 episode reward: 1.3000,                 loss: -0.3445
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 9286.25,                last time consumption/overall running time: 1082.6122s / 189078.1479 s
env0_first_0:                 episode reward: 5.9500,                 loss: -0.3632
env0_second_0:                 episode reward: -5.9500,                 loss: -0.3642
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 9923.5,                last time consumption/overall running time: 1183.6560s / 190261.8039 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3770
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3725
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 9245.0,                last time consumption/overall running time: 1088.5389s / 191350.3428 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3569
env0_second_0:                 episode reward: -2.2500,                 loss: -0.3564
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 9389.85,                last time consumption/overall running time: 1102.0138s / 192452.3566 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.3691
env0_second_0:                 episode reward: -5.2000,                 loss: -0.3684
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 9618.25,                last time consumption/overall running time: 1157.7279s / 193610.0845 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.3578
env0_second_0:                 episode reward: -7.1000,                 loss: -0.3542
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 9798.15,                last time consumption/overall running time: 1164.3976s / 194774.4820 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.3705
env0_second_0:                 episode reward: 2.7000,                 loss: -0.3727
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 9510.65,                last time consumption/overall running time: 1122.5792s / 195897.0613 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.3626
env0_second_0:                 episode reward: -4.9000,                 loss: -0.3578
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 9289.65,                last time consumption/overall running time: 1092.4542s / 196989.5154 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3640
env0_second_0:                 episode reward: -1.1000,                 loss: -0.3684
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 9385.6,                last time consumption/overall running time: 1108.3165s / 198097.8319 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.3673
env0_second_0:                 episode reward: -4.6000,                 loss: -0.3684
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1194.5601s / 199292.3920 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.3691
env0_second_0:                 episode reward: -4.0500,                 loss: -0.3729
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 8937.0,                last time consumption/overall running time: 1048.9381s / 200341.3301 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3552
env0_second_0:                 episode reward: 0.0000,                 loss: -0.3425
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 8166.85,                last time consumption/overall running time: 980.1049s / 201321.4351 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.3551
env0_second_0:                 episode reward: -6.5500,                 loss: -0.3596
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 7643.65,                last time consumption/overall running time: 915.8790s / 202237.3141 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3482
env0_second_0:                 episode reward: 1.3500,                 loss: -0.3442
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 8894.1,                last time consumption/overall running time: 1065.8641s / 203303.1782 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.3574
env0_second_0:                 episode reward: 2.3500,                 loss: -0.3607
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 8794.5,                last time consumption/overall running time: 1050.4768s / 204353.6550 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3665
env0_second_0:                 episode reward: -1.3000,                 loss: -0.3762
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 9659.55,                last time consumption/overall running time: 1143.6327s / 205497.2877 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3761
env0_second_0:                 episode reward: -0.5500,                 loss: -0.3734
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 9631.45,                last time consumption/overall running time: 1163.9771s / 206661.2648 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.3581
env0_second_0:                 episode reward: -2.3500,                 loss: -0.3554
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 9805.15,                last time consumption/overall running time: 1175.4280s / 207836.6928 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3808
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3789
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 9287.6,                last time consumption/overall running time: 1087.5068s / 208924.1996 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.3630
env0_second_0:                 episode reward: -5.6000,                 loss: -0.3702
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 9591.4,                last time consumption/overall running time: 1056.1398s / 209980.3394 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3806
env0_second_0:                 episode reward: -0.0500,                 loss: -0.3840
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 9216.75,                last time consumption/overall running time: 1016.7247s / 210997.0641 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.3671
env0_second_0:                 episode reward: -4.6000,                 loss: -0.3683
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 9163.55,                last time consumption/overall running time: 1012.7663s / 212009.8304 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.3527
env0_second_0:                 episode reward: -4.6000,                 loss: -0.3555
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 8903.6,                last time consumption/overall running time: 978.6153s / 212988.4457 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.3580
env0_second_0:                 episode reward: 0.7500,                 loss: -0.3619
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1116.8801s / 214105.3258 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3753
env0_second_0:                 episode reward: -1.7500,                 loss: -0.3764
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 8729.3,                last time consumption/overall running time: 977.8758s / 215083.2016 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.3437
env0_second_0:                 episode reward: -5.7000,                 loss: -0.3522
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 9395.6,                last time consumption/overall running time: 1029.8609s / 216113.0624 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.3853
env0_second_0:                 episode reward: -1.9500,                 loss: -0.3864
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 9039.75,                last time consumption/overall running time: 1046.0893s / 217159.1517 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3670
env0_second_0:                 episode reward: 0.6000,                 loss: -0.3593
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 9715.2,                last time consumption/overall running time: 1071.5698s / 218230.7215 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3862
env0_second_0:                 episode reward: -2.8500,                 loss: -0.3909
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 9586.8,                last time consumption/overall running time: 1118.1129s / 219348.8345 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.3812
env0_second_0:                 episode reward: 1.7000,                 loss: -0.3790
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 9524.75,                last time consumption/overall running time: 1144.9055s / 220493.7400 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.3793
env0_second_0:                 episode reward: 2.7000,                 loss: -0.3835
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 8955.8,                last time consumption/overall running time: 1095.4742s / 221589.2142 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.3620
env0_second_0:                 episode reward: 1.7500,                 loss: -0.3738
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 8910.0,                last time consumption/overall running time: 1101.6670s / 222690.8812 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3704
env0_second_0:                 episode reward: -0.0500,                 loss: -0.3746
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 9283.3,                last time consumption/overall running time: 1128.2208s / 223819.1020 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.3753
env0_second_0:                 episode reward: 0.9000,                 loss: -0.3786
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 9443.4,                last time consumption/overall running time: 1140.8049s / 224959.9070 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3728
env0_second_0:                 episode reward: -1.0500,                 loss: -0.3797
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 9021.95,                last time consumption/overall running time: 1102.7617s / 226062.6687 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3772
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3832
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 9107.05,                last time consumption/overall running time: 1102.7735s / 227165.4422 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3786
env0_second_0:                 episode reward: -1.4500,                 loss: -0.3822
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 9501.2,                last time consumption/overall running time: 1166.5788s / 228332.0210 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.3851
env0_second_0:                 episode reward: 0.8000,                 loss: -0.3948
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 9312.75,                last time consumption/overall running time: 1126.2182s / 229458.2392 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3754
env0_second_0:                 episode reward: -1.1500,                 loss: -0.3821
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 8509.0,                last time consumption/overall running time: 1006.8261s / 230465.0653 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.3691
env0_second_0:                 episode reward: -4.7000,                 loss: -0.3743
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 8943.7,                last time consumption/overall running time: 1068.4778s / 231533.5431 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3745
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3786
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 9724.8,                last time consumption/overall running time: 1206.8347s / 232740.3778 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.3805
env0_second_0:                 episode reward: 3.8500,                 loss: -0.3854
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 9562.75,                last time consumption/overall running time: 1186.4774s / 233926.8553 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.3983
env0_second_0:                 episode reward: -4.0000,                 loss: -0.3983
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 9946.8,                last time consumption/overall running time: 1237.3173s / 235164.1726 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3679
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3722
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 9280.4,                last time consumption/overall running time: 1151.9127s / 236316.0853 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.3801
env0_second_0:                 episode reward: -4.6000,                 loss: -0.3887
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 8087.55,                last time consumption/overall running time: 1010.9621s / 237327.0474 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.3478
env0_second_0:                 episode reward: -3.8500,                 loss: -0.3617
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 9438.8,                last time consumption/overall running time: 1155.5740s / 238482.6214 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.3952
env0_second_0:                 episode reward: -6.7000,                 loss: -0.4003
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 9021.1,                last time consumption/overall running time: 1097.9713s / 239580.5927 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.3647
env0_second_0:                 episode reward: 2.7500,                 loss: -0.3706
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 8738.4,                last time consumption/overall running time: 1080.3818s / 240660.9745 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3767
env0_second_0:                 episode reward: -1.7500,                 loss: -0.3841
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 9784.6,                last time consumption/overall running time: 1209.5347s / 241870.5092 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.3892
env0_second_0:                 episode reward: 4.6000,                 loss: -0.3930
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 9293.65,                last time consumption/overall running time: 1138.6524s / 243009.1617 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.3852
env0_second_0:                 episode reward: -2.5000,                 loss: -0.3853
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 9242.1,                last time consumption/overall running time: 1135.1608s / 244144.3225 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.3748
env0_second_0:                 episode reward: 1.9000,                 loss: -0.3839
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 8942.75,                last time consumption/overall running time: 1099.6656s / 245243.9881 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3703
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3803
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1221.8675s / 246465.8557 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.3915
env0_second_0:                 episode reward: -0.8500,                 loss: -0.3995
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1241.0203s / 247706.8760 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.3884
env0_second_0:                 episode reward: 0.7500,                 loss: -0.3930
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 9806.9,                last time consumption/overall running time: 1198.7992s / 248905.6752 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3910
env0_second_0:                 episode reward: -2.4000,                 loss: -0.3926
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 9381.5,                last time consumption/overall running time: 1162.7670s / 250068.4422 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3871
env0_second_0:                 episode reward: -0.3000,                 loss: -0.3473
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 9034.5,                last time consumption/overall running time: 1116.6051s / 251185.0472 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.3809
env0_second_0:                 episode reward: -4.3000,                 loss: -0.3861
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 9250.15,                last time consumption/overall running time: 1157.3168s / 252342.3640 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.3899
env0_second_0:                 episode reward: -4.4500,                 loss: -0.3881
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 9078.9,                last time consumption/overall running time: 1117.4097s / 253459.7738 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3748
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3685
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 8842.55,                last time consumption/overall running time: 1082.2718s / 254542.0455 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3852
env0_second_0:                 episode reward: 0.0000,                 loss: -0.3769
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 9350.6,                last time consumption/overall running time: 1129.5403s / 255671.5858 s
env0_first_0:                 episode reward: 8.3000,                 loss: -0.3793
env0_second_0:                 episode reward: -8.3000,                 loss: -0.3837
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 9607.65,                last time consumption/overall running time: 1025.8604s / 256697.4462 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.3957
env0_second_0:                 episode reward: -0.3500,                 loss: -0.4023
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 9052.4,                last time consumption/overall running time: 895.0798s / 257592.5260 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.3941
env0_second_0:                 episode reward: -6.5000,                 loss: -0.3980
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 8862.2,                last time consumption/overall running time: 886.0544s / 258478.5804 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3760
env0_second_0:                 episode reward: -1.2500,                 loss: -0.3830
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 9460.0,                last time consumption/overall running time: 942.9328s / 259421.5132 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.3786
env0_second_0:                 episode reward: -0.8000,                 loss: -0.3858
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 985.2351s / 260406.7482 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.3981
env0_second_0:                 episode reward: 2.7500,                 loss: -0.4057
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 8823.35,                last time consumption/overall running time: 874.2376s / 261280.9858 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3855
env0_second_0:                 episode reward: -2.8500,                 loss: -0.3838
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 9410.6,                last time consumption/overall running time: 911.9038s / 262192.8896 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3915
env0_second_0:                 episode reward: -1.3000,                 loss: -0.3933
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 9591.85,                last time consumption/overall running time: 947.7183s / 263140.6078 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.3992
env0_second_0:                 episode reward: -4.3000,                 loss: -0.4059
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 9612.5,                last time consumption/overall running time: 940.3039s / 264080.9117 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.3951
env0_second_0:                 episode reward: -5.4000,                 loss: -0.3968
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 9641.65,                last time consumption/overall running time: 947.0532s / 265027.9650 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.3907
env0_second_0:                 episode reward: -0.3500,                 loss: -0.4011
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 9165.7,                last time consumption/overall running time: 885.2886s / 265913.2535 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3785
env0_second_0:                 episode reward: 0.2000,                 loss: -0.3931
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 9055.95,                last time consumption/overall running time: 892.9496s / 266806.2032 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.3879
env0_second_0:                 episode reward: -3.9000,                 loss: -0.3942
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 9006.15,                last time consumption/overall running time: 1101.9560s / 267908.1591 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.3838
env0_second_0:                 episode reward: 1.2000,                 loss: -0.3902
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1385.9351s / 269294.0943 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.3995
env0_second_0:                 episode reward: -4.4000,                 loss: -0.4056
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 9585.05,                last time consumption/overall running time: 1321.8113s / 270615.9056 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.3842
env0_second_0:                 episode reward: -2.9000,                 loss: -0.3884
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 9005.55,                last time consumption/overall running time: 1231.6008s / 271847.5064 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.4058
env0_second_0:                 episode reward: -3.2500,                 loss: -0.4095
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 9206.3,                last time consumption/overall running time: 1237.2894s / 273084.7958 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.3873
env0_second_0:                 episode reward: -5.7500,                 loss: -0.3969
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 9661.65,                last time consumption/overall running time: 1303.6334s / 274388.4291 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.4045
env0_second_0:                 episode reward: -3.7000,                 loss: -0.4097
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 8657.15,                last time consumption/overall running time: 1176.7783s / 275565.2075 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3676
env0_second_0:                 episode reward: -0.3000,                 loss: -0.3799
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 9713.5,                last time consumption/overall running time: 1320.9517s / 276886.1592 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.4022
env0_second_0:                 episode reward: -3.0000,                 loss: -0.4055
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 8175.7,                last time consumption/overall running time: 1113.3124s / 277999.4716 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3686
env0_second_0:                 episode reward: -2.8500,                 loss: -0.3708
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 9774.9,                last time consumption/overall running time: 1321.2571s / 279320.7286 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3976
env0_second_0:                 episode reward: 1.3500,                 loss: -0.4077
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 9638.7,                last time consumption/overall running time: 1301.0683s / 280621.7969 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3904
env0_second_0:                 episode reward: -1.9000,                 loss: -0.4019
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 7973.25,                last time consumption/overall running time: 1078.8800s / 281700.6769 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.3737
env0_second_0:                 episode reward: -5.8000,                 loss: -0.3787
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 8906.65,                last time consumption/overall running time: 1205.3238s / 282906.0006 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3964
env0_second_0:                 episode reward: -1.8500,                 loss: -0.4022
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 9644.85,                last time consumption/overall running time: 1299.4697s / 284205.4703 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.4013
env0_second_0:                 episode reward: -1.1000,                 loss: -0.4021
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 8479.0,                last time consumption/overall running time: 1139.7426s / 285345.2129 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.3956
env0_second_0:                 episode reward: -2.3500,                 loss: -0.3647
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 9549.15,                last time consumption/overall running time: 1289.4214s / 286634.6344 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3849
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3896
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 8777.15,                last time consumption/overall running time: 1179.9731s / 287814.6075 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.3909
env0_second_0:                 episode reward: 2.5000,                 loss: -0.3930
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 9193.9,                last time consumption/overall running time: 1237.8637s / 289052.4712 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.3844
env0_second_0:                 episode reward: 2.3500,                 loss: -0.3895
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 8891.15,                last time consumption/overall running time: 1196.8274s / 290249.2986 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.3892
env0_second_0:                 episode reward: 1.8500,                 loss: -0.3947
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 9788.5,                last time consumption/overall running time: 1314.6910s / 291563.9896 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.4024
env0_second_0:                 episode reward: 2.1000,                 loss: -0.4059
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 8773.3,                last time consumption/overall running time: 1183.0611s / 292747.0507 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3838
env0_second_0:                 episode reward: -1.9000,                 loss: -0.3925
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 8958.7,                last time consumption/overall running time: 1201.7009s / 293948.7516 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.3840
env0_second_0:                 episode reward: -2.8000,                 loss: -0.3975
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 9407.9,                last time consumption/overall running time: 1263.8050s / 295212.5566 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3998
env0_second_0:                 episode reward: -1.8000,                 loss: -0.4087
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 9092.95,                last time consumption/overall running time: 1224.2452s / 296436.8018 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.4044
env0_second_0:                 episode reward: -2.5000,                 loss: -0.4152
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 8380.5,                last time consumption/overall running time: 1124.4239s / 297561.2257 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3867
env0_second_0:                 episode reward: -0.9000,                 loss: -0.3999
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 9247.25,                last time consumption/overall running time: 1242.9842s / 298804.2099 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3975
env0_second_0:                 episode reward: 1.3000,                 loss: -0.4035
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 9677.45,                last time consumption/overall running time: 1287.9050s / 300092.1149 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3949
env0_second_0:                 episode reward: -1.0500,                 loss: -0.4060
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 9794.9,                last time consumption/overall running time: 1307.1255s / 301399.2405 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.3938
env0_second_0:                 episode reward: -3.9000,                 loss: -0.3975
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 9252.9,                last time consumption/overall running time: 1235.0387s / 302634.2792 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.4042
env0_second_0:                 episode reward: -3.5000,                 loss: -0.3999
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 9508.25,                last time consumption/overall running time: 1276.1750s / 303910.4542 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.3893
env0_second_0:                 episode reward: -4.0500,                 loss: -0.3966
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 9587.35,                last time consumption/overall running time: 1273.0151s / 305183.4694 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3913
env0_second_0:                 episode reward: -0.7500,                 loss: -0.4043
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 9525.2,                last time consumption/overall running time: 1300.1186s / 306483.5879 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.3998
env0_second_0:                 episode reward: 3.8000,                 loss: -0.4076
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 8440.1,                last time consumption/overall running time: 1154.4784s / 307638.0663 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3762
env0_second_0:                 episode reward: -1.8500,                 loss: -0.3813
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1370.2942s / 309008.3605 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.4042
env0_second_0:                 episode reward: -4.7500,                 loss: -0.4097
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 9580.85,                last time consumption/overall running time: 1307.5366s / 310315.8970 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.4096
env0_second_0:                 episode reward: 0.0500,                 loss: -0.4128
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 9100.35,                last time consumption/overall running time: 1209.0161s / 311524.9132 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.3864
env0_second_0:                 episode reward: -5.7500,                 loss: -0.3959
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 9418.9,                last time consumption/overall running time: 1227.8436s / 312752.7568 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.4038
env0_second_0:                 episode reward: 0.8500,                 loss: -0.4119
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 9753.95,                last time consumption/overall running time: 1274.5685s / 314027.3253 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3986
env0_second_0:                 episode reward: 0.6500,                 loss: -0.4041
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 9600.05,                last time consumption/overall running time: 1304.4863s / 315331.8116 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.3780
env0_second_0:                 episode reward: 0.9500,                 loss: -0.3826
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 9973.8,                last time consumption/overall running time: 1364.3964s / 316696.2080 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.4008
env0_second_0:                 episode reward: -1.1000,                 loss: -0.4107
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1363.1121s / 318059.3201 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.3970
env0_second_0:                 episode reward: -3.7000,                 loss: -0.4038
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1363.7460s / 319423.0660 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.3921
env0_second_0:                 episode reward: -6.4000,                 loss: -0.3991
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 9124.5,                last time consumption/overall running time: 1239.6629s / 320662.7289 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.3818
env0_second_0:                 episode reward: -4.1000,                 loss: -0.4010
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 9157.1,                last time consumption/overall running time: 1241.4918s / 321904.2207 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3964
env0_second_0:                 episode reward: -0.0500,                 loss: -0.4114
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 9650.0,                last time consumption/overall running time: 1315.5221s / 323219.7428 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.4050
env0_second_0:                 episode reward: 0.4500,                 loss: -0.4160
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 8738.45,                last time consumption/overall running time: 1186.7108s / 324406.4536 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3826
env0_second_0:                 episode reward: -2.0000,                 loss: -0.3833
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 9764.95,                last time consumption/overall running time: 1320.2708s / 325726.7245 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.4083
env0_second_0:                 episode reward: -0.1000,                 loss: -0.4191
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 9621.6,                last time consumption/overall running time: 1255.4865s / 326982.2110 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.4155
env0_second_0:                 episode reward: 0.3500,                 loss: -0.4238
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 8843.7,                last time consumption/overall running time: 1171.5407s / 328153.7517 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.4022
env0_second_0:                 episode reward: -0.4500,                 loss: -0.4118
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 9665.2,                last time consumption/overall running time: 1284.9783s / 329438.7300 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.4023
env0_second_0:                 episode reward: 0.0000,                 loss: -0.4097
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 8366.2,                last time consumption/overall running time: 1072.4854s / 330511.2154 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.3841
env0_second_0:                 episode reward: 3.1000,                 loss: -0.4008
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 9628.55,                last time consumption/overall running time: 1252.7848s / 331764.0002 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.4021
env0_second_0:                 episode reward: -4.6000,                 loss: -0.4080
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 9809.45,                last time consumption/overall running time: 1273.4258s / 333037.4261 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.3892
env0_second_0:                 episode reward: 2.6000,                 loss: -0.3972
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 9227.95,                last time consumption/overall running time: 1181.2511s / 334218.6771 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.3955
env0_second_0:                 episode reward: 1.8000,                 loss: -0.4002
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 9594.95,                last time consumption/overall running time: 1221.8524s / 335440.5296 s