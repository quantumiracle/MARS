{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "num_states = 3\n",
    "num_actions_per_player = 2\n",
    "num_actions = num_actions_per_player**2\n",
    "num_trans = 3\n",
    "reward_range = [-1,1]\n",
    "\n",
    "def generate_random_trans_and_rewards():\n",
    "    trans_prob_matrices = []\n",
    "    reward_matrices = []\n",
    "    for _ in range(num_trans):\n",
    "        trans_prob_matrix = []\n",
    "        reward_matrix = []\n",
    "        for s in range(num_states):\n",
    "            trans_prob_matrix_for_s = []\n",
    "            reward_matrix_for_s = []\n",
    "            for a in range(num_actions):\n",
    "                rands = np.random.uniform(0,1, num_states)\n",
    "                rand_probs = list(rands/sum(rands))\n",
    "                trans_prob_matrix_for_s.append(rand_probs)\n",
    "                rs = np.random.uniform(*reward_range, num_states)\n",
    "                reward_matrix_for_s.append(list(rs))\n",
    "            trans_prob_matrix.append(trans_prob_matrix_for_s)\n",
    "            reward_matrix.append(reward_matrix_for_s)\n",
    "        trans_prob_matrices.append(trans_prob_matrix)\n",
    "        reward_matrices.append(reward_matrix)\n",
    "    return trans_prob_matrices, reward_matrices\n",
    "\n",
    "tpmxs, rmxs =  generate_random_trans_and_rewards()  # shape: (trans, state, action, next_state)\n",
    "print(np.array(tpmxs).shape, np.array(rmxs).shape, rmxs) # shape: (trans, state, action, next_state)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 3, 4, 3) (3, 3, 4, 3) [[[[0.21123743774355774, 0.1711753238597562, -0.17610521369565313], [-0.4467982890426656, 0.06548356678565703, -0.1651711687075217], [-0.037786825907280885, -0.6662661898215996, 0.41012368726779735], [-0.47343150573854764, 0.86270836668386, 0.9486577603738413]], [[-0.2876291306053398, -0.504835375531637, 0.09335899326261665], [-0.11356981306151481, 0.3960231516678929, 0.06363797880791311], [-0.7770320290713522, -0.27875838330354674, -0.4865296776499146], [0.294644435117732, -0.38371822115834275, 0.11812439744915082]], [[-0.3808343319214458, 0.18090467396156962, -0.5704275828534058], [0.782374497590993, 0.5581036449940866, 0.8048557868956976], [-0.8478519511468259, 0.6469293284244844, -0.45941974101210725], [-0.01790355624675577, -0.7286287645361322, 0.8207678544839876]]], [[[0.4245523326939702, 0.13727141303554635, -0.7295434104126808], [-0.59054614290371, 0.0769522669560021, 0.3228690622512722], [0.4325722039205977, 0.5922347700709569, 0.6382580108128053], [0.7497023589026395, 0.895388649157413, -0.1386989495139883]], [[-0.7958462788015772, -0.35610684583193275, -0.711940406370954], [-0.5505187351931073, 0.18731864809550025, 0.44368055851858546], [-0.049931434688332166, 0.15096708533468384, -0.7676378991025699], [0.4434508345202992, -0.16659184334729615, -0.7199622211693617]], [[0.692431053084364, 0.2580103487962069, 0.3116732581220183], [0.47602811415672264, 0.6726265467728509, -0.8717573747374163], [-0.293472442642577, -0.9567068448766578, -0.7849887389714985], [-0.21325285036920483, 0.5461929871635653, 0.8070352088116735]]], [[[-0.8262984052124625, -0.11559527139535564, -0.7100049787923819], [0.1382033874876154, 0.5075325778080575, -0.9081498214638939], [-0.23017672697018865, 0.15940622066963162, -0.668173299882185], [0.6443644512755464, 0.957319327650574, 0.8889774713862859]], [[0.9286064256037054, 0.4575759879401873, -0.25288519556519806], [0.30012165388234124, 0.5612906696660389, -0.11462722404931736], [-0.9708576793048349, -0.6968107609276577, 0.9931403892768762], [-0.7566792906070805, -0.7760024689114975, 0.8564378555783219]], [[0.6875319334273824, -0.312304349406209, -0.10563496378890869], [-0.9457930934077228, 0.47223113936233196, -0.539347666609427], [-0.528311951491109, 0.44438037503092054, -0.6872782010187195], [0.587903391292262, 0.8863753626921154, -0.31338916558583096]]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import ecos\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def NashEquilibriumECOSSolver(M):\n",
    "    \"\"\"\n",
    "    https://github.com/embotech/ecos-python\n",
    "    min  c*x\n",
    "    s.t. A*x = b\n",
    "         G*x <= h\n",
    "    https://github.com/embotech/ecos/wiki/Usage-from-MATLAB\n",
    "    args: \n",
    "        c,b,h: numpy.array\n",
    "        A, G: Scipy sparse matrix\n",
    "    \"\"\"\n",
    "    row, col = M.shape\n",
    "    c = np.zeros(row+1)\n",
    "    # max z\n",
    "    c[-1] = -1  \n",
    "    \n",
    "    # x1+x2+...+xn=1\n",
    "    A = np.ones(row+1)\n",
    "    A[-1] = 0.\n",
    "    A = csr_matrix([A])\n",
    "    b=np.array([1.])\n",
    "    \n",
    "    # M.T*x<=z\n",
    "    G1 = np.ones((col, row+1))\n",
    "    G1[:col, :row] = -1. * M.T\n",
    "    # x>=0\n",
    "    G2 = np.zeros((row, row+1))\n",
    "    for i in range(row):\n",
    "        G2[i, i]=-1. \n",
    "    # x<=1.\n",
    "    G3 = np.zeros((row, row+1))\n",
    "    for i in range(row):\n",
    "        G3[i, i]=1. \n",
    "    G = csr_matrix(np.concatenate((G1, G2, G3)))\n",
    "    h = np.concatenate((np.zeros(2*row), np.ones(row)))\n",
    "    \n",
    "    # specify number of variables\n",
    "    dims={'l': col+2*row, 'q': []}\n",
    "                       \n",
    "    solution = ecos.solve(c,G,h,dims,A,b, verbose=False)\n",
    "\n",
    "    p1_value = solution['x'][:row]\n",
    "    p2_value = solution['z'][:col] # z is the dual variable of x\n",
    "    # There are at least two bad cases with above constrained optimization,\n",
    "    # where the constraints are not fully satisfied (some numerical issue):\n",
    "    # 1. the sum of vars is larger than 1.\n",
    "    # 2. the value of var may be negative.\n",
    "    abs_p1_value = np.abs(p1_value)\n",
    "    abs_p2_value = np.abs(p2_value)\n",
    "    p1_value = abs_p1_value/np.sum(abs_p1_value)\n",
    "    p2_value = abs_p2_value/np.sum(abs_p2_value)\n",
    "\n",
    "    return (p1_value, p2_value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class ArbitraryMDP():\n",
    "    def __init__(self, num_states=3, num_actions_per_player=2, num_trans=3):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions_per_player = num_actions_per_player\n",
    "        self.num_actions = self.num_actions_per_player**2\n",
    "        self.num_trans = num_trans\n",
    "        self.reward_range = [-1,1]\n",
    "        self.state = None\n",
    "        self._construct_game()\n",
    "\n",
    "    def _construct_game(self, ):\n",
    "        self.trans_prob_matrices, self.reward_matrices = self.generate_random_trans_and_rewards()\n",
    "\n",
    "    def generate_random_trans_and_rewards(self,):\n",
    "        trans_prob_matrices = []\n",
    "        reward_matrices = []\n",
    "        for _ in range(self.num_trans):\n",
    "            trans_prob_matrix = []\n",
    "            reward_matrix = []\n",
    "            for s in range(self.num_states):\n",
    "                trans_prob_matrix_for_s = []\n",
    "                reward_matrix_for_s = []\n",
    "                for a in range(self.num_actions):\n",
    "                    rands = np.random.uniform(0,1, self.num_states)\n",
    "                    rand_probs = list(rands/sum(rands))\n",
    "                    trans_prob_matrix_for_s.append(rand_probs)\n",
    "                    rs = np.random.uniform(*self.reward_range, self.num_states)\n",
    "                    reward_matrix_for_s.append(list(rs))\n",
    "                trans_prob_matrix.append(trans_prob_matrix_for_s)\n",
    "                reward_matrix.append(reward_matrix_for_s)\n",
    "            trans_prob_matrices.append(trans_prob_matrix)\n",
    "            reward_matrices.append(reward_matrix)\n",
    "\n",
    "        return trans_prob_matrices, reward_matrices\n",
    "\n",
    "    def reset(self, ):\n",
    "        self.state = np.random.randint(0, self.num_states)  # randomly pick one state as initial\n",
    "        self.trans = 0\n",
    "        obs = self.state\n",
    "        return obs\n",
    "\n",
    "    def step(self, a):\n",
    "        \"\"\"The environment transition function.\n",
    "        For a given state and action, the transition is stochastic. \n",
    "        For representation of states, considering the num_states=3 case, the first three states are (0,1,2);\n",
    "        after one transition, the possible states are (3,4,5), etc. Such that states after different numbers of \n",
    "        transitions can be distinguished. \n",
    "\n",
    "        :param a: action\n",
    "        \"\"\"\n",
    "        trans_prob = self.trans_prob_matrices[self.trans][self.state%self.num_states][a]\n",
    "        next_state = np.random.choice([i for i in range(self.num_states)], p=trans_prob) + (self.trans+1) * self.num_states\n",
    "        reward = self.reward_matrices[self.trans][self.state%self.num_states][a][next_state%self.num_states]\n",
    "\n",
    "        self.state = next_state\n",
    "        obs = self.state\n",
    "        self.trans += 1\n",
    "        done = False if self.trans < self.num_trans else True\n",
    "\n",
    "        return obs, reward, done, None\n",
    "\n",
    "    def NEsolver(self,):\n",
    "        self.Nash_v = []\n",
    "        self.Nash_strategies = []\n",
    "        for tm, rm in zip(self.trans_prob_matrices[::-1], self.reward_matrices[::-1]): # inverse enumerate \n",
    "            if len(self.Nash_v) > 0:\n",
    "                rm = np.array(rm)+np.array(self.Nash_v[-1])  # broadcast sum on rm's last dim, last one in Nash_v is for the next state\n",
    "            trm = np.einsum(\"ijk,ijk->ij\", tm, rm)  # transition prob * reward for the last dimension in (state, action, next_state)\n",
    "            trm = trm.reshape(-1, self.num_actions_per_player, self.num_actions_per_player) # action list to matrix\n",
    "            ne_values = []\n",
    "            ne_strategies = []\n",
    "            for s_payoff in trm:\n",
    "                ne = NashEquilibriumECOSSolver(s_payoff)\n",
    "                ne_strategies.append(ne)\n",
    "                ne_value = ne[0]@s_payoff@ne[1].T\n",
    "                ne_values.append(ne_value)  # each value is a Nash equilibrium value on one state\n",
    "            self.Nash_v.append(ne_values)  # (trans, state)\n",
    "            self.Nash_strategies.append(ne_strategies)\n",
    "        self.Nash_v = self.Nash_v[::-1]\n",
    "        print('Nash strategies of all states: ', self.Nash_strategies)\n",
    "        self.Nash_strategies = self.Nash_strategies[::-1]\n",
    "        print('Nash values of all states: ', self.Nash_v)\n",
    "        print('Nash strategies of all states: ', self.Nash_strategies)\n",
    "\n",
    "env = ArbitraryMDP()\n",
    "env.NEsolver()\n",
    "obs = env.reset()\n",
    "print('ini: ', obs)    \n",
    "\n",
    "for _ in range(env.num_trans+1):\n",
    "    o,r,d,_ = env.step(1)\n",
    "    print(o,r,d)\n",
    "    if d:\n",
    "        break\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nash values of all states:  [[-0.07034894510037229, -0.12165961496544798, -0.1231529747442372], [-0.11650771089793736, -0.4427388156496247, -0.2291649596228951], [0.047601410235858975, -0.5273669918931514, 0.5957175655384962]]\n",
      "Nash strategies of all states:  [[(array([0.87662394, 0.12337606]), array([0.94613562, 0.05386438])), (array([1.84944776e-11, 1.00000000e+00]), array([4.85412952e-11, 1.00000000e+00])), (array([0.33493167, 0.66506833]), array([0.45890273, 0.54109727]))], [(array([3.1754892e-11, 1.0000000e+00]), array([1.23189702e-11, 1.00000000e+00])), (array([0.77744955, 0.22255045]), array([0.48238818, 0.51761182])), (array([6.83524686e-10, 9.99999999e-01]), array([1.00000000e+00, 2.04625125e-10]))], [(array([1.94368420e-09, 9.99999998e-01]), array([9.99999999e-01, 5.24423023e-10])), (array([1.38224865e-10, 1.00000000e+00]), array([9.48760378e-11, 1.00000000e+00])), (array([8.49308535e-12, 1.00000000e+00]), array([4.27167993e-11, 1.00000000e+00]))]]\n",
      "ini:  1\n",
      "4 -0.8113310662584976 False\n",
      "6 -0.6415462770190046 False\n",
      "9 -0.7527026839215745 True\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/quantumiracle/anaconda3/envs/res/lib/python3.6/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.\n",
      "  warn(\"Converting G to a CSC matrix; may take a while.\")\n",
      "/home/quantumiracle/anaconda3/envs/res/lib/python3.6/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.\n",
      "  warn(\"Converting A to a CSC matrix; may take a while.\")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def kl(p, q):\n",
    "    \"\"\"Kullback-Leibler divergence D(P || Q) for discrete distributions\n",
    "    Parameters\n",
    "    ----------\n",
    "    p, q : array-like, dtype=float, shape=n\n",
    "    Discrete probability distributions.\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=np.float)\n",
    "    q = np.asarray(q, dtype=np.float)\n",
    "\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "a=np.array([0.5,0.5])\n",
    "b=np.array([0.,1.])\n",
    "kl(a,b)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/quantumiracle/anaconda3/envs/res/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "np.arange(2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('res': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "interpreter": {
   "hash": "230862b98d5539ea4f6edff4f834846d7aac25a9878478a73da403f7aa963d5d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}