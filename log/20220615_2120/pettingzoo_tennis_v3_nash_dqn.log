Cannot import pettingzoo env:  combat_jet_v1
tennis_v3 pettingzoo
type:  pettingzoo
tennis_v3
Load tennis_v3 environment in type pettingzoo.
Env observation space: <bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7fa4357eadd8>> action space: <bound method aec_to_parallel_wrapper.action_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7fa4357eadd8>>
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7fa4357e7208>
random seed: [976, 986, 982, 869, 829]
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7fa4357e7208>
discrete_policy 18 Discrete(18)
NashDQNBase(
  (net): ImpalaCNN(
    (cnn_layers): ModuleList(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
    )
    (max_pool_layers): ModuleList(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
      (1): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
    )
    (residual_blocks_whole): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
    )
    (residual_blocks): ModuleList(
      (0): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
      (1): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=28224, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
      (5): Linear(in_features=128, out_features=324, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 18 Discrete(18)
NashDQNBase(
  (net): ImpalaCNN(
    (cnn_layers): ModuleList(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
    )
    (max_pool_layers): ModuleList(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
      (1): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
    )
    (residual_blocks_whole): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
    )
    (residual_blocks): ModuleList(
      (0): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
      (1): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=28224, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
      (5): Linear(in_features=128, out_features=324, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 18 Discrete(18)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'tennis_v3', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': False, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 5000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax', 'channel_list': [32, 16], 'kernel_size_list': [4, 4], 'stride_list': [1, 1]}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220615_2120/pettingzoo_tennis_v3_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220615_2120/pettingzoo_tennis_v3_nash_dqn.
Episode: 1/50000 (0.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 29.2844s / 29.2844 s
first_0:                 episode reward: 3.0000,                 loss: 0.0189
second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 889.5446s / 918.8291 s
first_0:                 episode reward: 2.3500,                 loss: 0.0174
second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 891.0804s / 1809.9095 s
first_0:                 episode reward: 2.5000,                 loss: 0.0176
second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 884.0087s / 2693.9182 s
first_0:                 episode reward: 2.3500,                 loss: 0.0176
second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 888.9734s / 3582.8916 s
first_0:                 episode reward: 2.0500,                 loss: 0.0175
second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 887.2163s / 4470.1079 s
first_0:                 episode reward: 1.8500,                 loss: 0.0177
second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 891.5433s / 5361.6512 s
first_0:                 episode reward: 2.2000,                 loss: 0.0177
second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 897.0980s / 6258.7492 s
first_0:                 episode reward: 2.4500,                 loss: 0.0181
second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 898.2233s / 7156.9725 s
first_0:                 episode reward: 1.7500,                 loss: 0.0178
second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 903.0517s / 8060.0242 s
first_0:                 episode reward: 2.2000,                 loss: 0.0173
second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 906.4670s / 8966.4912 s
first_0:                 episode reward: 2.0500,                 loss: 0.0177
second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 910.9611s / 9877.4523 s
first_0:                 episode reward: 1.8500,                 loss: 0.0175
second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 915.0334s / 10792.4856 s
first_0:                 episode reward: 2.4000,                 loss: 0.0178
second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 922.8118s / 11715.2974 s
first_0:                 episode reward: 2.6000,                 loss: 0.0176
second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 929.3108s / 12644.6082 s
first_0:                 episode reward: 2.4500,                 loss: 0.0176
second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 961.8378s / 13606.4459 s
first_0:                 episode reward: 2.2500,                 loss: 0.0179
second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 5718.8807s / 19325.3266 s
first_0:                 episode reward: 2.1500,                 loss: 0.0180
second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 933.9327s / 20259.2593 s
first_0:                 episode reward: 2.3000,                 loss: 0.0181
second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 976.5888s / 21235.8481 s
first_0:                 episode reward: 2.1000,                 loss: 0.0177
second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 966.4092s / 22202.2573 s
first_0:                 episode reward: 2.2000,                 loss: 0.0182
second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 925.6403s / 23127.8976 s
first_0:                 episode reward: 2.5000,                 loss: 0.0179
second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 924.7070s / 24052.6046 s
first_0:                 episode reward: 2.3000,                 loss: 0.0183
second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 923.0158s / 24975.6204 s
first_0:                 episode reward: 1.8000,                 loss: 0.0183
second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 921.7328s / 25897.3532 s
first_0:                 episode reward: 2.8000,                 loss: 0.0184
second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 926.6087s / 26823.9618 s
first_0:                 episode reward: 1.8500,                 loss: 0.0179
second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 921.2847s / 27745.2465 s
first_0:                 episode reward: 2.2000,                 loss: 0.0183
second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 937.7296s / 28682.9761 s
first_0:                 episode reward: 2.5000,                 loss: 0.0181
second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 922.6890s / 29605.6651 s
first_0:                 episode reward: 1.6500,                 loss: 0.0182
second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 923.1579s / 30528.8230 s
first_0:                 episode reward: 2.0000,                 loss: 0.0184
second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 920.8886s / 31449.7115 s
first_0:                 episode reward: 2.2000,                 loss: 0.0180
second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 925.0116s / 32374.7232 s
first_0:                 episode reward: 2.1500,                 loss: 0.0181
second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 921.1272s / 33295.8503 s
first_0:                 episode reward: 2.4000,                 loss: 0.0182
second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 918.7938s / 34214.6441 s
first_0:                 episode reward: 2.3500,                 loss: 0.0180
second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 919.5805s / 35134.2246 s
first_0:                 episode reward: 2.3000,                 loss: 0.0181
second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 917.4234s / 36051.6480 s
first_0:                 episode reward: 2.1500,                 loss: 0.0179
second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 919.9315s / 36971.5795 s
first_0:                 episode reward: 2.6500,                 loss: 0.0181
second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 921.9846s / 37893.5641 s
first_0:                 episode reward: 2.1500,                 loss: 0.0181
second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 918.4581s / 38812.0222 s
first_0:                 episode reward: 2.5000,                 loss: 0.0181
second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 917.1074s / 39729.1296 s
first_0:                 episode reward: 1.9000,                 loss: 0.0180
second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 921.0172s / 40650.1468 s
first_0:                 episode reward: 2.0000,                 loss: 0.0183
second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 923.9148s / 41574.0616 s
first_0:                 episode reward: 2.2500,                 loss: 0.0184
second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 928.1536s / 42502.2152 s
first_0:                 episode reward: 2.2000,                 loss: 0.0180
second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 921.3207s / 43423.5359 s
first_0:                 episode reward: 2.0000,                 loss: 0.0183
second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 926.2982s / 44349.8341 s
first_0:                 episode reward: 2.2500,                 loss: 0.0182
second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 923.1786s / 45273.0127 s
first_0:                 episode reward: 1.7000,                 loss: 0.0182
second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 922.8432s / 46195.8559 s
first_0:                 episode reward: 2.5500,                 loss: 0.0183
second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 921.2057s / 47117.0616 s
first_0:                 episode reward: 2.3500,                 loss: 0.0181
second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 919.8164s / 48036.8779 s
first_0:                 episode reward: 2.3000,                 loss: 0.0180
second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 961/50000 (1.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 918.9721s / 48955.8501 s
first_0:                 episode reward: 2.6000,                 loss: 0.0183
second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 981/50000 (1.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 919.1561s / 49875.0062 s
first_0:                 episode reward: 2.2500,                 loss: 0.0181
second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 1001/50000 (2.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 919.7952s / 50794.8015 s
first_0:                 episode reward: 2.1000,                 loss: 0.0181
second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1021/50000 (2.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 920.3882s / 51715.1897 s
first_0:                 episode reward: 2.3000,                 loss: 0.0180
second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1041/50000 (2.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 923.0183s / 52638.2080 s
first_0:                 episode reward: 1.9000,                 loss: 0.0185
second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 1061/50000 (2.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 921.7374s / 53559.9454 s
first_0:                 episode reward: 2.1000,                 loss: 0.0182
second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 1081/50000 (2.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 921.4843s / 54481.4297 s
first_0:                 episode reward: 2.4000,                 loss: 0.0182
second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 1101/50000 (2.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 918.6677s / 55400.0974 s
first_0:                 episode reward: 1.8000,                 loss: 0.0182
second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 1121/50000 (2.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 920.9782s / 56321.0756 s
first_0:                 episode reward: 2.1500,                 loss: 0.0182
second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1141/50000 (2.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 923.2877s / 57244.3633 s
first_0:                 episode reward: 2.0500,                 loss: 0.0183
second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 1161/50000 (2.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 922.7736s / 58167.1369 s
first_0:                 episode reward: 2.2500,                 loss: 0.0184
second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 1181/50000 (2.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 920.9963s / 59088.1332 s