Cannot import pettingzoo env:  combat_jet_v1
double_dunk_v3 pettingzoo
type:  pettingzoo
double_dunk_v3
Load double_dunk_v3 environment in type pettingzoo.
Env observation space: <bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f6149fd5be0>> action space: <bound method aec_to_parallel_wrapper.action_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f6149fd5be0>>
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f6149fd1160>
random seed: [641, 798, 849, 535, 569]
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f6149fd1160>
discrete_policy 18 Discrete(18)
NashDQNBase(
  (net): ImpalaCNN(
    (cnn_layers): ModuleList(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
    )
    (max_pool_layers): ModuleList(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
      (1): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
    )
    (residual_blocks_whole): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
    )
    (residual_blocks): ModuleList(
      (0): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
      (1): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=28224, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
      (5): Linear(in_features=128, out_features=324, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 18 Discrete(18)
NashDQNBase(
  (net): ImpalaCNN(
    (cnn_layers): ModuleList(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
    )
    (max_pool_layers): ModuleList(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
      (1): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
    )
    (residual_blocks_whole): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
    )
    (residual_blocks): ModuleList(
      (0): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
      (1): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=28224, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
      (5): Linear(in_features=128, out_features=324, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 18 Discrete(18)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'double_dunk_v3', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': False, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 5000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax', 'channel_list': [32, 16], 'kernel_size_list': [4, 4], 'stride_list': [1, 1]}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220615_2120/pettingzoo_double_dunk_v3_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220615_2120/pettingzoo_double_dunk_v3_nash_dqn.
Episode: 1/50000 (0.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 29.3142s / 29.3142 s
first_0:                 episode reward: 0.0000,                 loss: 0.0168
second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 853.7422s / 883.0564 s
first_0:                 episode reward: -0.5500,                 loss: 0.0168
second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 857.5374s / 1740.5938 s
first_0:                 episode reward: -2.1000,                 loss: 0.0164
second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 871.5115s / 2612.1053 s
first_0:                 episode reward: -1.5000,                 loss: 0.0160
second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 875.7850s / 3487.8903 s
first_0:                 episode reward: -1.7000,                 loss: 0.0156
second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 874.9248s / 4362.8151 s
first_0:                 episode reward: -0.9000,                 loss: 0.0158
second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 879.2414s / 5242.0565 s
first_0:                 episode reward: -1.7500,                 loss: 0.0157
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 891.7110s / 6133.7675 s
first_0:                 episode reward: -0.3000,                 loss: 0.0155
second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 902.5846s / 7036.3521 s
first_0:                 episode reward: -1.1000,                 loss: 0.0154
second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 900.9424s / 7937.2944 s
first_0:                 episode reward: -1.3000,                 loss: 0.0156
second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 902.4797s / 8839.7741 s
first_0:                 episode reward: -2.3500,                 loss: 0.0155
second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 909.9546s / 9749.7287 s
first_0:                 episode reward: -0.4500,                 loss: 0.0155
second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 912.2885s / 10662.0172 s
first_0:                 episode reward: -2.5500,                 loss: 0.0154
second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 912.9041s / 11574.9213 s
first_0:                 episode reward: -2.2500,                 loss: 0.0154
second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 916.1148s / 12491.0361 s
first_0:                 episode reward: -1.9500,                 loss: 0.0153
second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 924.5235s / 13415.5597 s
first_0:                 episode reward: -2.1500,                 loss: 0.0153
second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 5702.5337s / 19118.0934 s
first_0:                 episode reward: -1.7500,                 loss: 0.0156
second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 918.0637s / 20036.1571 s
first_0:                 episode reward: -1.4500,                 loss: 0.0153
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 915.7270s / 20951.8841 s
first_0:                 episode reward: -0.6000,                 loss: 0.0156
second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 917.5946s / 21869.4788 s
first_0:                 episode reward: -1.3500,                 loss: 0.0152
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 914.8205s / 22784.2993 s
first_0:                 episode reward: -1.7500,                 loss: 0.0151
second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 914.5231s / 23698.8224 s
first_0:                 episode reward: -1.1000,                 loss: 0.0153
second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 920.0553s / 24618.8777 s
first_0:                 episode reward: -1.6000,                 loss: 0.0155
second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 916.7463s / 25535.6239 s
first_0:                 episode reward: -1.5000,                 loss: 0.0156
second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 921.6919s / 26457.3158 s
first_0:                 episode reward: -2.0000,                 loss: 0.0154
second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 918.6711s / 27375.9869 s
first_0:                 episode reward: -1.8000,                 loss: 0.0154
second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 926.5734s / 28302.5603 s
first_0:                 episode reward: -1.5500,                 loss: 0.0155
second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 928.0106s / 29230.5709 s
first_0:                 episode reward: -1.5500,                 loss: 0.0156
second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 922.6298s / 30153.2007 s
first_0:                 episode reward: -0.7000,                 loss: 0.0153
second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 920.3371s / 31073.5378 s
first_0:                 episode reward: -1.9500,                 loss: 0.0153
second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 918.1776s / 31991.7155 s
first_0:                 episode reward: -1.2000,                 loss: 0.0156
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 918.1280s / 32909.8435 s
first_0:                 episode reward: -1.2000,                 loss: 0.0156
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 914.0297s / 33823.8732 s
first_0:                 episode reward: -1.7500,                 loss: 0.0157
second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 916.4985s / 34740.3717 s
first_0:                 episode reward: -1.4000,                 loss: 0.0152
second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 913.9486s / 35654.3203 s
first_0:                 episode reward: -1.4500,                 loss: 0.0155
second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 917.6789s / 36571.9992 s
first_0:                 episode reward: -1.8000,                 loss: 0.0154
second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 918.1576s / 37490.1568 s
first_0:                 episode reward: -1.2500,                 loss: 0.0155
second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 917.8748s / 38408.0316 s
first_0:                 episode reward: -1.4500,                 loss: 0.0153
second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 917.3774s / 39325.4091 s
first_0:                 episode reward: -2.0000,                 loss: 0.0154
second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 922.6994s / 40248.1085 s
first_0:                 episode reward: -1.4500,                 loss: 0.0155
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 930.8593s / 41178.9678 s
first_0:                 episode reward: -1.3500,                 loss: 0.0156
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 936.3919s / 42115.3597 s
first_0:                 episode reward: -1.7500,                 loss: 0.0156
second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 925.3319s / 43040.6916 s
first_0:                 episode reward: -1.6000,                 loss: 0.0155
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 925.9980s / 43966.6896 s
first_0:                 episode reward: -0.8000,                 loss: 0.0153
second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 925.7085s / 44892.3981 s
first_0:                 episode reward: -2.0500,                 loss: 0.0156
second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 921.0061s / 45813.4042 s
first_0:                 episode reward: -0.8500,                 loss: 0.0152
second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 919.8607s / 46733.2649 s
first_0:                 episode reward: -1.2000,                 loss: 0.0155
second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 918.3101s / 47651.5750 s
first_0:                 episode reward: -1.2500,                 loss: 0.0154
second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 961/50000 (1.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 915.0180s / 48566.5930 s
first_0:                 episode reward: -2.1500,                 loss: 0.0154
second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 981/50000 (1.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 913.7215s / 49480.3145 s
first_0:                 episode reward: -2.1500,                 loss: 0.0155
second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1001/50000 (2.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 914.1591s / 50394.4736 s
first_0:                 episode reward: -1.7500,                 loss: 0.0156
second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1021/50000 (2.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 919.7837s / 51314.2573 s
first_0:                 episode reward: -0.9500,                 loss: 0.0157
second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1041/50000 (2.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 922.3331s / 52236.5904 s
first_0:                 episode reward: -1.3000,                 loss: 0.0154
second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1061/50000 (2.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 926.3881s / 53162.9785 s
first_0:                 episode reward: -1.6500,                 loss: 0.0155
second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1081/50000 (2.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 926.4974s / 54089.4759 s
first_0:                 episode reward: -1.6000,                 loss: 0.0155
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1101/50000 (2.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 926.4963s / 55015.9722 s
first_0:                 episode reward: -1.0500,                 loss: 0.0154
second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1121/50000 (2.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 921.7182s / 55937.6904 s
first_0:                 episode reward: -0.5000,                 loss: 0.0153
second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1141/50000 (2.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 927.5244s / 56865.2148 s
first_0:                 episode reward: -0.9000,                 loss: 0.0154
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1161/50000 (2.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 932.7041s / 57797.9189 s
first_0:                 episode reward: -0.5500,                 loss: 0.0157
second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1181/50000 (2.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 931.1583s / 58729.0772 s