pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [51, 18]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220115_0159/pettingzoo_surround_v1_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220115_0159/pettingzoo_surround_v1_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 1751.0,                last time consumption/overall running time: 14.9004s / 14.9004 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.0123
env0_second_0:                 episode reward: -2.0000,                 loss: -0.0225
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1499.95,                last time consumption/overall running time: 225.7270s / 240.6274 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0301
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0315
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1456.55,                last time consumption/overall running time: 220.0273s / 460.6547 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.0292
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0293
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1365.6,                last time consumption/overall running time: 204.5193s / 665.1740 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0375
env0_second_0:                 episode reward: -0.8000,                 loss: -0.0405
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1462.85,                last time consumption/overall running time: 221.0433s / 886.2173 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0305
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0342
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1436.15,                last time consumption/overall running time: 214.6097s / 1100.8270 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0461
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0467
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1386.2,                last time consumption/overall running time: 207.4671s / 1308.2940 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0341
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0357
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1398.1,                last time consumption/overall running time: 209.2181s / 1517.5122 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0409
env0_second_0:                 episode reward: -0.7500,                 loss: -0.0420
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1476.75,                last time consumption/overall running time: 224.0004s / 1741.5126 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0560
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0561
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1443.95,                last time consumption/overall running time: 215.8651s / 1957.3777 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0348
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0351
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1381.4,                last time consumption/overall running time: 207.9374s / 2165.3151 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.0435
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0428
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1379.6,                last time consumption/overall running time: 209.5247s / 2374.8398 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.0304
env0_second_0:                 episode reward: 0.8000,                 loss: -0.0319
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1470.4,                last time consumption/overall running time: 223.5142s / 2598.3541 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0408
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0417
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1398.75,                last time consumption/overall running time: 209.6972s / 2808.0513 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.0525
env0_second_0:                 episode reward: 2.3500,                 loss: -0.0541
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1447.65,                last time consumption/overall running time: 216.9870s / 3025.0383 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0319
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0346
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1435.2,                last time consumption/overall running time: 215.7355s / 3240.7738 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.0382
env0_second_0:                 episode reward: -1.2500,                 loss: -0.0402
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1432.45,                last time consumption/overall running time: 217.0046s / 3457.7784 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0478
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0488
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1397.9,                last time consumption/overall running time: 207.9913s / 3665.7698 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0432
env0_second_0:                 episode reward: -0.8000,                 loss: -0.0435
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1410.75,                last time consumption/overall running time: 212.1238s / 3877.8935 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0447
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0449
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1375.6,                last time consumption/overall running time: 204.5353s / 4082.4288 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.0527
env0_second_0:                 episode reward: 0.8000,                 loss: -0.0505
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1473.75,                last time consumption/overall running time: 219.0209s / 4301.4497 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0470
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0391
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1437.45,                last time consumption/overall running time: 214.1655s / 4515.6152 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.0548
env0_second_0:                 episode reward: -0.4500,                 loss: -0.0536
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1410.9,                last time consumption/overall running time: 211.8888s / 4727.5040 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0522
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0546
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1322.5,                last time consumption/overall running time: 198.2638s / 4925.7678 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0592
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0510
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1424.4,                last time consumption/overall running time: 213.4452s / 5139.2130 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0601
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0590
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1406.1,                last time consumption/overall running time: 212.1591s / 5351.3721 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0547
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0569
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1422.05,                last time consumption/overall running time: 213.8448s / 5565.2169 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0461
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0481
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1375.1,                last time consumption/overall running time: 206.8892s / 5772.1060 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0519
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0515
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1444.65,                last time consumption/overall running time: 217.6454s / 5989.7515 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0577
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0491
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1421.35,                last time consumption/overall running time: 214.3349s / 6204.0864 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0650
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0622
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1431.35,                last time consumption/overall running time: 213.8449s / 6417.9313 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0790
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0765
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1451.4,                last time consumption/overall running time: 219.1043s / 6637.0356 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0718
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0614
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1454.85,                last time consumption/overall running time: 216.4292s / 6853.4648 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.0702
env0_second_0:                 episode reward: -1.4000,                 loss: -0.0564
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1363.7,                last time consumption/overall running time: 206.9171s / 7060.3819 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0662
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0735
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1429.25,                last time consumption/overall running time: 216.9185s / 7277.3004 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0868
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0882
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1445.35,                last time consumption/overall running time: 218.5116s / 7495.8120 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0744
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0600
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1374.2,                last time consumption/overall running time: 205.9115s / 7701.7235 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0442
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0632
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1396.75,                last time consumption/overall running time: 211.4790s / 7913.2025 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.0513
env0_second_0:                 episode reward: 1.7000,                 loss: -0.0766
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1418.45,                last time consumption/overall running time: 215.5934s / 8128.7959 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0625
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0565
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1460.85,                last time consumption/overall running time: 219.7684s / 8348.5643 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0614
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0656
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1356.9,                last time consumption/overall running time: 207.8204s / 8556.3847 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0564
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0624
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1444.25,                last time consumption/overall running time: 215.9861s / 8772.3708 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0709
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0628
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1398.35,                last time consumption/overall running time: 211.3859s / 8983.7567 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0846
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0601
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1424.8,                last time consumption/overall running time: 215.1168s / 9198.8735 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0705
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0666
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1432.55,                last time consumption/overall running time: 217.1378s / 9416.0113 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0790
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0621
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1392.75,                last time consumption/overall running time: 212.4731s / 9628.4845 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.0920
env0_second_0:                 episode reward: 2.5000,                 loss: -0.0841
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1404.1,                last time consumption/overall running time: 210.1347s / 9838.6192 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0719
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0523
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1453.95,                last time consumption/overall running time: 220.0557s / 10058.6748 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.0646
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0626
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1496.7,                last time consumption/overall running time: 225.3867s / 10284.0615 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0682
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0482
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1433.2,                last time consumption/overall running time: 218.5467s / 10502.6083 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0513
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0495
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1451.1,                last time consumption/overall running time: 219.9741s / 10722.5824 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0544
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0608
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1362.45,                last time consumption/overall running time: 204.6506s / 10927.2330 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0609
env0_second_0:                 episode reward: 0.1500,                 loss: -0.0645
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1427.7,                last time consumption/overall running time: 215.0014s / 11142.2344 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0647
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0795
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1454.6,                last time consumption/overall running time: 217.0527s / 11359.2871 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0749
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0683
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1464.1,                last time consumption/overall running time: 219.3538s / 11578.6409 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0705
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0566
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1399.4,                last time consumption/overall running time: 211.3192s / 11789.9601 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0576
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0565
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1381.75,                last time consumption/overall running time: 209.2144s / 11999.1745 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0731
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0541
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1435.85,                last time consumption/overall running time: 215.1674s / 12214.3419 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.0773
env0_second_0:                 episode reward: 1.1500,                 loss: -0.0599
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1337.2,                last time consumption/overall running time: 203.6352s / 12417.9771 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0477
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0478
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1339.35,                last time consumption/overall running time: 201.1653s / 12619.1424 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0830
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0700
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1496.0,                last time consumption/overall running time: 223.0394s / 12842.1818 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.0619
env0_second_0:                 episode reward: -1.1500,                 loss: -0.0744
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1444.8,                last time consumption/overall running time: 217.6639s / 13059.8457 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0729
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0589
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1496.4,                last time consumption/overall running time: 225.7560s / 13285.6017 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0734
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0581
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1428.45,                last time consumption/overall running time: 214.7673s / 13500.3690 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.0873
env0_second_0:                 episode reward: 1.9000,                 loss: -0.0899
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1453.55,                last time consumption/overall running time: 218.0541s / 13718.4231 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0875
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0781
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1389.85,                last time consumption/overall running time: 210.4539s / 13928.8770 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0575
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0543
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1454.15,                last time consumption/overall running time: 218.5360s / 14147.4130 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0854
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0763
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1467.3,                last time consumption/overall running time: 222.6406s / 14370.0536 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0607
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0844
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1445.5,                last time consumption/overall running time: 217.4413s / 14587.4948 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0672
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0744
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1518.05,                last time consumption/overall running time: 228.8971s / 14816.3919 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0723
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0729
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1541.25,                last time consumption/overall running time: 233.5643s / 15049.9562 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0668
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0653
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1378.6,                last time consumption/overall running time: 210.4637s / 15260.4199 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0738
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0720
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1402.85,                last time consumption/overall running time: 211.6078s / 15472.0276 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0735
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0924
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1438.6,                last time consumption/overall running time: 217.7436s / 15689.7712 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0674
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0740
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1440.5,                last time consumption/overall running time: 216.8227s / 15906.5940 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0740
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0753
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1487.15,                last time consumption/overall running time: 224.5888s / 16131.1827 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0627
env0_second_0:                 episode reward: -0.8000,                 loss: -0.0764
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1434.5,                last time consumption/overall running time: 216.7714s / 16347.9541 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0810
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0647
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1405.5,                last time consumption/overall running time: 211.8510s / 16559.8051 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0920
env0_second_0:                 episode reward: -0.8000,                 loss: -0.0801
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1375.55,                last time consumption/overall running time: 208.6168s / 16768.4219 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0658
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0670
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1500.55,                last time consumption/overall running time: 228.1914s / 16996.6133 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0846
env0_second_0:                 episode reward: -0.5500,                 loss: -0.0757
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1473.1,                last time consumption/overall running time: 221.7580s / 17218.3713 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0859
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0924
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1458.05,                last time consumption/overall running time: 219.3499s / 17437.7212 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0916
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0896
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1500.55,                last time consumption/overall running time: 224.6467s / 17662.3679 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0819
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0916
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1459.15,                last time consumption/overall running time: 220.0987s / 17882.4666 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0923
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0954
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1414.0,                last time consumption/overall running time: 213.8145s / 18096.2811 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0830
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0604
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1498.75,                last time consumption/overall running time: 228.0364s / 18324.3175 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0900
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0671
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1415.6,                last time consumption/overall running time: 212.4523s / 18536.7698 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0786
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0729
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1440.9,                last time consumption/overall running time: 219.4681s / 18756.2379 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.0696
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0674
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1401.85,                last time consumption/overall running time: 212.1171s / 18968.3550 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0730
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0843
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1526.15,                last time consumption/overall running time: 227.5558s / 19195.9108 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0569
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0594
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1415.75,                last time consumption/overall running time: 214.3038s / 19410.2145 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0849
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0868
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1465.85,                last time consumption/overall running time: 220.8598s / 19631.0744 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0713
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0631
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1416.5,                last time consumption/overall running time: 213.6165s / 19844.6909 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0932
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0674
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1442.4,                last time consumption/overall running time: 216.3372s / 20061.0281 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0736
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0739
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1383.25,                last time consumption/overall running time: 209.3381s / 20270.3662 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0939
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0914
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1391.7,                last time consumption/overall running time: 210.8893s / 20481.2556 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0603
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0727
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1495.7,                last time consumption/overall running time: 226.1912s / 20707.4467 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0904
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0777
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1431.35,                last time consumption/overall running time: 214.0201s / 20921.4669 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0743
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0835
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1418.5,                last time consumption/overall running time: 212.6491s / 21134.1160 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.0831
env0_second_0:                 episode reward: 1.7500,                 loss: -0.0941
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1381.4,                last time consumption/overall running time: 212.1842s / 21346.3002 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0712
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0752
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1446.5,                last time consumption/overall running time: 216.8829s / 21563.1831 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.0551
env0_second_0:                 episode reward: 1.0500,                 loss: -0.0713
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1411.9,                last time consumption/overall running time: 213.2658s / 21776.4489 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.1013
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0875
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1490.1,                last time consumption/overall running time: 226.3864s / 22002.8353 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0830
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0834
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1455.65,                last time consumption/overall running time: 217.0007s / 22219.8360 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0604
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0802
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1495.75,                last time consumption/overall running time: 224.7995s / 22444.6355 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0660
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0931
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1423.1,                last time consumption/overall running time: 214.6407s / 22659.2762 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0814
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0892
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1400.25,                last time consumption/overall running time: 209.3332s / 22868.6094 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0641
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0710
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1465.3,                last time consumption/overall running time: 220.0765s / 23088.6859 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0865
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0754
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1432.1,                last time consumption/overall running time: 215.0814s / 23303.7673 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0730
env0_second_0:                 episode reward: -0.8000,                 loss: -0.1010
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1364.7,                last time consumption/overall running time: 204.4303s / 23508.1976 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0837
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0865
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1458.3,                last time consumption/overall running time: 218.4974s / 23726.6950 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0743
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0619
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1386.1,                last time consumption/overall running time: 208.7499s / 23935.4450 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.0911
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0800
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1365.35,                last time consumption/overall running time: 206.1253s / 24141.5702 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0691
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0674
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1438.4,                last time consumption/overall running time: 218.8604s / 24360.4307 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0718
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0736
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1491.85,                last time consumption/overall running time: 220.7113s / 24581.1420 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0594
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0621
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1465.85,                last time consumption/overall running time: 220.6200s / 24801.7620 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0769
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0788
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1407.7,                last time consumption/overall running time: 211.4760s / 25013.2381 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0929
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0820
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1468.3,                last time consumption/overall running time: 219.9957s / 25233.2338 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0902
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0807
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1446.35,                last time consumption/overall running time: 217.3480s / 25450.5818 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0806
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0781
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1450.5,                last time consumption/overall running time: 219.3903s / 25669.9721 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0724
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0780
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1448.5,                last time consumption/overall running time: 218.3353s / 25888.3074 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0652
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0822
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1405.1,                last time consumption/overall running time: 212.1870s / 26100.4944 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0586
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0649
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1417.75,                last time consumption/overall running time: 214.7535s / 26315.2479 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0597
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0704
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1412.65,                last time consumption/overall running time: 213.5848s / 26528.8327 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.0557
env0_second_0:                 episode reward: -0.4500,                 loss: -0.0613
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1473.85,                last time consumption/overall running time: 221.7752s / 26750.6079 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0850
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0736
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1410.55,                last time consumption/overall running time: 213.4857s / 26964.0936 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.0824
env0_second_0:                 episode reward: 2.0000,                 loss: -0.0763
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 1325.95,                last time consumption/overall running time: 197.6665s / 27161.7601 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0778
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0780
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1439.8,                last time consumption/overall running time: 214.2404s / 27376.0005 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0760
env0_second_0:                 episode reward: -0.5500,                 loss: -0.0849
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1459.6,                last time consumption/overall running time: 221.6842s / 27597.6846 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0756
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0847
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1468.55,                last time consumption/overall running time: 219.0194s / 27816.7040 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.0807
env0_second_0:                 episode reward: -1.6500,                 loss: -0.0840
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1513.95,                last time consumption/overall running time: 228.0026s / 28044.7066 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0654
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0691
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1399.9,                last time consumption/overall running time: 213.8245s / 28258.5311 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.0723
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0832
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1384.6,                last time consumption/overall running time: 210.0055s / 28468.5366 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0747
env0_second_0:                 episode reward: -0.7500,                 loss: -0.0885
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1445.6,                last time consumption/overall running time: 220.3694s / 28688.9060 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.0839
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0867
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1392.1,                last time consumption/overall running time: 209.6610s / 28898.5670 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.0643
env0_second_0:                 episode reward: -0.4500,                 loss: -0.0855
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1407.5,                last time consumption/overall running time: 212.3569s / 29110.9240 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0784
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0742
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1473.35,                last time consumption/overall running time: 220.9340s / 29331.8579 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0971
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0793
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1340.3,                last time consumption/overall running time: 203.4126s / 29535.2705 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.0750
env0_second_0:                 episode reward: 1.7500,                 loss: -0.0855
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1345.9,                last time consumption/overall running time: 203.5421s / 29738.8126 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0875
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0946
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1386.2,                last time consumption/overall running time: 209.0141s / 29947.8267 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.0828
env0_second_0:                 episode reward: -0.9000,                 loss: -0.0805
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1419.75,                last time consumption/overall running time: 213.4742s / 30161.3009 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0879
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0947
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1428.15,                last time consumption/overall running time: 218.0363s / 30379.3373 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0955
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0983
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1374.05,                last time consumption/overall running time: 208.4758s / 30587.8130 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0881
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0815
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1496.1,                last time consumption/overall running time: 225.5584s / 30813.3715 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.0861
env0_second_0:                 episode reward: -0.4500,                 loss: -0.0901
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1437.45,                last time consumption/overall running time: 215.6884s / 31029.0599 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0710
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0736
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1425.25,                last time consumption/overall running time: 216.4564s / 31245.5162 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0920
env0_second_0:                 episode reward: 0.1500,                 loss: -0.0716
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1461.1,                last time consumption/overall running time: 217.8607s / 31463.3770 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0888
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0876
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1375.65,                last time consumption/overall running time: 207.0370s / 31670.4139 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.1005
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0819
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1436.85,                last time consumption/overall running time: 217.3919s / 31887.8059 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1005
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0834
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1448.15,                last time consumption/overall running time: 218.7840s / 32106.5899 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1061
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0917
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1375.8,                last time consumption/overall running time: 208.4491s / 32315.0390 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0853
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0710
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1427.95,                last time consumption/overall running time: 216.1199s / 32531.1589 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.0799
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0696
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1497.0,                last time consumption/overall running time: 224.8048s / 32755.9637 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0881
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0602
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1492.35,                last time consumption/overall running time: 225.1220s / 32981.0857 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0892
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0565
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1398.5,                last time consumption/overall running time: 208.3947s / 33189.4803 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0808
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0580
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1404.35,                last time consumption/overall running time: 213.3068s / 33402.7871 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.0728
env0_second_0:                 episode reward: 1.3000,                 loss: -0.0696
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1395.95,                last time consumption/overall running time: 209.6946s / 33612.4817 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0570
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0703
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1457.85,                last time consumption/overall running time: 219.2453s / 33831.7270 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0796
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0634
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1498.55,                last time consumption/overall running time: 223.0685s / 34054.7955 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0845
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0657
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1497.1,                last time consumption/overall running time: 227.3033s / 34282.0988 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0905
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0821
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1437.35,                last time consumption/overall running time: 213.9504s / 34496.0491 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0955
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0776
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1392.1,                last time consumption/overall running time: 211.9252s / 34707.9743 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0905
env0_second_0:                 episode reward: -1.5000,                 loss: -0.0832
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1416.25,                last time consumption/overall running time: 215.5843s / 34923.5586 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0825
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0660
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1437.3,                last time consumption/overall running time: 216.3515s / 35139.9102 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0826
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0851
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1373.05,                last time consumption/overall running time: 185.6757s / 35325.5859 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.0578
env0_second_0:                 episode reward: 1.9500,                 loss: -0.0676
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1505.3,                last time consumption/overall running time: 210.0654s / 35535.6512 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0894
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0743
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1500.1,                last time consumption/overall running time: 203.5042s / 35739.1555 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0728
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0680
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1451.4,                last time consumption/overall running time: 198.4308s / 35937.5863 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.0861
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0702
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1411.75,                last time consumption/overall running time: 187.9564s / 36125.5426 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.0874
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0867
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1431.4,                last time consumption/overall running time: 193.3468s / 36318.8894 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.0844
env0_second_0:                 episode reward: 1.6000,                 loss: -0.0837
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1375.95,                last time consumption/overall running time: 186.9961s / 36505.8855 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0602
env0_second_0:                 episode reward: 1.2500,                 loss: -0.0719
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1381.1,                last time consumption/overall running time: 188.4412s / 36694.3267 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.0737
env0_second_0:                 episode reward: 1.9500,                 loss: -0.0918
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1442.5,                last time consumption/overall running time: 193.2027s / 36887.5294 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0673
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0813
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1432.55,                last time consumption/overall running time: 190.3010s / 37077.8304 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0834
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0614
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1402.35,                last time consumption/overall running time: 190.8795s / 37268.7098 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0979
env0_second_0:                 episode reward: -1.0500,                 loss: -0.0813
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1412.45,                last time consumption/overall running time: 190.0142s / 37458.7241 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0838
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0760
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1339.7,                last time consumption/overall running time: 179.4758s / 37638.1999 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.0843
env0_second_0:                 episode reward: 1.1500,                 loss: -0.0897
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1404.2,                last time consumption/overall running time: 186.8942s / 37825.0941 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.0862
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0867
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1426.5,                last time consumption/overall running time: 193.1612s / 38018.2553 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.0929
env0_second_0:                 episode reward: 0.8000,                 loss: -0.0860
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1429.3,                last time consumption/overall running time: 192.3992s / 38210.6545 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0795
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0716
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1370.15,                last time consumption/overall running time: 185.2638s / 38395.9183 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0794
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0683
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1469.4,                last time consumption/overall running time: 195.9310s / 38591.8494 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.0965
env0_second_0:                 episode reward: -1.0000,                 loss: -0.0906
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1416.9,                last time consumption/overall running time: 195.8017s / 38787.6510 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0760
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0850
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1380.7,                last time consumption/overall running time: 187.0293s / 38974.6804 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0843
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0834
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1458.3,                last time consumption/overall running time: 196.3563s / 39171.0366 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1037
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0899
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1439.3,                last time consumption/overall running time: 194.5264s / 39365.5630 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0924
env0_second_0:                 episode reward: 0.1500,                 loss: -0.0941
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1448.65,                last time consumption/overall running time: 194.8278s / 39560.3909 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0799
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0925
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1428.75,                last time consumption/overall running time: 194.9313s / 39755.3221 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0763
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0858
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1472.85,                last time consumption/overall running time: 199.1574s / 39954.4795 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0639
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0867
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1433.05,                last time consumption/overall running time: 192.5396s / 40147.0191 s
env0_first_0:                 episode reward: -3.0000,                 loss: -0.0597
env0_second_0:                 episode reward: 3.0000,                 loss: -0.0765
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1465.5,                last time consumption/overall running time: 197.7692s / 40344.7883 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0682
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0837
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1454.9,                last time consumption/overall running time: 196.1034s / 40540.8916 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0794
env0_second_0:                 episode reward: -1.5000,                 loss: -0.0826
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1469.4,                last time consumption/overall running time: 195.3477s / 40736.2393 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.0787
env0_second_0:                 episode reward: 1.9500,                 loss: -0.0899
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1451.15,                last time consumption/overall running time: 191.9497s / 40928.1890 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0664
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0804
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1480.3,                last time consumption/overall running time: 195.1677s / 41123.3566 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0779
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0754
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1408.05,                last time consumption/overall running time: 188.9783s / 41312.3349 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0659
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0847
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1441.0,                last time consumption/overall running time: 193.1890s / 41505.5239 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0761
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0760
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1450.5,                last time consumption/overall running time: 196.5817s / 41702.1056 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0757
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0808
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1408.05,                last time consumption/overall running time: 187.3239s / 41889.4295 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0778
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0772
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1387.9,                last time consumption/overall running time: 186.9297s / 42076.3593 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0680
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0765
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1428.75,                last time consumption/overall running time: 198.4908s / 42274.8500 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0888
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0867
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1432.95,                last time consumption/overall running time: 193.4720s / 42468.3220 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0959
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0868
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1497.3,                last time consumption/overall running time: 207.0350s / 42675.3571 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0691
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0580
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1447.7,                last time consumption/overall running time: 196.9647s / 42872.3218 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0685
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0560
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1479.6,                last time consumption/overall running time: 202.6591s / 43074.9809 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0850
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0730
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1466.3,                last time consumption/overall running time: 198.7717s / 43273.7526 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.0774
env0_second_0:                 episode reward: -1.1000,                 loss: -0.0788
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1511.1,                last time consumption/overall running time: 204.5756s / 43478.3282 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.0958
env0_second_0:                 episode reward: 1.5000,                 loss: -0.0942
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1548.45,                last time consumption/overall running time: 209.1781s / 43687.5063 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.0844
env0_second_0:                 episode reward: 1.7000,                 loss: -0.0818
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1393.4,                last time consumption/overall running time: 187.0275s / 43874.5338 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0799
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0781
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1403.95,                last time consumption/overall running time: 189.4895s / 44064.0233 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0926
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0871
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1390.55,                last time consumption/overall running time: 188.8562s / 44252.8795 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.0617
env0_second_0:                 episode reward: 1.1000,                 loss: -0.0788
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1469.5,                last time consumption/overall running time: 197.0201s / 44449.8996 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0699
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0838
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1408.1,                last time consumption/overall running time: 187.2840s / 44637.1835 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0792
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0925
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1432.5,                last time consumption/overall running time: 193.3796s / 44830.5632 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0800
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0938
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1389.25,                last time consumption/overall running time: 186.0998s / 45016.6630 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.0678
env0_second_0:                 episode reward: 1.0500,                 loss: -0.0803
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1430.8,                last time consumption/overall running time: 193.0973s / 45209.7602 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0750
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0929
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1487.6,                last time consumption/overall running time: 199.2159s / 45408.9762 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.0822
env0_second_0:                 episode reward: -1.0000,                 loss: -0.0946
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1382.8,                last time consumption/overall running time: 185.0910s / 45594.0672 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0837
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0865
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1519.2,                last time consumption/overall running time: 204.6580s / 45798.7252 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.0950
env0_second_0:                 episode reward: -1.1000,                 loss: -0.0736
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1427.2,                last time consumption/overall running time: 195.0015s / 45993.7267 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.0716
env0_second_0:                 episode reward: -1.0000,                 loss: -0.0855
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1393.15,                last time consumption/overall running time: 187.3442s / 46181.0709 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0666
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0784
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1321.1,                last time consumption/overall running time: 184.0554s / 46365.1263 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0809
env0_second_0:                 episode reward: -0.5500,                 loss: -0.0803
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1451.15,                last time consumption/overall running time: 196.6009s / 46561.7272 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0655
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0847
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1468.35,                last time consumption/overall running time: 199.5009s / 46761.2282 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0747
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0822
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1551.25,                last time consumption/overall running time: 206.5530s / 46967.7811 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0728
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0812
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1487.1,                last time consumption/overall running time: 200.4280s / 47168.2092 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0727
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0741
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1460.75,                last time consumption/overall running time: 200.2505s / 47368.4597 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0748
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0680
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1442.95,                last time consumption/overall running time: 197.2674s / 47565.7271 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0932
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0712
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1491.7,                last time consumption/overall running time: 205.8139s / 47771.5410 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.0907
env0_second_0:                 episode reward: 1.1000,                 loss: -0.0716
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1407.1,                last time consumption/overall running time: 190.9863s / 47962.5273 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.0789
env0_second_0:                 episode reward: 1.7000,                 loss: -0.0670
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1468.3,                last time consumption/overall running time: 203.6641s / 48166.1914 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.1028
env0_second_0:                 episode reward: -0.4500,                 loss: -0.0963
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1442.1,                last time consumption/overall running time: 193.7564s / 48359.9478 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0501
env0_second_0:                 episode reward: -0.7500,                 loss: -0.0509
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1462.7,                last time consumption/overall running time: 194.9488s / 48554.8966 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0850
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0811
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1467.75,                last time consumption/overall running time: 202.0523s / 48756.9489 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1023
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0926
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1449.25,                last time consumption/overall running time: 192.0837s / 48949.0326 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0965
env0_second_0:                 episode reward: -0.7500,                 loss: -0.0975
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1393.05,                last time consumption/overall running time: 190.3076s / 49139.3402 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0924
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0858
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1415.35,                last time consumption/overall running time: 191.3676s / 49330.7078 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0919
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0992
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1506.8,                last time consumption/overall running time: 202.8214s / 49533.5292 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.0985
env0_second_0:                 episode reward: -1.0000,                 loss: -0.0862
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1435.25,                last time consumption/overall running time: 194.8246s / 49728.3537 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.0812
env0_second_0:                 episode reward: 0.8000,                 loss: -0.0764
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1430.5,                last time consumption/overall running time: 192.4495s / 49920.8032 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0711
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0778
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1429.65,                last time consumption/overall running time: 195.7481s / 50116.5513 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0859
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0857
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1435.95,                last time consumption/overall running time: 191.2105s / 50307.7618 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0846
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0841
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1407.95,                last time consumption/overall running time: 191.7063s / 50499.4681 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0875
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0791
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1377.2,                last time consumption/overall running time: 184.0295s / 50683.4976 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0676
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0685
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1401.15,                last time consumption/overall running time: 187.5755s / 50871.0731 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0811
env0_second_0:                 episode reward: 1.4000,                 loss: -0.0608
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1394.3,                last time consumption/overall running time: 190.3378s / 51061.4109 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0817
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0569
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 1418.95,                last time consumption/overall running time: 192.3177s / 51253.7286 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0879
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0701
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1511.35,                last time consumption/overall running time: 205.5860s / 51459.3146 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.0820
env0_second_0:                 episode reward: 1.3000,                 loss: -0.0710
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1430.8,                last time consumption/overall running time: 197.2777s / 51656.5923 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0655
env0_second_0:                 episode reward: 0.1500,                 loss: -0.0700
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1472.6,                last time consumption/overall running time: 195.8630s / 51852.4554 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1031
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0978
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1423.3,                last time consumption/overall running time: 193.1339s / 52045.5893 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.1038
env0_second_0:                 episode reward: 1.5000,                 loss: -0.0977
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1376.45,                last time consumption/overall running time: 186.4059s / 52231.9953 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.0958
env0_second_0:                 episode reward: 1.7500,                 loss: -0.0849
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1455.25,                last time consumption/overall running time: 200.1842s / 52432.1795 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0671
env0_second_0:                 episode reward: 1.8500,                 loss: -0.0641
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1447.5,                last time consumption/overall running time: 197.5852s / 52629.7647 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0826
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0957
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1427.15,                last time consumption/overall running time: 196.9601s / 52826.7248 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.0869
env0_second_0:                 episode reward: 1.1000,                 loss: -0.1038
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1443.75,                last time consumption/overall running time: 195.7852s / 53022.5100 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0994
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0981
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1411.95,                last time consumption/overall running time: 189.5669s / 53212.0769 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0713
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0936
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1419.0,                last time consumption/overall running time: 193.9967s / 53406.0736 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0782
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0808
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1447.9,                last time consumption/overall running time: 192.7964s / 53598.8700 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0827
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0935
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1472.35,                last time consumption/overall running time: 198.1134s / 53796.9834 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0780
env0_second_0:                 episode reward: 1.2500,                 loss: -0.0929
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1472.85,                last time consumption/overall running time: 197.8146s / 53994.7981 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0782
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0872
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1419.5,                last time consumption/overall running time: 190.5292s / 54185.3273 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0693
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0855
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1508.9,                last time consumption/overall running time: 202.7671s / 54388.0944 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0686
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0880
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1497.2,                last time consumption/overall running time: 199.7758s / 54587.8702 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0740
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0965
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1448.75,                last time consumption/overall running time: 194.3139s / 54782.1841 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0858
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0898
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1487.05,                last time consumption/overall running time: 199.0484s / 54981.2325 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0743
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0900
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1477.25,                last time consumption/overall running time: 197.8114s / 55179.0439 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.0805
env0_second_0:                 episode reward: 1.2000,                 loss: -0.0942
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1372.35,                last time consumption/overall running time: 190.2024s / 55369.2463 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0790
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0899
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1391.65,                last time consumption/overall running time: 185.0398s / 55554.2861 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0957
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0980
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1421.75,                last time consumption/overall running time: 191.7764s / 55746.0625 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0876
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0765
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1446.8,                last time consumption/overall running time: 194.2489s / 55940.3114 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0876
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0843
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1421.75,                last time consumption/overall running time: 191.3229s / 56131.6343 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0901
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0768
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1429.8,                last time consumption/overall running time: 194.8931s / 56326.5274 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0899
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0870
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 1442.4,                last time consumption/overall running time: 191.2873s / 56517.8147 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.0777
env0_second_0:                 episode reward: -1.9000,                 loss: -0.0907
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1398.7,                last time consumption/overall running time: 187.4637s / 56705.2784 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0744
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0776
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1390.9,                last time consumption/overall running time: 187.1745s / 56892.4529 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0710
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0871
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 1496.3,                last time consumption/overall running time: 201.2703s / 57093.7232 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0668
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0751
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1425.9,                last time consumption/overall running time: 189.4556s / 57283.1788 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0791
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0902
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1400.05,                last time consumption/overall running time: 191.5570s / 57474.7358 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0611
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0797
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1448.45,                last time consumption/overall running time: 196.5475s / 57671.2833 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0645
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0888
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1450.85,                last time consumption/overall running time: 196.5547s / 57867.8380 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0814
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0728
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1339.35,                last time consumption/overall running time: 179.2102s / 58047.0482 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0775
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0766
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1473.2,                last time consumption/overall running time: 199.2394s / 58246.2876 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.0856
env0_second_0:                 episode reward: -1.6500,                 loss: -0.0977
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1404.05,                last time consumption/overall running time: 188.2371s / 58434.5247 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0779
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0864
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1430.25,                last time consumption/overall running time: 194.1312s / 58628.6559 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0887
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0890
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1449.15,                last time consumption/overall running time: 193.9545s / 58822.6104 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0702
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0886
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 1373.7,                last time consumption/overall running time: 185.6453s / 59008.2557 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.0736
env0_second_0:                 episode reward: 1.1500,                 loss: -0.0800
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 1449.1,                last time consumption/overall running time: 198.5932s / 59206.8489 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.0846
env0_second_0:                 episode reward: 1.5500,                 loss: -0.0937
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 1507.4,                last time consumption/overall running time: 202.8124s / 59409.6613 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0824
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0805
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1438.15,                last time consumption/overall running time: 193.0703s / 59602.7315 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0725
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0865
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 1463.0,                last time consumption/overall running time: 199.5440s / 59802.2755 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.0926
env0_second_0:                 episode reward: 1.2000,                 loss: -0.0974
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1535.5,                last time consumption/overall running time: 205.5266s / 60007.8022 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0892
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0879
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1355.3,                last time consumption/overall running time: 184.5166s / 60192.3188 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.1006
env0_second_0:                 episode reward: 1.1500,                 loss: -0.0855
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 1467.15,                last time consumption/overall running time: 195.6111s / 60387.9299 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.0900
env0_second_0:                 episode reward: -1.4500,                 loss: -0.0743
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1390.2,                last time consumption/overall running time: 185.6537s / 60573.5835 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0845
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0895
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1451.6,                last time consumption/overall running time: 195.5013s / 60769.0848 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0704
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0825
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 1380.3,                last time consumption/overall running time: 186.6565s / 60955.7413 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0721
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1005
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 1416.6,                last time consumption/overall running time: 191.5766s / 61147.3179 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0983
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0991
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 1484.85,                last time consumption/overall running time: 192.5768s / 61339.8947 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.0850
env0_second_0:                 episode reward: -1.1500,                 loss: -0.0883
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 1470.15,                last time consumption/overall running time: 194.7294s / 61534.6241 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.0639
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0959
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 1489.65,                last time consumption/overall running time: 200.3638s / 61734.9879 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0824
env0_second_0:                 episode reward: 0.5500,                 loss: -0.1045
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 1414.35,                last time consumption/overall running time: 191.0485s / 61926.0365 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0665
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0870
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 1507.15,                last time consumption/overall running time: 200.3071s / 62126.3436 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0808
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1009
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 1400.65,                last time consumption/overall running time: 187.6249s / 62313.9684 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0926
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0916
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1486.75,                last time consumption/overall running time: 199.1315s / 62513.1000 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.0815
env0_second_0:                 episode reward: 1.6000,                 loss: -0.0766
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 1439.4,                last time consumption/overall running time: 192.4401s / 62705.5400 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0929
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0778
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1398.3,                last time consumption/overall running time: 186.7995s / 62892.3396 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0704
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0718
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1452.9,                last time consumption/overall running time: 194.7420s / 63087.0815 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0803
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0728
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1424.9,                last time consumption/overall running time: 191.9891s / 63279.0707 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0842
env0_second_0:                 episode reward: -0.8000,                 loss: -0.0747
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 1443.3,                last time consumption/overall running time: 197.2232s / 63476.2939 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0959
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0960
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 1481.25,                last time consumption/overall running time: 200.3691s / 63676.6629 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0941
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0907
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1418.5,                last time consumption/overall running time: 196.9933s / 63873.6562 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.0796
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0813
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 1473.15,                last time consumption/overall running time: 197.8315s / 64071.4877 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0893
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0848
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 1435.85,                last time consumption/overall running time: 194.5956s / 64266.0834 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0827
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0896
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 1392.85,                last time consumption/overall running time: 183.1905s / 64449.2739 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0631
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0686
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1419.6,                last time consumption/overall running time: 188.4581s / 64637.7320 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0812
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0758
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1518.8,                last time consumption/overall running time: 205.2946s / 64843.0266 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0831
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0985
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 1578.8,                last time consumption/overall running time: 207.4358s / 65050.4625 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0850
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0881
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1514.8,                last time consumption/overall running time: 205.2787s / 65255.7412 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0898
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0922
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 1543.2,                last time consumption/overall running time: 211.4089s / 65467.1501 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0742
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0834
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 1436.6,                last time consumption/overall running time: 195.4397s / 65662.5898 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.0890
env0_second_0:                 episode reward: 1.3000,                 loss: -0.0906
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 1435.05,                last time consumption/overall running time: 195.7146s / 65858.3044 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0781
env0_second_0:                 episode reward: -0.5500,                 loss: -0.0780
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 1479.4,                last time consumption/overall running time: 205.1351s / 66063.4396 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1033
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1033
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 1394.4,                last time consumption/overall running time: 196.6736s / 66260.1132 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0937
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0958
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 1432.75,                last time consumption/overall running time: 195.6769s / 66455.7901 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0841
env0_second_0:                 episode reward: -1.0500,                 loss: -0.0893
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 1419.95,                last time consumption/overall running time: 194.6136s / 66650.4036 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0883
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1012
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 1403.95,                last time consumption/overall running time: 191.5491s / 66841.9527 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.0901
env0_second_0:                 episode reward: -2.8000,                 loss: -0.0965
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 1471.55,                last time consumption/overall running time: 200.5984s / 67042.5511 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0909
env0_second_0:                 episode reward: -0.7500,                 loss: -0.0881
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 1432.45,                last time consumption/overall running time: 196.2605s / 67238.8116 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1007
env0_second_0:                 episode reward: 1.2500,                 loss: -0.0937
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 1427.4,                last time consumption/overall running time: 196.1262s / 67434.9378 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1008
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0974
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1452.7,                last time consumption/overall running time: 191.5899s / 67626.5277 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0883
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0850
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 1435.3,                last time consumption/overall running time: 196.7295s / 67823.2573 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0899
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0849
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 1411.5,                last time consumption/overall running time: 190.2477s / 68013.5050 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0909
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0877
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 1417.75,                last time consumption/overall running time: 192.0117s / 68205.5166 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.0919
env0_second_0:                 episode reward: 0.8000,                 loss: -0.0938
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 1389.95,                last time consumption/overall running time: 192.4602s / 68397.9768 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0876
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0905
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 1445.45,                last time consumption/overall running time: 195.9739s / 68593.9507 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0953
env0_second_0:                 episode reward: 1.4000,                 loss: -0.0931
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 1415.1,                last time consumption/overall running time: 192.9653s / 68786.9159 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.0824
env0_second_0:                 episode reward: 1.3500,                 loss: -0.0897
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 1417.0,                last time consumption/overall running time: 197.9872s / 68984.9031 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0912
env0_second_0:                 episode reward: 0.1500,                 loss: -0.0900
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 1492.45,                last time consumption/overall running time: 203.2608s / 69188.1640 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0883
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0970
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 1389.7,                last time consumption/overall running time: 189.5552s / 69377.7192 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0834
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0747
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1416.15,                last time consumption/overall running time: 196.0592s / 69573.7784 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0963
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0934
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 1452.75,                last time consumption/overall running time: 193.4279s / 69767.2063 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0832
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0832
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 1467.65,                last time consumption/overall running time: 197.8235s / 69965.0298 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0930
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0888
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 1425.95,                last time consumption/overall running time: 192.0280s / 70157.0578 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1056
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1082
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 1464.25,                last time consumption/overall running time: 198.0689s / 70355.1267 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0730
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0736
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 1411.05,                last time consumption/overall running time: 190.8493s / 70545.9759 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0862
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0591
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 1412.3,                last time consumption/overall running time: 190.4059s / 70736.3818 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1084
env0_second_0:                 episode reward: -1.2500,                 loss: -0.1024
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 1454.9,                last time consumption/overall running time: 200.3621s / 70936.7439 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0983
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1030
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 1405.9,                last time consumption/overall running time: 192.5111s / 71129.2550 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.0925
env0_second_0:                 episode reward: 1.6000,                 loss: -0.0937
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 1398.4,                last time consumption/overall running time: 189.0573s / 71318.3123 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0953
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1029
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 1402.2,                last time consumption/overall running time: 194.2504s / 71512.5627 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0891
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0756
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 1428.85,                last time consumption/overall running time: 192.7800s / 71705.3426 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0834
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1052
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 1360.55,                last time consumption/overall running time: 187.2573s / 71892.5999 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0734
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0815
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 1387.15,                last time consumption/overall running time: 190.6929s / 72083.2929 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0932
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0835
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 1491.5,                last time consumption/overall running time: 203.8008s / 72287.0936 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1017
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0949
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 1427.5,                last time consumption/overall running time: 196.1087s / 72483.2024 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0564
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0807
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 1378.75,                last time consumption/overall running time: 189.4330s / 72672.6353 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0695
env0_second_0:                 episode reward: 2.2500,                 loss: -0.0871
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 1457.5,                last time consumption/overall running time: 201.8728s / 72874.5081 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0814
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0950
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1508.05,                last time consumption/overall running time: 207.3685s / 73081.8766 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0858
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0936
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 1408.05,                last time consumption/overall running time: 194.5012s / 73276.3778 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0922
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0974
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 1446.85,                last time consumption/overall running time: 196.0366s / 73472.4144 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0921
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0935
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1391.5,                last time consumption/overall running time: 190.1598s / 73662.5743 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0944
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0940
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 1476.5,                last time consumption/overall running time: 203.5950s / 73866.1693 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0962
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0908
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 1375.95,                last time consumption/overall running time: 185.7501s / 74051.9194 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.1009
env0_second_0:                 episode reward: 1.1500,                 loss: -0.1021
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 1517.1,                last time consumption/overall running time: 202.4043s / 74254.3237 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0884
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0856
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 1501.8,                last time consumption/overall running time: 207.1695s / 74461.4932 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0856
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0836
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 1491.65,                last time consumption/overall running time: 201.8034s / 74663.2966 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.0997
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0983
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 1420.4,                last time consumption/overall running time: 194.0486s / 74857.3452 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.0928
env0_second_0:                 episode reward: 1.0500,                 loss: -0.0821
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 1470.55,                last time consumption/overall running time: 202.9078s / 75060.2530 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0873
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0849
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1501.8,                last time consumption/overall running time: 202.4179s / 75262.6709 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0931
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0854
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1487.45,                last time consumption/overall running time: 203.1992s / 75465.8701 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0785
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0858
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1370.65,                last time consumption/overall running time: 190.0342s / 75655.9043 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0874
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0899
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 1404.25,                last time consumption/overall running time: 192.3956s / 75848.2999 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0819
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0801
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 1432.25,                last time consumption/overall running time: 201.7254s / 76050.0253 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0943
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0983
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 1473.75,                last time consumption/overall running time: 204.0544s / 76254.0797 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1044
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0895
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 1459.1,                last time consumption/overall running time: 198.5722s / 76452.6520 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1076
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1091
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 1443.7,                last time consumption/overall running time: 197.8074s / 76650.4594 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0971
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0804
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 1416.3,                last time consumption/overall running time: 195.2318s / 76845.6911 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0985
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1001
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 1392.1,                last time consumption/overall running time: 193.2123s / 77038.9034 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0825
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0841
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 1391.15,                last time consumption/overall running time: 193.1491s / 77232.0526 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0729
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0742
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 1503.85,                last time consumption/overall running time: 202.1678s / 77434.2204 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0884
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0867
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 1421.6,                last time consumption/overall running time: 196.9102s / 77631.1306 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0855
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0831
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 1503.2,                last time consumption/overall running time: 207.0708s / 77838.2013 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0939
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0926
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1443.2,                last time consumption/overall running time: 200.1715s / 78038.3728 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0904
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0974
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 1407.4,                last time consumption/overall running time: 193.1921s / 78231.5650 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0811
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0808
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 1451.1,                last time consumption/overall running time: 201.2525s / 78432.8175 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1035
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1047
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 1493.55,                last time consumption/overall running time: 204.2183s / 78637.0358 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0722
env0_second_0:                 episode reward: -0.5500,                 loss: -0.0793
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 1387.2,                last time consumption/overall running time: 191.1268s / 78828.1626 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0793
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0902
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 1418.3,                last time consumption/overall running time: 196.2509s / 79024.4135 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.0759
env0_second_0:                 episode reward: 1.6000,                 loss: -0.0858
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 1416.3,                last time consumption/overall running time: 195.9462s / 79220.3597 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0886
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0863
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 1507.6,                last time consumption/overall running time: 204.1291s / 79424.4888 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0876
env0_second_0:                 episode reward: 1.2500,                 loss: -0.0868
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 1387.9,                last time consumption/overall running time: 185.1905s / 79609.6793 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0825
env0_second_0:                 episode reward: -0.5500,                 loss: -0.0844
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 1403.1,                last time consumption/overall running time: 194.8993s / 79804.5786 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0789
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0830
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 1434.4,                last time consumption/overall running time: 191.5903s / 79996.1689 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0928
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0932
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 1436.85,                last time consumption/overall running time: 197.7406s / 80193.9096 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0920
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0935
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 1423.75,                last time consumption/overall running time: 195.1252s / 80389.0348 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0820
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0920
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 1438.7,                last time consumption/overall running time: 204.0552s / 80593.0899 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0970
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0913
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 1433.15,                last time consumption/overall running time: 200.1846s / 80793.2745 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.0768
env0_second_0:                 episode reward: 1.6000,                 loss: -0.0823
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 1415.7,                last time consumption/overall running time: 191.5184s / 80984.7929 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0825
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0850
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 1437.65,                last time consumption/overall running time: 194.8243s / 81179.6172 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.0973
env0_second_0:                 episode reward: 1.0500,                 loss: -0.0990
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 1478.4,                last time consumption/overall running time: 197.4581s / 81377.0753 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0988
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0828
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 1392.55,                last time consumption/overall running time: 190.3653s / 81567.4406 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1023
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0741
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 1443.8,                last time consumption/overall running time: 196.3290s / 81763.7696 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0717
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0541
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 1389.35,                last time consumption/overall running time: 188.9875s / 81952.7571 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0904
env0_second_0:                 episode reward: 0.1500,                 loss: -0.0928
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 1490.25,                last time consumption/overall running time: 201.4597s / 82154.2168 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0936
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0966
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 1429.7,                last time consumption/overall running time: 193.7092s / 82347.9260 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0855
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0888
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 1403.25,                last time consumption/overall running time: 197.0678s / 82544.9938 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1116
env0_second_0:                 episode reward: 2.1000,                 loss: -0.1101
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 1474.8,                last time consumption/overall running time: 201.7225s / 82746.7163 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1000
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1065
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 1462.25,                last time consumption/overall running time: 198.4470s / 82945.1634 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0883
env0_second_0:                 episode reward: -0.8000,                 loss: -0.0842
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 1455.7,                last time consumption/overall running time: 194.6421s / 83139.8055 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0887
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0819
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 1421.8,                last time consumption/overall running time: 198.1407s / 83337.9462 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0918
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1065
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 1483.0,                last time consumption/overall running time: 202.5877s / 83540.5339 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0769
env0_second_0:                 episode reward: -0.7500,                 loss: -0.0834
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 1432.25,                last time consumption/overall running time: 192.9397s / 83733.4735 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0903
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0953
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 1367.05,                last time consumption/overall running time: 185.5872s / 83919.0607 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.0993
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0926
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 1400.1,                last time consumption/overall running time: 193.2185s / 84112.2792 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0963
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0940
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 1365.5,                last time consumption/overall running time: 190.2927s / 84302.5719 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.0978
env0_second_0:                 episode reward: 1.3500,                 loss: -0.0995
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 1528.4,                last time consumption/overall running time: 215.0468s / 84517.6187 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0943
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0993
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 1411.05,                last time consumption/overall running time: 200.6789s / 84718.2975 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.0835
env0_second_0:                 episode reward: -1.4000,                 loss: -0.0865
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 1318.35,                last time consumption/overall running time: 185.9200s / 84904.2175 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.0998
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1036
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 1451.6,                last time consumption/overall running time: 197.6897s / 85101.9072 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0834
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0819
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 1493.45,                last time consumption/overall running time: 200.0537s / 85301.9609 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0840
env0_second_0:                 episode reward: 1.4000,                 loss: -0.0975
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 1367.7,                last time consumption/overall running time: 184.3663s / 85486.3272 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0951
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0990
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 1523.0,                last time consumption/overall running time: 206.3802s / 85692.7074 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0962
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1043
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 1414.75,                last time consumption/overall running time: 189.5135s / 85882.2210 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0743
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0830
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 1392.75,                last time consumption/overall running time: 191.8178s / 86074.0388 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1013
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0931
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 1448.6,                last time consumption/overall running time: 197.1093s / 86271.1481 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0892
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0876
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 1422.2,                last time consumption/overall running time: 192.0580s / 86463.2061 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0819
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0858
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 1395.0,                last time consumption/overall running time: 181.7987s / 86645.0048 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0723
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0939
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 1326.95,                last time consumption/overall running time: 178.7238s / 86823.7286 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0845
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0922
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 1545.15,                last time consumption/overall running time: 206.9517s / 87030.6803 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0627
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0774
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 1469.7,                last time consumption/overall running time: 207.2256s / 87237.9059 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0917
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1012
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 1468.25,                last time consumption/overall running time: 197.6429s / 87435.5488 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0762
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0656
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 1469.2,                last time consumption/overall running time: 206.6860s / 87642.2348 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0870
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0907
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 1453.5,                last time consumption/overall running time: 183.0030s / 87825.2378 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0937
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0886
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 1471.05,                last time consumption/overall running time: 189.0686s / 88014.3064 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.1044
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0935
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 1410.5,                last time consumption/overall running time: 180.0628s / 88194.3692 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0852
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0754
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 1442.05,                last time consumption/overall running time: 181.7484s / 88376.1176 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0794
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0940
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 1489.1,                last time consumption/overall running time: 191.7737s / 88567.8913 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0866
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0817
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 1384.6,                last time consumption/overall running time: 180.3938s / 88748.2850 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0778
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0814
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 1342.15,                last time consumption/overall running time: 172.4130s / 88920.6980 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0881
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0842
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 1331.7,                last time consumption/overall running time: 164.0226s / 89084.7206 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0878
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0919
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1425.6,                last time consumption/overall running time: 181.6103s / 89266.3309 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.0950
env0_second_0:                 episode reward: -1.6000,                 loss: -0.0979
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 1366.35,                last time consumption/overall running time: 171.7579s / 89438.0888 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1010
env0_second_0:                 episode reward: -1.4500,                 loss: -0.0995
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 1451.1,                last time consumption/overall running time: 186.9923s / 89625.0811 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0898
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0886
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 1475.4,                last time consumption/overall running time: 190.5846s / 89815.6657 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0812
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0861
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 1436.25,                last time consumption/overall running time: 182.9977s / 89998.6633 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0974
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0956
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 1522.4,                last time consumption/overall running time: 190.8495s / 90189.5128 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.1031
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0927
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 1332.9,                last time consumption/overall running time: 170.4638s / 90359.9766 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0781
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0769
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 1425.95,                last time consumption/overall running time: 191.5169s / 90551.4934 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0845
env0_second_0:                 episode reward: 1.4000,                 loss: -0.0883
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 1377.2,                last time consumption/overall running time: 177.3107s / 90728.8042 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0914
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0972
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 1472.6,                last time consumption/overall running time: 183.6602s / 90912.4643 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.1064
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1081
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 1439.9,                last time consumption/overall running time: 180.5866s / 91093.0509 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0939
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0950
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 1404.8,                last time consumption/overall running time: 187.5851s / 91280.6360 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0862
env0_second_0:                 episode reward: 0.1500,                 loss: -0.0821
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 1487.2,                last time consumption/overall running time: 187.8935s / 91468.5295 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0843
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0877
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 1443.4,                last time consumption/overall running time: 182.8569s / 91651.3864 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.0797
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0862
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 1349.75,                last time consumption/overall running time: 170.5736s / 91821.9600 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.0714
env0_second_0:                 episode reward: 1.1000,                 loss: -0.0788
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 1420.25,                last time consumption/overall running time: 176.8089s / 91998.7689 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.0772
env0_second_0:                 episode reward: -1.2500,                 loss: -0.0887
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 1411.75,                last time consumption/overall running time: 182.4331s / 92181.2020 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0751
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0958
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 1511.85,                last time consumption/overall running time: 194.4727s / 92375.6747 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0713
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0939
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 1410.65,                last time consumption/overall running time: 176.1687s / 92551.8434 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0842
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0914
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 1389.3,                last time consumption/overall running time: 172.4550s / 92724.2984 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0926
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0984
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 1475.4,                last time consumption/overall running time: 188.2049s / 92912.5033 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.0813
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0957
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 1453.5,                last time consumption/overall running time: 188.0665s / 93100.5698 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0825
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0952
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 1535.4,                last time consumption/overall running time: 193.4455s / 93294.0153 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0862
env0_second_0:                 episode reward: 1.4000,                 loss: -0.1007
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 1432.0,                last time consumption/overall running time: 176.5825s / 93470.5977 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.0885
env0_second_0:                 episode reward: -1.2500,                 loss: -0.0861
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 1478.4,                last time consumption/overall running time: 185.1955s / 93655.7932 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1052
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0890
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 1408.4,                last time consumption/overall running time: 175.0399s / 93830.8331 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0786
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0794
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 1524.65,                last time consumption/overall running time: 193.5955s / 94024.4286 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0955
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0892
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 1454.15,                last time consumption/overall running time: 186.1087s / 94210.5373 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0809
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0926
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 1451.6,                last time consumption/overall running time: 182.5414s / 94393.0787 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0932
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0920
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 1429.25,                last time consumption/overall running time: 181.7139s / 94574.7926 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0910
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0922
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 1427.6,                last time consumption/overall running time: 181.7112s / 94756.5038 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.0743
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0835
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 1425.3,                last time consumption/overall running time: 175.9024s / 94932.4062 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0719
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1043
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 1511.8,                last time consumption/overall running time: 192.5064s / 95124.9126 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0767
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0782
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 1467.25,                last time consumption/overall running time: 187.2740s / 95312.1865 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0876
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0898
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 1389.15,                last time consumption/overall running time: 177.4775s / 95489.6641 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.0930
env0_second_0:                 episode reward: 1.1000,                 loss: -0.0742
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 1425.8,                last time consumption/overall running time: 180.0326s / 95669.6967 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0834
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0626
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 1455.2,                last time consumption/overall running time: 182.1865s / 95851.8832 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.0920
env0_second_0:                 episode reward: -1.4000,                 loss: -0.0864
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 1441.15,                last time consumption/overall running time: 185.0578s / 96036.9409 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0858
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0725
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 1448.8,                last time consumption/overall running time: 185.8619s / 96222.8028 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0930
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0880
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 1496.0,                last time consumption/overall running time: 185.9862s / 96408.7890 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0903
env0_second_0:                 episode reward: -0.5500,                 loss: -0.0900
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 1407.2,                last time consumption/overall running time: 181.0333s / 96589.8223 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.0706
env0_second_0:                 episode reward: 1.1000,                 loss: -0.0738
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 1391.4,                last time consumption/overall running time: 179.8013s / 96769.6236 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0772
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0916
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 1418.9,                last time consumption/overall running time: 179.3055s / 96948.9290 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0757
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0824
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 1411.3,                last time consumption/overall running time: 176.5310s / 97125.4600 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0909
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1038
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 1527.15,                last time consumption/overall running time: 192.5345s / 97317.9946 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0933
env0_second_0:                 episode reward: 1.2500,                 loss: -0.0929
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 1395.9,                last time consumption/overall running time: 172.9224s / 97490.9170 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0851
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0863
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 1381.05,                last time consumption/overall running time: 175.2578s / 97666.1747 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.0935
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0873
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 1347.15,                last time consumption/overall running time: 171.8516s / 97838.0263 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0766
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0878
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 1396.55,                last time consumption/overall running time: 173.1160s / 98011.1423 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0870
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0912
env1_first_0:                 episode reward: -0.3000,                 loss: nanLoad surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
/home/zihan/research/MARS/mars/rl/agents/nash_ppo.py:181: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  s,a,r,s_prime,prob_a,done_mask =    torch.tensor(s_lst, dtype=torch.float).to(self.device), torch.tensor(a_lst).to(self.device), \
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 1396.25,                last time consumption/overall running time: 176.6892s / 98187.8315 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0932
env0_second_0:                 episode reward: -1.0500,                 loss: -0.0928
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 1366.25,                last time consumption/overall running time: 172.0746s / 98359.9061 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0890
env0_second_0:                 episode reward: -1.5000,                 loss: -0.0786
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 1425.8,                last time consumption/overall running time: 180.7267s / 98540.6328 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.0992
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0882
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 1471.3,                last time consumption/overall running time: 191.7568s / 98732.3896 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1172
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1110
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 1403.95,                last time consumption/overall running time: 180.5225s / 98912.9121 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0897
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0662
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 1444.6,                last time consumption/overall running time: 185.8473s / 99098.7594 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1028
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0960
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 1435.05,                last time consumption/overall running time: 182.0840s / 99280.8434 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.0965
env0_second_0:                 episode reward: -1.2500,                 loss: -0.0892
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 1436.25,                last time consumption/overall running time: 181.5139s / 99462.3572 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0963
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0867
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 1415.5,                last time consumption/overall running time: 178.9349s / 99641.2921 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0863
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0881
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 1438.25,                last time consumption/overall running time: 181.9791s / 99823.2713 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0950
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0773
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
