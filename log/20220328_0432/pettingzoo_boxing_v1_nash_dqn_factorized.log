Traceback (most recent call last):
  File "/home/zihan/research/MARS/general_train.py", line 1, in <module>
    from mars.utils.func import LoadYAML2Dict
  File "/home/zihan/research/MARS/mars/utils/func.py", line 7, in <module>
    from mars.rl.agents  import *
  File "/home/zihan/research/MARS/mars/rl/agents/__init__.py", line 1, in <module>
    from .dqn import DQN
  File "/home/zihan/research/MARS/mars/rl/agents/dqn.py", line 1, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [23, 24]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNFactorized', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 150, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_factorized', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220328_0432/pettingzoo_boxing_v1_nash_dqn_factorized. 
 Save logs to: /home/zihan/research/MARS/data/log/20220328_0432/pettingzoo_boxing_v1_nash_dqn_factorized.
Episode: 1/50000 (0.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 2.6024s / 2.6024 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0040
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0051
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 219.3076s / 221.9101 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0053
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0055
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 223.8062s / 445.7163 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0068
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0072
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 224.1566s / 669.8728 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0070
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0078
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 222.2645s / 892.1373 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0069
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0077
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 223.2072s / 1115.3446 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0076
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0092
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 225.3374s / 1340.6820 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0080
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0098
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 224.0499s / 1564.7319 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0086
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0098
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 224.7549s / 1789.4867 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0092
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0098
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 225.6810s / 2015.1677 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0100
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0113
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 224.5128s / 2239.6805 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0123
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0133
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 225.9454s / 2465.6259 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0132
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0145
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 226.4116s / 2692.0375 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0135
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0155
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 227.8556s / 2919.8931 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0134
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0158
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 228.2236s / 3148.1167 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0135
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0146
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 228.9049s / 3377.0216 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0142
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0138
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 230.4152s / 3607.4368 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0148
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0140
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 230.3357s / 3837.7725 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0158
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0156
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 230.0152s / 4067.7877 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0172
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0226
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 232.6071s / 4300.3949 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0203
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0315
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 234.8933s / 4535.2882 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0222
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0194
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 232.2646s / 4767.5528 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0263
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0226
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 233.9430s / 5001.4958 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0307
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0324
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 233.7588s / 5235.2546 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0357
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0373
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 233.7301s / 5468.9847 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0389
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0403
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 236.5803s / 5705.5650 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0374
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0396
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 238.5808s / 5944.1458 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0405
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0255
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 238.8653s / 6183.0111 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0390
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0259
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 237.9792s / 6420.9903 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0391
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0208
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 241.8341s / 6662.8244 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0398
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0212
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 240.7672s / 6903.5916 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0394
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0262
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 242.5736s / 7146.1652 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0386
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0418
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 245.0465s / 7391.2116 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0393
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0985
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 244.2342s / 7635.4459 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0405
env0_second_0:                 episode reward: 0.1500,                 loss: 0.1732
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 244.4802s / 7879.9261 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0425
env0_second_0:                 episode reward: -0.4000,                 loss: 0.3913
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 246.8367s / 8126.7628 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0423
env0_second_0:                 episode reward: 0.2000,                 loss: 0.4111
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 246.6309s / 8373.3937 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0436
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2374
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 251.8091s / 8625.2028 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0446
env0_second_0:                 episode reward: 0.0000,                 loss: 0.2298
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 251.1938s / 8876.3966 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0439
env0_second_0:                 episode reward: 0.2000,                 loss: 0.3932
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 252.5660s / 9128.9626 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0453
env0_second_0:                 episode reward: 0.3000,                 loss: 0.3211
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 251.8616s / 9380.8241 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0462
env0_second_0:                 episode reward: 0.4500,                 loss: 0.3421
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 250.7266s / 9631.5507 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0489
env0_second_0:                 episode reward: -0.4500,                 loss: 0.3792
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 252.6486s / 9884.1994 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0497
env0_second_0:                 episode reward: -0.2500,                 loss: 0.6092
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 249.5593s / 10133.7587 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0496
env0_second_0:                 episode reward: 0.2500,                 loss: 0.8870
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 252.1778s / 10385.9365 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0506
env0_second_0:                 episode reward: -0.2000,                 loss: 0.8498
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 252.3990s / 10638.3355 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0546
env0_second_0:                 episode reward: -0.0500,                 loss: 0.8840
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 250.2616s / 10888.5971 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0561
env0_second_0:                 episode reward: 0.4000,                 loss: 0.9415
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 252.7417s / 11141.3388 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0600
env0_second_0:                 episode reward: 0.0500,                 loss: 1.0998
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 961/50000 (1.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 249.7636s / 11391.1024 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0599
env0_second_0:                 episode reward: 0.3500,                 loss: 1.3402
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 981/50000 (1.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 252.8250s / 11643.9274 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0621
env0_second_0:                 episode reward: 0.1000,                 loss: 1.6778
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1001/50000 (2.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 251.5862s / 11895.5136 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0660
env0_second_0:                 episode reward: -0.1500,                 loss: 1.9146
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1021/50000 (2.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 255.1055s / 12150.6191 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0675
env0_second_0:                 episode reward: 0.0500,                 loss: 2.3052
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1041/50000 (2.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 250.4357s / 12401.0548 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0704
env0_second_0:                 episode reward: -0.2500,                 loss: 2.7366
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1061/50000 (2.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 252.3082s / 12653.3630 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0756
env0_second_0:                 episode reward: 0.1500,                 loss: 3.0327
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1081/50000 (2.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 253.5212s / 12906.8843 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0821
env0_second_0:                 episode reward: -0.0500,                 loss: 3.2143
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1101/50000 (2.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 252.5217s / 13159.4060 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0895
env0_second_0:                 episode reward: 0.1000,                 loss: 3.5694
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1121/50000 (2.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 254.5004s / 13413.9063 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.1064
env0_second_0:                 episode reward: 0.4000,                 loss: 3.8980
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1141/50000 (2.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 251.6780s / 13665.5844 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.1155
env0_second_0:                 episode reward: 0.2000,                 loss: 4.1659
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1161/50000 (2.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 253.3579s / 13918.9422 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.1227
env0_second_0:                 episode reward: 0.4000,                 loss: 4.3366
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1181/50000 (2.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 254.2009s / 14173.1431 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.1342
env0_second_0:                 episode reward: 0.0000,                 loss: 4.3981
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1201/50000 (2.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 256.6777s / 14429.8208 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.1541
env0_second_0:                 episode reward: -0.3500,                 loss: 4.1906
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1221/50000 (2.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 253.3652s / 14683.1861 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1607
env0_second_0:                 episode reward: 0.3000,                 loss: 4.1574
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1241/50000 (2.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 253.0210s / 14936.2070 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1713
env0_second_0:                 episode reward: -0.0500,                 loss: 4.0834
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1261/50000 (2.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 255.3787s / 15191.5857 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1821
env0_second_0:                 episode reward: 0.0500,                 loss: 4.1134
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1281/50000 (2.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 251.9572s / 15443.5429 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.1938
env0_second_0:                 episode reward: 0.0000,                 loss: 3.9648
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 1301/50000 (2.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 257.7202s / 15701.2631 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.1982
env0_second_0:                 episode reward: -0.1500,                 loss: 4.0082
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1321/50000 (2.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 253.5242s / 15954.7873 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2119
env0_second_0:                 episode reward: -0.0500,                 loss: 3.7524
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1341/50000 (2.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 254.8200s / 16209.6072 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2256
env0_second_0:                 episode reward: -0.1500,                 loss: 3.6108
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1361/50000 (2.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 252.1488s / 16461.7560 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2253
env0_second_0:                 episode reward: -0.1500,                 loss: 3.5273
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1381/50000 (2.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 252.8734s / 16714.6294 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2267
env0_second_0:                 episode reward: 0.0500,                 loss: 3.6610
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 1401/50000 (2.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 251.3742s / 16966.0036 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2253
env0_second_0:                 episode reward: 0.0500,                 loss: 3.5751
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1421/50000 (2.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 255.1483s / 17221.1520 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2068
env0_second_0:                 episode reward: 0.1000,                 loss: 3.4714
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1441/50000 (2.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 256.4090s / 17477.5610 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1932
env0_second_0:                 episode reward: -0.0500,                 loss: 3.3489
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1461/50000 (2.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 255.6200s / 17733.1810 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1874
env0_second_0:                 episode reward: 0.1000,                 loss: 3.1923
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 1481/50000 (2.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 254.8464s / 17988.0275 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.1908
env0_second_0:                 episode reward: -0.2000,                 loss: 3.2575
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1501/50000 (3.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 251.5982s / 18239.6256 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.1840
env0_second_0:                 episode reward: -0.1000,                 loss: 3.2855
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1521/50000 (3.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 249.5851s / 18489.2108 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.1776
env0_second_0:                 episode reward: -0.2000,                 loss: 3.3502
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1541/50000 (3.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 250.3986s / 18739.6094 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1698
env0_second_0:                 episode reward: -0.0500,                 loss: 3.4848
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1561/50000 (3.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 251.0691s / 18990.6785 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1755
env0_second_0:                 episode reward: 0.0500,                 loss: 3.5881
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1581/50000 (3.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 255.3197s / 19245.9982 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.1778
env0_second_0:                 episode reward: -0.5500,                 loss: 3.5982
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1601/50000 (3.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 255.1329s / 19501.1311 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1876
env0_second_0:                 episode reward: 0.0500,                 loss: 3.6970
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1621/50000 (3.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 255.8035s / 19756.9346 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.1799
env0_second_0:                 episode reward: -0.7500,                 loss: 3.8821
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1641/50000 (3.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 251.4634s / 20008.3980 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.1726
env0_second_0:                 episode reward: 0.1500,                 loss: 3.9745
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1661/50000 (3.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 250.3955s / 20258.7935 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.1633
env0_second_0:                 episode reward: 0.1500,                 loss: 4.2481
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1681/50000 (3.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 255.7891s / 20514.5826 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1545
env0_second_0:                 episode reward: 0.5000,                 loss: 4.2884
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1701/50000 (3.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 255.2613s / 20769.8438 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.1515
env0_second_0:                 episode reward: 0.1500,                 loss: 4.2445
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 1721/50000 (3.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 256.0952s / 21025.9390 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.1466
env0_second_0:                 episode reward: 0.2000,                 loss: 4.0958
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1741/50000 (3.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 254.8228s / 21280.7619 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.1453
env0_second_0:                 episode reward: 0.8500,                 loss: 4.0472
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1761/50000 (3.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 249.0208s / 21529.7827 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1574
env0_second_0:                 episode reward: 0.5000,                 loss: 3.8932
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1781/50000 (3.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 250.7805s / 21780.5632 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1514
env0_second_0:                 episode reward: 0.3000,                 loss: 3.9100
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1801/50000 (3.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 256.8273s / 22037.3905 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1648
env0_second_0:                 episode reward: 0.3000,                 loss: 3.9467
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1821/50000 (3.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 254.4962s / 22291.8867 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.1682
env0_second_0:                 episode reward: 0.0000,                 loss: 3.7851
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1841/50000 (3.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 252.4976s / 22544.3843 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.1770
env0_second_0:                 episode reward: -0.1500,                 loss: 3.4025
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1861/50000 (3.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 249.2722s / 22793.6565 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.1845
env0_second_0:                 episode reward: 0.2500,                 loss: 3.0598
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1881/50000 (3.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 256.9506s / 23050.6071 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.1827
env0_second_0:                 episode reward: 0.2000,                 loss: 2.7550
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1901/50000 (3.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 250.2941s / 23300.9012 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.1859
env0_second_0:                 episode reward: 0.6000,                 loss: 2.4714
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1921/50000 (3.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 254.9203s / 23555.8215 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.1959
env0_second_0:                 episode reward: 0.4500,                 loss: 2.2756
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1941/50000 (3.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 253.8434s / 23809.6649 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2110
env0_second_0:                 episode reward: 0.2000,                 loss: 2.1172
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1961/50000 (3.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 255.1360s / 24064.8009 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2068
env0_second_0:                 episode reward: 0.2000,                 loss: 1.9585
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1981/50000 (3.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 254.9166s / 24319.7175 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.1932
env0_second_0:                 episode reward: -0.2000,                 loss: 1.9977
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2001/50000 (4.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 254.9401s / 24574.6576 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.1806
env0_second_0:                 episode reward: -0.2000,                 loss: 1.8115
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2021/50000 (4.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 252.5998s / 24827.2573 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.1819
env0_second_0:                 episode reward: 0.0000,                 loss: 1.7787
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2041/50000 (4.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 249.9828s / 25077.2401 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1659
env0_second_0:                 episode reward: 0.3000,                 loss: 1.7616
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2061/50000 (4.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 249.1246s / 25326.3648 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.1628
env0_second_0:                 episode reward: 0.6500,                 loss: 1.8274
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2081/50000 (4.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 252.5138s / 25578.8786 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.1616
env0_second_0:                 episode reward: 0.2000,                 loss: 1.8616
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 2101/50000 (4.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 253.9370s / 25832.8156 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.1564
env0_second_0:                 episode reward: -0.3000,                 loss: 1.9110
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 2121/50000 (4.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 254.4799s / 26087.2955 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.1488
env0_second_0:                 episode reward: 0.2500,                 loss: 2.0594
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2141/50000 (4.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 255.2690s / 26342.5645 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.1430
env0_second_0:                 episode reward: -0.1000,                 loss: 2.1339
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2161/50000 (4.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 254.5298s / 26597.0943 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.1355
env0_second_0:                 episode reward: 0.6500,                 loss: 2.4103
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 2181/50000 (4.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 256.4995s / 26853.5938 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.1377
env0_second_0:                 episode reward: 0.1500,                 loss: 2.8535
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2201/50000 (4.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 256.5558s / 27110.1496 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.1350
env0_second_0:                 episode reward: 0.9500,                 loss: 3.3590
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2221/50000 (4.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 251.3636s / 27361.5132 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1275
env0_second_0:                 episode reward: 0.0500,                 loss: 3.6290
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2241/50000 (4.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 253.1182s / 27614.6314 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.1234
env0_second_0:                 episode reward: 0.4500,                 loss: 4.0164
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2261/50000 (4.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 253.9266s / 27868.5580 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.1244
env0_second_0:                 episode reward: 0.0000,                 loss: 4.7258
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 2281/50000 (4.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 253.5850s / 28122.1430 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1258
env0_second_0:                 episode reward: 0.3000,                 loss: 5.6651
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2301/50000 (4.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 255.8469s / 28377.9899 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.1227
env0_second_0:                 episode reward: -0.1000,                 loss: 6.2627
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2321/50000 (4.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 254.2744s / 28632.2643 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.1226
env0_second_0:                 episode reward: 0.7000,                 loss: 7.5186
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2341/50000 (4.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 253.3977s / 28885.6620 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.1263
env0_second_0:                 episode reward: 0.6500,                 loss: 8.0575
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2361/50000 (4.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 251.6039s / 29137.2659 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.1254
env0_second_0:                 episode reward: 0.7500,                 loss: 8.8858
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2381/50000 (4.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 253.9699s / 29391.2358 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.1252
env0_second_0:                 episode reward: 0.4000,                 loss: 9.4839
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2401/50000 (4.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 251.7282s / 29642.9640 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.1342
env0_second_0:                 episode reward: 0.6500,                 loss: 10.6302
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 2421/50000 (4.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 250.8889s / 29893.8529 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1320
env0_second_0:                 episode reward: 0.0500,                 loss: 10.4884
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2441/50000 (4.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 251.0303s / 30144.8832 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1265
env0_second_0:                 episode reward: 0.5000,                 loss: 10.4954
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2461/50000 (4.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 254.2838s / 30399.1671 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.1158
env0_second_0:                 episode reward: 1.5000,                 loss: 9.9679
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 2481/50000 (4.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 251.6506s / 30650.8177 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.1120
env0_second_0:                 episode reward: 1.6500,                 loss: 9.5809
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2501/50000 (5.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 254.7477s / 30905.5655 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1109
env0_second_0:                 episode reward: 0.5000,                 loss: 9.0483
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2521/50000 (5.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 254.6717s / 31160.2372 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.1124
env0_second_0:                 episode reward: 1.3500,                 loss: 8.1015
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2541/50000 (5.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 256.2607s / 31416.4979 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.1119
env0_second_0:                 episode reward: 0.4000,                 loss: 7.7288
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 2561/50000 (5.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 258.8936s / 31675.3916 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.1147
env0_second_0:                 episode reward: 0.5500,                 loss: 7.1011
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 2581/50000 (5.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 255.6707s / 31931.0623 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.1148
env0_second_0:                 episode reward: 0.7500,                 loss: 6.9045
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2601/50000 (5.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 256.2236s / 32187.2859 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1127
env0_second_0:                 episode reward: 0.3000,                 loss: 6.4537
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2621/50000 (5.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 253.8847s / 32441.1706 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.1064
env0_second_0:                 episode reward: 0.5500,                 loss: 5.9918
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 2641/50000 (5.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 253.2704s / 32694.4410 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.1012
env0_second_0:                 episode reward: 0.8000,                 loss: 5.9196
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2661/50000 (5.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 251.2528s / 32945.6938 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.1031
env0_second_0:                 episode reward: -0.1500,                 loss: 5.5662
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2681/50000 (5.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 250.8968s / 33196.5905 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0983
env0_second_0:                 episode reward: 0.5000,                 loss: 5.6815
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2701/50000 (5.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 252.6476s / 33449.2382 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0949
env0_second_0:                 episode reward: 0.7500,                 loss: 6.1782
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2721/50000 (5.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 255.0486s / 33704.2868 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0951
env0_second_0:                 episode reward: -0.2500,                 loss: 7.9689
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2741/50000 (5.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 256.5868s / 33960.8736 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0989
env0_second_0:                 episode reward: 0.4000,                 loss: 10.4061
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2761/50000 (5.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 257.4053s / 34218.2789 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.1048
env0_second_0:                 episode reward: -0.2500,                 loss: 11.0253
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2781/50000 (5.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 254.7646s / 34473.0435 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.1084
env0_second_0:                 episode reward: 0.6500,                 loss: 10.3544
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2801/50000 (5.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 258.5850s / 34731.6286 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.1205
env0_second_0:                 episode reward: 0.3500,                 loss: 14.4639
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2821/50000 (5.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 259.3597s / 34990.9883 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1260
env0_second_0:                 episode reward: 0.0500,                 loss: 14.4383
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2841/50000 (5.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 256.6706s / 35247.6588 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.1324
env0_second_0:                 episode reward: -0.1000,                 loss: 20.1720
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 2861/50000 (5.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 255.0799s / 35502.7388 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.1434
env0_second_0:                 episode reward: -0.4500,                 loss: 20.0573
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2881/50000 (5.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 256.4740s / 35759.2128 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.1523
env0_second_0:                 episode reward: 0.2000,                 loss: 17.6364
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2901/50000 (5.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 257.2550s / 36016.4678 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1551
env0_second_0:                 episode reward: 0.1000,                 loss: 14.7313
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2921/50000 (5.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 255.9504s / 36272.4182 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1843
env0_second_0:                 episode reward: -0.0500,                 loss: 14.6704
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2941/50000 (5.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 266.2966s / 36538.7148 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1978
env0_second_0:                 episode reward: 0.1000,                 loss: 13.8477
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2961/50000 (5.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 266.1868s / 36804.9016 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2099
env0_second_0:                 episode reward: 0.0500,                 loss: 14.8977
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 2981/50000 (5.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 264.9847s / 37069.8863 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.2339
env0_second_0:                 episode reward: 0.5500,                 loss: 16.2729
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3001/50000 (6.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 266.4150s / 37336.3013 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.2454
env0_second_0:                 episode reward: 0.6000,                 loss: 16.3569
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3021/50000 (6.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 266.1282s / 37602.4295 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.2542
env0_second_0:                 episode reward: -0.3500,                 loss: 15.7272
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3041/50000 (6.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 265.4978s / 37867.9273 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.2584
env0_second_0:                 episode reward: 0.3000,                 loss: 15.1066
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3061/50000 (6.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 268.0826s / 38136.0099 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.2508
env0_second_0:                 episode reward: 0.9500,                 loss: 14.3437
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3081/50000 (6.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 270.9977s / 38407.0077 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.2563
env0_second_0:                 episode reward: 0.5000,                 loss: 12.6710
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3101/50000 (6.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 270.7371s / 38677.7447 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.2462
env0_second_0:                 episode reward: 1.0000,                 loss: 10.7335
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3121/50000 (6.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 270.2813s / 38948.0260 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.2365
env0_second_0:                 episode reward: 2.0000,                 loss: 9.0281
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 3141/50000 (6.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 271.0799s / 39219.1059 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.2299
env0_second_0:                 episode reward: -0.2000,                 loss: 8.4578
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 3161/50000 (6.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 270.0464s / 39489.1523 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.2131
env0_second_0:                 episode reward: 1.1500,                 loss: 7.7814
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 3181/50000 (6.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 270.3782s / 39759.5306 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1903
env0_second_0:                 episode reward: -0.0500,                 loss: 7.2410
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3201/50000 (6.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 267.8258s / 40027.3564 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.1841
env0_second_0:                 episode reward: 1.0000,                 loss: 6.3988
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 3221/50000 (6.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 268.2582s / 40295.6146 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.1546
env0_second_0:                 episode reward: 0.2000,                 loss: 6.0851
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3241/50000 (6.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 267.2411s / 40562.8557 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1430
env0_second_0:                 episode reward: 0.3000,                 loss: 5.5058
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3261/50000 (6.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 265.9325s / 40828.7881 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.1458
env0_second_0:                 episode reward: 0.3500,                 loss: 5.4053
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3281/50000 (6.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 267.2621s / 41096.0502 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.1434
env0_second_0:                 episode reward: 0.0000,                 loss: 5.3274
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 3301/50000 (6.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 268.6441s / 41364.6943 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.1427
env0_second_0:                 episode reward: 0.1500,                 loss: 5.0094
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3321/50000 (6.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 268.7020s / 41633.3963 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1354
env0_second_0:                 episode reward: 0.3000,                 loss: 4.8251
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3341/50000 (6.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 269.5203s / 41902.9166 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.1172
env0_second_0:                 episode reward: 0.2500,                 loss: 4.7412
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3361/50000 (6.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 267.6456s / 42170.5622 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.1102
env0_second_0:                 episode reward: -0.1000,                 loss: 4.9670
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3381/50000 (6.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 268.3282s / 42438.8904 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.1042
env0_second_0:                 episode reward: 0.5500,                 loss: 4.7798
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3401/50000 (6.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 267.8772s / 42706.7676 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.1070
env0_second_0:                 episode reward: 0.0000,                 loss: 4.5135
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3421/50000 (6.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 269.4381s / 42976.2056 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.1046
env0_second_0:                 episode reward: -0.1500,                 loss: 4.5346
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 3441/50000 (6.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 266.6632s / 43242.8689 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1005
env0_second_0:                 episode reward: 0.1000,                 loss: 4.1716
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3461/50000 (6.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 267.1709s / 43510.0397 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1041
env0_second_0:                 episode reward: -0.0500,                 loss: 4.4469
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3481/50000 (6.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 266.7101s / 43776.7499 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0991
env0_second_0:                 episode reward: 0.1500,                 loss: 4.5734
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3501/50000 (7.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 267.0743s / 44043.8242 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0932
env0_second_0:                 episode reward: 0.6000,                 loss: 3.9883
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3521/50000 (7.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 267.3455s / 44311.1697 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0867
env0_second_0:                 episode reward: 0.0500,                 loss: 3.7610
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3541/50000 (7.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 265.2759s / 44576.4457 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0804
env0_second_0:                 episode reward: 0.4500,                 loss: 3.6141
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3561/50000 (7.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 268.5949s / 44845.0406 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0820
env0_second_0:                 episode reward: 0.4500,                 loss: 3.5038
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3581/50000 (7.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 266.6805s / 45111.7212 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0757
env0_second_0:                 episode reward: -0.0500,                 loss: 3.2009
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3601/50000 (7.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 268.0971s / 45379.8183 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0725
env0_second_0:                 episode reward: 0.2000,                 loss: 3.3171
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3621/50000 (7.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 268.4591s / 45648.2774 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0674
env0_second_0:                 episode reward: 0.2000,                 loss: 3.2169
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3641/50000 (7.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 268.4400s / 45916.7174 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0632
env0_second_0:                 episode reward: -0.2000,                 loss: 3.2366
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3661/50000 (7.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 268.9139s / 46185.6313 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0636
env0_second_0:                 episode reward: 0.0500,                 loss: 3.2375
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3681/50000 (7.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 269.1701s / 46454.8014 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0608
env0_second_0:                 episode reward: 0.1500,                 loss: 3.3197
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3701/50000 (7.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 269.2620s / 46724.0634 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0574
env0_second_0:                 episode reward: -0.3000,                 loss: 3.3107
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3721/50000 (7.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 269.4483s / 46993.5117 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0543
env0_second_0:                 episode reward: -0.0500,                 loss: 3.3289
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3741/50000 (7.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 268.7871s / 47262.2988 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0559
env0_second_0:                 episode reward: 0.3000,                 loss: 3.2472
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3761/50000 (7.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 263.8964s / 47526.1952 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0589
env0_second_0:                 episode reward: 0.4500,                 loss: 3.0011
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3781/50000 (7.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 270.4307s / 47796.6259 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0565
env0_second_0:                 episode reward: 0.2000,                 loss: 2.8350
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3801/50000 (7.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 267.4129s / 48064.0388 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0565
env0_second_0:                 episode reward: 0.0500,                 loss: 2.6120
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3821/50000 (7.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 269.6910s / 48333.7299 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0595
env0_second_0:                 episode reward: 0.7000,                 loss: 2.5199
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 3841/50000 (7.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 268.2927s / 48602.0225 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0595
env0_second_0:                 episode reward: 0.0500,                 loss: 2.2666
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3861/50000 (7.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 267.4811s / 48869.5036 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0618
env0_second_0:                 episode reward: 0.8000,                 loss: 2.1074
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3881/50000 (7.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 268.6306s / 49138.1342 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0611
env0_second_0:                 episode reward: -0.4500,                 loss: 1.9364
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3901/50000 (7.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 270.0122s / 49408.1465 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0616
env0_second_0:                 episode reward: 0.3500,                 loss: 1.8616
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3921/50000 (7.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 269.8364s / 49677.9829 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0604
env0_second_0:                 episode reward: 0.3500,                 loss: 1.8105
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3941/50000 (7.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 262.8218s / 49940.8047 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0603
env0_second_0:                 episode reward: -0.2000,                 loss: 1.7932
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3961/50000 (7.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 266.8644s / 50207.6691 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0593
env0_second_0:                 episode reward: 0.4000,                 loss: 1.7583
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3981/50000 (7.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 267.2106s / 50474.8797 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0582
env0_second_0:                 episode reward: 0.1500,                 loss: 1.8183
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4001/50000 (8.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 266.4126s / 50741.2923 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0576
env0_second_0:                 episode reward: 0.6500,                 loss: 1.8564
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4021/50000 (8.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 267.4665s / 51008.7588 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0577
env0_second_0:                 episode reward: 0.5000,                 loss: 1.8413
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4041/50000 (8.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 268.6174s / 51277.3762 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0561
env0_second_0:                 episode reward: 0.8500,                 loss: 1.8720
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 4061/50000 (8.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 269.7938s / 51547.1701 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0567
env0_second_0:                 episode reward: 0.2000,                 loss: 1.9967
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4081/50000 (8.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 266.9201s / 51814.0902 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0561
env0_second_0:                 episode reward: 0.2000,                 loss: 1.9526
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4101/50000 (8.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 267.8887s / 52081.9789 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0538
env0_second_0:                 episode reward: 0.0000,                 loss: 2.0935
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4121/50000 (8.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 266.2680s / 52348.2468 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0533
env0_second_0:                 episode reward: -0.1500,                 loss: 2.0337
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 4141/50000 (8.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 265.8709s / 52614.1177 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0534
env0_second_0:                 episode reward: -0.2000,                 loss: 2.0260
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 4161/50000 (8.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 265.5626s / 52879.6803 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0594
env0_second_0:                 episode reward: 0.1000,                 loss: 2.1752
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 4181/50000 (8.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 264.4709s / 53144.1513 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0606
env0_second_0:                 episode reward: -0.2000,                 loss: 2.3874
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4201/50000 (8.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 267.0916s / 53411.2428 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0651
env0_second_0:                 episode reward: -0.3500,                 loss: 2.5346
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 4221/50000 (8.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 268.6219s / 53679.8647 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0697
env0_second_0:                 episode reward: 0.3000,                 loss: 2.5543
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 4241/50000 (8.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 266.4519s / 53946.3167 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0725
env0_second_0:                 episode reward: -0.1500,                 loss: 2.6115
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 4261/50000 (8.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 263.8895s / 54210.2062 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0736
env0_second_0:                 episode reward: 0.5500,                 loss: 2.7513
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4281/50000 (8.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 265.2667s / 54475.4729 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0673
env0_second_0:                 episode reward: 0.3000,                 loss: 2.8419
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4301/50000 (8.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 264.1047s / 54739.5776 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0665
env0_second_0:                 episode reward: 0.0000,                 loss: 3.1730
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4321/50000 (8.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 267.3188s / 55006.8963 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0670
env0_second_0:                 episode reward: 0.1000,                 loss: 3.6602
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4341/50000 (8.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 266.1102s / 55273.0065 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0639
env0_second_0:                 episode reward: 0.1500,                 loss: 3.7561
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 4361/50000 (8.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 265.7516s / 55538.7581 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0633
env0_second_0:                 episode reward: 0.2500,                 loss: 3.8079
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4381/50000 (8.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 268.3932s / 55807.1514 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0595
env0_second_0:                 episode reward: 0.1500,                 loss: 4.1312
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 4401/50000 (8.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 265.2612s / 56072.4125 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0561
env0_second_0:                 episode reward: -0.0500,                 loss: 4.3794
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 4421/50000 (8.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 263.4227s / 56335.8353 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0531
env0_second_0:                 episode reward: 0.6500,                 loss: 4.3108
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4441/50000 (8.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 263.9042s / 56599.7395 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0498
env0_second_0:                 episode reward: -0.1500,                 loss: 4.1146
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 4461/50000 (8.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 264.0405s / 56863.7800 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0454
env0_second_0:                 episode reward: -0.1000,                 loss: 4.1101
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 4481/50000 (8.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 265.1187s / 57128.8988 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0432
env0_second_0:                 episode reward: 0.1000,                 loss: 4.1715
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 4501/50000 (9.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 263.9370s / 57392.8358 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0471
env0_second_0:                 episode reward: 0.3500,                 loss: 4.2183
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 4521/50000 (9.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 267.1654s / 57660.0012 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0444
env0_second_0:                 episode reward: 0.1500,                 loss: 4.5515
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 4541/50000 (9.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 266.1422s / 57926.1434 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0392
env0_second_0:                 episode reward: 0.1000,                 loss: 4.3997
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 4561/50000 (9.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 265.0931s / 58191.2365 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0360
env0_second_0:                 episode reward: -0.0500,                 loss: 4.5115
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4581/50000 (9.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 264.4452s / 58455.6817 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0349
env0_second_0:                 episode reward: 0.2000,                 loss: 5.6075
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 4601/50000 (9.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 269.2377s / 58724.9195 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0359
env0_second_0:                 episode reward: 0.0500,                 loss: 7.2797
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 4621/50000 (9.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 265.7641s / 58990.6835 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0341
env0_second_0:                 episode reward: -0.3500,                 loss: 7.8726
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 4641/50000 (9.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 268.6785s / 59259.3620 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0316
env0_second_0:                 episode reward: -0.3000,                 loss: 7.9153
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 4661/50000 (9.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 267.5325s / 59526.8945 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0328
env0_second_0:                 episode reward: -0.1000,                 loss: 8.4892
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4681/50000 (9.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 266.9607s / 59793.8553 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0337
env0_second_0:                 episode reward: 0.1000,                 loss: 11.5256
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 4701/50000 (9.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 268.0971s / 60061.9523 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0353
env0_second_0:                 episode reward: 0.3500,                 loss: 10.8719
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 4721/50000 (9.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 267.0750s / 60329.0273 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0376
env0_second_0:                 episode reward: 0.2000,                 loss: 8.5507
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 4741/50000 (9.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 261.4376s / 60590.4649 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0395
env0_second_0:                 episode reward: 1.0500,                 loss: 8.5786
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4761/50000 (9.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 253.8979s / 60844.3628 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0464
env0_second_0:                 episode reward: 0.2000,                 loss: 15.2278
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 4781/50000 (9.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 255.2264s / 61099.5892 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0526
env0_second_0:                 episode reward: -0.1000,                 loss: 19.4845
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 4801/50000 (9.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 253.0371s / 61352.6263 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0572
env0_second_0:                 episode reward: 0.5000,                 loss: 17.3779
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 4821/50000 (9.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 253.6958s / 61606.3222 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0634
env0_second_0:                 episode reward: 0.6000,                 loss: 15.0367
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 4841/50000 (9.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 252.4597s / 61858.7819 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0785
env0_second_0:                 episode reward: -0.1000,                 loss: 19.7512
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 4861/50000 (9.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 252.2263s / 62111.0082 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0892
env0_second_0:                 episode reward: 0.4000,                 loss: 18.8446
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 4881/50000 (9.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 252.6376s / 62363.6457 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0971
env0_second_0:                 episode reward: 0.4000,                 loss: 20.4661
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 4901/50000 (9.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 252.9146s / 62616.5603 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1118
env0_second_0:                 episode reward: 0.0500,                 loss: 18.6925
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 4921/50000 (9.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 253.7980s / 62870.3583 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1035
env0_second_0:                 episode reward: -0.0500,                 loss: 18.6170
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4941/50000 (9.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 252.5577s / 63122.9160 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1003
env0_second_0:                 episode reward: 0.5000,                 loss: 19.9340
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 4961/50000 (9.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 252.3975s / 63375.3136 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0939
env0_second_0:                 episode reward: 0.2500,                 loss: 24.1990
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4981/50000 (9.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 252.8501s / 63628.1637 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0970
env0_second_0:                 episode reward: -0.1500,                 loss: 22.9580
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5001/50000 (10.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 252.3658s / 63880.5294 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0938
env0_second_0:                 episode reward: 0.1000,                 loss: 27.5881
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 5021/50000 (10.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 251.8036s / 64132.3330 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0921
env0_second_0:                 episode reward: 0.1500,                 loss: 33.1760
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5041/50000 (10.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 253.8088s / 64386.1418 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0788
env0_second_0:                 episode reward: 0.3000,                 loss: 34.1655
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5061/50000 (10.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 252.8516s / 64638.9934 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0732
env0_second_0:                 episode reward: -0.3000,                 loss: 42.0244
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 5081/50000 (10.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 253.9028s / 64892.8961 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0727
env0_second_0:                 episode reward: 1.2000,                 loss: 45.0850
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5101/50000 (10.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 252.1561s / 65145.0522 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0703
env0_second_0:                 episode reward: 0.1000,                 loss: 44.8506
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 5121/50000 (10.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 254.1426s / 65399.1948 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0686
env0_second_0:                 episode reward: 0.1000,                 loss: 46.4288
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5141/50000 (10.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 253.5154s / 65652.7102 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0671
env0_second_0:                 episode reward: 0.4000,                 loss: 56.7382
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 5161/50000 (10.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 254.1905s / 65906.9006 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0641
env0_second_0:                 episode reward: -0.2000,                 loss: 55.9070
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 5181/50000 (10.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 254.5562s / 66161.4568 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0561
env0_second_0:                 episode reward: -0.2000,                 loss: 62.7320
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 5201/50000 (10.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 254.3860s / 66415.8428 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0611
env0_second_0:                 episode reward: 0.5000,                 loss: 63.9209
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 5221/50000 (10.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 253.4082s / 66669.2510 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0589
env0_second_0:                 episode reward: 0.2000,                 loss: 85.9747
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 5241/50000 (10.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 254.2603s / 66923.5113 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0560
env0_second_0:                 episode reward: -0.1000,                 loss: 78.7844
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 5261/50000 (10.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 253.4636s / 67176.9749 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0533
env0_second_0:                 episode reward: 0.0000,                 loss: 90.4522
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 5281/50000 (10.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 254.2628s / 67431.2377 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0495
env0_second_0:                 episode reward: 0.5000,                 loss: 105.2021
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 5301/50000 (10.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 252.8046s / 67684.0423 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0533
env0_second_0:                 episode reward: 0.2500,                 loss: 108.3014
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5321/50000 (10.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 252.3768s / 67936.4190 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0587
env0_second_0:                 episode reward: -0.0500,                 loss: 115.0550
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 5341/50000 (10.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 250.0737s / 68186.4927 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0558
env0_second_0:                 episode reward: 0.5000,                 loss: 125.5670
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 5361/50000 (10.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 251.8656s / 68438.3583 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0537
env0_second_0:                 episode reward: 0.1500,                 loss: 112.4635
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 5381/50000 (10.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 253.0047s / 68691.3630 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0542
env0_second_0:                 episode reward: 0.3000,                 loss: 120.6633
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 5401/50000 (10.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 253.8777s / 68945.2407 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0593
env0_second_0:                 episode reward: 0.2500,                 loss: 155.8875
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 5421/50000 (10.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 255.0559s / 69200.2966 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0662
env0_second_0:                 episode reward: 0.0000,                 loss: 132.3036
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 5441/50000 (10.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 254.4990s / 69454.7956 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0678
env0_second_0:                 episode reward: 0.0500,                 loss: 158.7889
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 5461/50000 (10.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 251.9066s / 69706.7022 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0716
env0_second_0:                 episode reward: 0.0000,                 loss: 194.4534
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 5481/50000 (10.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 253.3908s / 69960.0930 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0716
env0_second_0:                 episode reward: 0.5000,                 loss: 186.2955
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5501/50000 (11.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 254.5104s / 70214.6034 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0764
env0_second_0:                 episode reward: 0.1500,                 loss: 193.3052
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5521/50000 (11.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 253.9876s / 70468.5910 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0782
env0_second_0:                 episode reward: 0.5500,                 loss: 176.4655
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 5541/50000 (11.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 255.7259s / 70724.3169 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0871
env0_second_0:                 episode reward: 0.0000,                 loss: 196.1271
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5561/50000 (11.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 253.4635s / 70977.7804 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0820
env0_second_0:                 episode reward: 0.7500,                 loss: 210.2373
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 5581/50000 (11.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 252.6446s / 71230.4250 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0775
env0_second_0:                 episode reward: 0.0000,                 loss: 214.6247
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5601/50000 (11.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 251.2544s / 71481.6795 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0760
env0_second_0:                 episode reward: -0.0500,                 loss: 269.2847
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5621/50000 (11.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 252.7004s / 71734.3799 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0897
env0_second_0:                 episode reward: 0.4500,                 loss: 254.4457
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5641/50000 (11.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 253.3850s / 71987.7648 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1037
env0_second_0:                 episode reward: -0.0500,                 loss: 249.6905
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 5661/50000 (11.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 251.8879s / 72239.6527 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1246
env0_second_0:                 episode reward: 0.3000,                 loss: 348.4300
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 5681/50000 (11.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 252.7793s / 72492.4321 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.1397
env0_second_0:                 episode reward: 0.4500,                 loss: 301.2430
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 5701/50000 (11.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 252.4604s / 72744.8924 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1677
env0_second_0:                 episode reward: 0.0500,                 loss: 350.4373
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 5721/50000 (11.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 254.9217s / 72999.8141 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.2068
env0_second_0:                 episode reward: 0.7000,                 loss: 320.5626
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 5741/50000 (11.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 256.2651s / 73256.0792 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.2499
env0_second_0:                 episode reward: 0.3500,                 loss: 434.5932
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 5761/50000 (11.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 255.2810s / 73511.3602 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.2994
env0_second_0:                 episode reward: 0.4500,                 loss: 384.5416
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5781/50000 (11.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 251.2123s / 73762.5725 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.3311
env0_second_0:                 episode reward: 2.5500,                 loss: 410.9839
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5801/50000 (11.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 252.1085s / 74014.6810 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.3648
env0_second_0:                 episode reward: 0.9500,                 loss: 533.1758
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 5821/50000 (11.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 251.5971s / 74266.2781 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.3897
env0_second_0:                 episode reward: 1.5500,                 loss: 481.7367
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 5841/50000 (11.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 250.1123s / 74516.3904 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.3696
env0_second_0:                 episode reward: 0.7000,                 loss: 573.6390
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 5861/50000 (11.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 251.8209s / 74768.2113 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.3647
env0_second_0:                 episode reward: 0.6000,                 loss: 543.6574
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 5881/50000 (11.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 255.0398s / 75023.2511 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.3687
env0_second_0:                 episode reward: -0.0500,                 loss: 701.0218
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 5901/50000 (11.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 253.0412s / 75276.2923 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.3861
env0_second_0:                 episode reward: 0.6500,                 loss: 683.1676
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5921/50000 (11.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 252.5402s / 75528.8325 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.3890
env0_second_0:                 episode reward: 0.0500,                 loss: 862.2571
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 5941/50000 (11.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 252.7882s / 75781.6206 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.4026
env0_second_0:                 episode reward: 1.6000,                 loss: 1052.5103
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5961/50000 (11.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 252.7319s / 76034.3526 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.3868
env0_second_0:                 episode reward: 0.3500,                 loss: 815.8951
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5981/50000 (11.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 251.8105s / 76286.1630 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.3545
env0_second_0:                 episode reward: 0.6000,                 loss: 947.3259
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 6001/50000 (12.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 252.0362s / 76538.1992 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.3057
env0_second_0:                 episode reward: -0.0500,                 loss: 962.7400
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 6021/50000 (12.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 253.9123s / 76792.1115 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.2684
env0_second_0:                 episode reward: 0.0000,                 loss: 904.2994
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 6041/50000 (12.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 251.3027s / 77043.4143 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2382
env0_second_0:                 episode reward: -0.0500,                 loss: 928.8793
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 6061/50000 (12.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 253.9997s / 77297.4140 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.2131
env0_second_0:                 episode reward: 0.4500,                 loss: 1021.7645
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 6081/50000 (12.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 254.0222s / 77551.4362 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.1919
env0_second_0:                 episode reward: 0.0000,                 loss: 1024.2660
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 6101/50000 (12.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 253.0197s / 77804.4559 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1897
env0_second_0:                 episode reward: 0.1000,                 loss: 1052.2844
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 6121/50000 (12.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 251.1687s / 78055.6247 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1684
env0_second_0:                 episode reward: 0.1000,                 loss: 1152.5788
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 6141/50000 (12.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 251.5057s / 78307.1304 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.1528
env0_second_0:                 episode reward: 0.0000,                 loss: 1199.2950
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 6161/50000 (12.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 253.1345s / 78560.2649 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1376
env0_second_0:                 episode reward: -0.0500,                 loss: 1201.8641
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 6181/50000 (12.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 254.7756s / 78815.0405 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1257
env0_second_0:                 episode reward: 0.1000,                 loss: 1303.0966
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 6201/50000 (12.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 254.3972s / 79069.4377 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.1119
env0_second_0:                 episode reward: -0.1500,                 loss: 1483.6798
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 6221/50000 (12.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 264.1950s / 79333.6327 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1036
env0_second_0:                 episode reward: 0.0500,                 loss: 1688.6694
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6241/50000 (12.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 269.8396s / 79603.4723 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0990
env0_second_0:                 episode reward: 0.1500,                 loss: 1818.4143
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 6261/50000 (12.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 270.6517s / 79874.1240 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0981
env0_second_0:                 episode reward: 0.0000,                 loss: 1892.2015
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 6281/50000 (12.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 267.6513s / 80141.7753 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0959
env0_second_0:                 episode reward: 0.0500,                 loss: 2047.4613
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 6301/50000 (12.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 287.6921s / 80429.4674 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0952
env0_second_0:                 episode reward: 0.3000,                 loss: 2350.3465
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 6321/50000 (12.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 293.7574s / 80723.2248 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0931
env0_second_0:                 episode reward: 0.0000,                 loss: 2259.1867
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 6341/50000 (12.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 293.8299s / 81017.0547 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.1064
env0_second_0:                 episode reward: 0.1500,                 loss: 2423.7545
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 6361/50000 (12.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 291.4961s / 81308.5509 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1110
env0_second_0:                 episode reward: -0.0500,                 loss: 2483.1866
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 6381/50000 (12.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 290.5094s / 81599.0603 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.1073
env0_second_0:                 episode reward: 0.9500,                 loss: 2499.0337
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 6401/50000 (12.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 289.9151s / 81888.9754 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.1120
env0_second_0:                 episode reward: 0.7500,                 loss: 2577.9245
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 6421/50000 (12.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 288.2105s / 82177.1859 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.1176
env0_second_0:                 episode reward: 0.6500,                 loss: 2641.2573
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 6441/50000 (12.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 288.5690s / 82465.7549 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.1216
env0_second_0:                 episode reward: 1.0500,                 loss: 2847.0831
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 6461/50000 (12.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 291.8293s / 82757.5842 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.1296
env0_second_0:                 episode reward: 1.7500,                 loss: 2959.4738
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 6481/50000 (12.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 292.5036s / 83050.0878 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.1329
env0_second_0:                 episode reward: 1.3000,                 loss: 2926.7735
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 6501/50000 (13.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 288.8963s / 83338.9841 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.1272
env0_second_0:                 episode reward: 0.5500,                 loss: 3005.4107
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 6521/50000 (13.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 290.7911s / 83629.7752 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.1188
env0_second_0:                 episode reward: 1.0500,                 loss: 3075.7441
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 6541/50000 (13.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 291.6734s / 83921.4486 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1177
env0_second_0:                 episode reward: 1.1000,                 loss: 2959.5863
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 6561/50000 (13.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 292.4829s / 84213.9315 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.1309
env0_second_0:                 episode reward: 0.1500,                 loss: 2886.6951
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 6581/50000 (13.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 291.8688s / 84505.8003 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.1257
env0_second_0:                 episode reward: 1.3500,                 loss: 2910.5592
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 6601/50000 (13.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 292.5059s / 84798.3061 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.1180
env0_second_0:                 episode reward: 1.4500,                 loss: 2920.6287
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 6621/50000 (13.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 291.1047s / 85089.4108 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.1155
env0_second_0:                 episode reward: 1.6000,                 loss: 2780.2799
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 6641/50000 (13.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 291.0562s / 85380.4670 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.1246
env0_second_0:                 episode reward: 2.5000,                 loss: 2858.6109
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 6661/50000 (13.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 291.0057s / 85671.4728 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.1147
env0_second_0:                 episode reward: 0.6500,                 loss: 2763.3459
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 6681/50000 (13.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 290.9604s / 85962.4332 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.1181
env0_second_0:                 episode reward: 0.8000,                 loss: 2687.1342
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 6701/50000 (13.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 288.4972s / 86250.9304 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.1239
env0_second_0:                 episode reward: 2.2500,                 loss: 2655.0656
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 6721/50000 (13.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 290.8505s / 86541.7809 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.1252
env0_second_0:                 episode reward: 2.1500,                 loss: 2514.3681
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 6741/50000 (13.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 291.6080s / 86833.3889 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.1260
env0_second_0:                 episode reward: 0.8500,                 loss: 2665.5679
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 6761/50000 (13.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 292.2759s / 87125.6647 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.1227
env0_second_0:                 episode reward: 0.4500,                 loss: 2514.9484
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 6781/50000 (13.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 290.8186s / 87416.4834 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.1343
env0_second_0:                 episode reward: 0.3500,                 loss: 2359.4505
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 6801/50000 (13.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 291.9334s / 87708.4168 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.1420
env0_second_0:                 episode reward: 0.2500,                 loss: 2214.6344
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 6821/50000 (13.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 293.1314s / 88001.5482 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1514
env0_second_0:                 episode reward: 0.1000,                 loss: 2193.3291
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 6841/50000 (13.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 292.4202s / 88293.9684 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.1795
env0_second_0:                 episode reward: 0.1500,                 loss: 2046.3172
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 6861/50000 (13.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 291.1371s / 88585.1055 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2331
env0_second_0:                 episode reward: -0.1500,                 loss: 1825.1402
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 6881/50000 (13.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 292.9902s / 88878.0957 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.3000
env0_second_0:                 episode reward: 0.1500,                 loss: 1765.3749
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 6901/50000 (13.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 292.9676s / 89171.0633 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.3529
env0_second_0:                 episode reward: 0.1000,                 loss: 1457.6400
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 6921/50000 (13.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 292.0114s / 89463.0747 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.3878
env0_second_0:                 episode reward: -0.3000,                 loss: 1493.9308
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 6941/50000 (13.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 290.7908s / 89753.8655 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.4829
env0_second_0:                 episode reward: -0.5500,                 loss: 1287.2840
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 6961/50000 (13.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 292.1976s / 90046.0631 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.5834
env0_second_0:                 episode reward: 0.1500,                 loss: 1237.8441
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 6981/50000 (13.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 291.7682s / 90337.8314 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.6908
env0_second_0:                 episode reward: 0.0500,                 loss: 1235.3151
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7001/50000 (14.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 290.6707s / 90628.5020 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.7783
env0_second_0:                 episode reward: 0.3000,                 loss: 1235.4803
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7021/50000 (14.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 291.2762s / 90919.7782 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.9202
env0_second_0:                 episode reward: -0.5500,                 loss: 1557.0697
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 7041/50000 (14.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 288.9952s / 91208.7734 s
env0_first_0:                 episode reward: -0.4500,                 loss: 1.0018
env0_second_0:                 episode reward: 0.4500,                 loss: 1141.5625
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 7061/50000 (14.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 290.3090s / 91499.0824 s
env0_first_0:                 episode reward: -0.3000,                 loss: 1.0457
env0_second_0:                 episode reward: 0.3000,                 loss: 1075.0800
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7081/50000 (14.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 291.5037s / 91790.5861 s
env0_first_0:                 episode reward: -0.4000,                 loss: 1.0970
env0_second_0:                 episode reward: 0.4000,                 loss: 1094.9525
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 7101/50000 (14.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 290.9563s / 92081.5424 s
env0_first_0:                 episode reward: -0.0500,                 loss: 1.1604
env0_second_0:                 episode reward: 0.0500,                 loss: 952.9035
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 7121/50000 (14.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 290.4088s / 92371.9512 s
env0_first_0:                 episode reward: -0.5500,                 loss: 1.1213
env0_second_0:                 episode reward: 0.5500,                 loss: 905.7840
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 7141/50000 (14.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 292.6875s / 92664.6387 s
env0_first_0:                 episode reward: 0.0500,                 loss: 1.0822
env0_second_0:                 episode reward: -0.0500,                 loss: 852.1819
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7161/50000 (14.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 290.1659s / 92954.8045 s
env0_first_0:                 episode reward: -0.9500,                 loss: 1.0826
env0_second_0:                 episode reward: 0.9500,                 loss: 850.0308
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7181/50000 (14.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 290.6813s / 93245.4858 s
env0_first_0:                 episode reward: 0.0500,                 loss: 1.0464
env0_second_0:                 episode reward: -0.0500,                 loss: 918.1962
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7201/50000 (14.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 291.7448s / 93537.2306 s
env0_first_0:                 episode reward: -0.7500,                 loss: 1.0061
env0_second_0:                 episode reward: 0.7500,                 loss: 868.9191
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 7221/50000 (14.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 291.6706s / 93828.9012 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.9535
env0_second_0:                 episode reward: 0.0000,                 loss: 866.7851
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 7241/50000 (14.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 290.8294s / 94119.7307 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.8987
env0_second_0:                 episode reward: 0.2500,                 loss: 884.9917
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7261/50000 (14.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 289.9257s / 94409.6564 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.8798
env0_second_0:                 episode reward: 0.1000,                 loss: 901.6748
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 7281/50000 (14.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 291.3106s / 94700.9669 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.8323
env0_second_0:                 episode reward: -0.0500,                 loss: 942.5238
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7301/50000 (14.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 290.7050s / 94991.6719 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.7458
env0_second_0:                 episode reward: 0.0500,                 loss: 992.3499
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 7321/50000 (14.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 291.3198s / 95282.9917 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.6844
env0_second_0:                 episode reward: 0.0000,                 loss: 943.0634
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 7341/50000 (14.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 295.3223s / 95578.3141 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.6312
env0_second_0:                 episode reward: 0.0000,                 loss: 897.4544
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 7361/50000 (14.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 292.8313s / 95871.1454 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.5352
env0_second_0:                 episode reward: 0.0000,                 loss: 1137.1420
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 7381/50000 (14.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 290.4039s / 96161.5493 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.5197
env0_second_0:                 episode reward: 0.0000,                 loss: 882.3648
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7401/50000 (14.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 289.4282s / 96450.9775 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.4936
env0_second_0:                 episode reward: 0.0000,                 loss: 859.3040
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 7421/50000 (14.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 291.0480s / 96742.0254 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.4907
env0_second_0:                 episode reward: 0.0500,                 loss: 954.7852
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 7441/50000 (14.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 289.6781s / 97031.7035 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.5453
env0_second_0:                 episode reward: -0.5500,                 loss: 1066.1815
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 7461/50000 (14.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 292.4342s / 97324.1377 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.6543
env0_second_0:                 episode reward: 0.3500,                 loss: 1260.7934
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7481/50000 (14.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 291.8717s / 97616.0094 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.7982
env0_second_0:                 episode reward: 0.0500,                 loss: 1606.6190
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 7501/50000 (15.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 299.8603s / 97915.8697 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.9374
env0_second_0:                 episode reward: -0.6000,                 loss: 1826.4689
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7521/50000 (15.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 300.1763s / 98216.0460 s
env0_first_0:                 episode reward: 0.8000,                 loss: 1.1080
env0_second_0:                 episode reward: -0.8000,                 loss: 2094.5856
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 7541/50000 (15.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 294.5503s / 98510.5963 s
env0_first_0:                 episode reward: 1.2000,                 loss: 1.2236
env0_second_0:                 episode reward: -1.2000,                 loss: 2359.7145
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 7561/50000 (15.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 292.0297s / 98802.6260 s
env0_first_0:                 episode reward: 0.0000,                 loss: 1.3643
env0_second_0:                 episode reward: 0.0000,                 loss: 2851.6709
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 7581/50000 (15.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 291.3915s / 99094.0174 s
env0_first_0:                 episode reward: 0.6500,                 loss: 1.4906
env0_second_0:                 episode reward: -0.6500,                 loss: 3518.4672
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 7601/50000 (15.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 291.4498s / 99385.4672 s
env0_first_0:                 episode reward: 0.0500,                 loss: 1.6112
env0_second_0:                 episode reward: -0.0500,                 loss: 4510.8472
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7621/50000 (15.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 293.0265s / 99678.4937 s
env0_first_0:                 episode reward: 0.1500,                 loss: 1.6080
env0_second_0:                 episode reward: -0.1500,                 loss: 5459.7134
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 7641/50000 (15.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 289.4687s / 99967.9624 s
env0_first_0:                 episode reward: -0.1000,                 loss: 1.5632
env0_second_0:                 episode reward: 0.1000,                 loss: 5674.8341
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 7661/50000 (15.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 291.7040s / 100259.6664 s
env0_first_0:                 episode reward: -0.2500,                 loss: 1.5635
env0_second_0:                 episode reward: 0.2500,                 loss: 5990.4865
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 7681/50000 (15.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 292.4832s / 100552.1495 s
env0_first_0:                 episode reward: 0.7000,                 loss: 1.4895
env0_second_0:                 episode reward: -0.7000,                 loss: 6610.6400
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 7701/50000 (15.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 291.6299s / 100843.7795 s
env0_first_0:                 episode reward: 1.0000,                 loss: 1.3792
env0_second_0:                 episode reward: -1.0000,                 loss: 7403.1950
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7721/50000 (15.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 293.3928s / 101137.1723 s
env0_first_0:                 episode reward: 1.2000,                 loss: 1.3231
env0_second_0:                 episode reward: -1.2000,                 loss: 8467.7642
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 7741/50000 (15.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 292.2527s / 101429.4250 s
env0_first_0:                 episode reward: -0.4000,                 loss: 1.1599
env0_second_0:                 episode reward: 0.4000,                 loss: 9062.3611
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 7761/50000 (15.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 294.1285s / 101723.5535 s
env0_first_0:                 episode reward: 0.4500,                 loss: 1.1131
env0_second_0:                 episode reward: -0.4500,                 loss: 9494.6221
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 7781/50000 (15.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 293.8950s / 102017.4485 s
env0_first_0:                 episode reward: 0.6000,                 loss: 1.0284
env0_second_0:                 episode reward: -0.6000,                 loss: 9829.4872
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 7801/50000 (15.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 291.6007s / 102309.0492 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.9927
env0_second_0:                 episode reward: -0.4000,                 loss: 9855.1943
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 7821/50000 (15.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 288.8184s / 102597.8676 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.9590
env0_second_0:                 episode reward: 0.3000,                 loss: 10113.7215
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 7841/50000 (15.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 291.8280s / 102889.6956 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.8842
env0_second_0:                 episode reward: -0.5000,                 loss: 10031.3582
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7861/50000 (15.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 291.7166s / 103181.4122 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.8840
env0_second_0:                 episode reward: 0.2000,                 loss: 10171.2613
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 7881/50000 (15.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 291.1288s / 103472.5410 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.8486
env0_second_0:                 episode reward: -0.8500,                 loss: 11179.7890
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 7901/50000 (15.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 291.3047s / 103763.8456 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.7883
env0_second_0:                 episode reward: 0.3000,                 loss: 11582.1849
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7921/50000 (15.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 290.2220s / 104054.0677 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.7410
env0_second_0:                 episode reward: 0.0000,                 loss: 12001.8651
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 7941/50000 (15.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 292.5357s / 104346.6034 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.6878
env0_second_0:                 episode reward: 0.9000,                 loss: 13142.8887
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7961/50000 (15.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 290.1679s / 104636.7713 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.5946
env0_second_0:                 episode reward: 0.2000,                 loss: 14024.3434
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 7981/50000 (15.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 294.7190s / 104931.4903 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.5374
env0_second_0:                 episode reward: 1.8000,                 loss: 14569.6488
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 8001/50000 (16.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 290.9970s / 105222.4874 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.4744
env0_second_0:                 episode reward: -0.3500,                 loss: 15661.4497
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 8021/50000 (16.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 291.0143s / 105513.5017 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.4247
env0_second_0:                 episode reward: 0.5500,                 loss: 16367.7325
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 8041/50000 (16.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 290.9991s / 105804.5008 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.3831
env0_second_0:                 episode reward: 1.5500,                 loss: 16564.6870
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8061/50000 (16.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 290.0319s / 106094.5328 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.3452
env0_second_0:                 episode reward: 0.1500,                 loss: 16676.7711
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 8081/50000 (16.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 290.4867s / 106385.0194 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.3121
env0_second_0:                 episode reward: -0.6000,                 loss: 17037.7961
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8101/50000 (16.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 291.8576s / 106676.8771 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.2918
env0_second_0:                 episode reward: -0.3500,                 loss: 18151.3478
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 8121/50000 (16.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 291.5374s / 106968.4145 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2679
env0_second_0:                 episode reward: 0.6500,                 loss: 18178.2767
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 8141/50000 (16.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 293.1454s / 107261.5599 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.2389
env0_second_0:                 episode reward: 1.1500,                 loss: 18830.4156
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 8161/50000 (16.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 292.3303s / 107553.8902 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2262
env0_second_0:                 episode reward: 0.2000,                 loss: 18968.8239
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8181/50000 (16.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 291.9793s / 107845.8695 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2151
env0_second_0:                 episode reward: -0.2500,                 loss: 18379.7807
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8201/50000 (16.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 293.9467s / 108139.8162 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2037
env0_second_0:                 episode reward: 0.0500,                 loss: 17299.0290
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8221/50000 (16.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 292.9224s / 108432.7387 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2035
env0_second_0:                 episode reward: 0.0500,                 loss: 17146.3758
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8241/50000 (16.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 293.9521s / 108726.6908 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2070
env0_second_0:                 episode reward: -0.0500,                 loss: 18408.5899
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 8261/50000 (16.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 291.5625s / 109018.2533 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2286
env0_second_0:                 episode reward: -0.0500,                 loss: 17885.4440
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8281/50000 (16.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 294.2267s / 109312.4800 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.2254
env0_second_0:                 episode reward: -0.2000,                 loss: 18481.2422
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8301/50000 (16.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 290.8373s / 109603.3173 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.2299
env0_second_0:                 episode reward: -0.1000,                 loss: 19513.2224
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 8321/50000 (16.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 289.6480s / 109892.9653 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.3073
env0_second_0:                 episode reward: -0.3000,                 loss: 20585.1469
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8341/50000 (16.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 291.7373s / 110184.7026 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.3609
env0_second_0:                 episode reward: -0.1000,                 loss: 21704.2928
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 8361/50000 (16.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 293.5397s / 110478.2422 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.4175
env0_second_0:                 episode reward: 0.0000,                 loss: 21619.5950
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 8381/50000 (16.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 293.8491s / 110772.0913 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.4757
env0_second_0:                 episode reward: 0.1500,                 loss: 21314.4017
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8401/50000 (16.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 292.7579s / 111064.8492 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.4028
env0_second_0:                 episode reward: 0.5500,                 loss: 22920.0767
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 8421/50000 (16.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 292.5113s / 111357.3605 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.4226
env0_second_0:                 episode reward: 0.5000,                 loss: 23879.9333
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 8441/50000 (16.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 293.5494s / 111650.9099 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.4401
env0_second_0:                 episode reward: 0.0500,                 loss: 25922.0108/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/research/MARS/mars/equilibrium_solver/eq_MWUsolver.py:86: RuntimeWarning: overflow encountered in exp
  policies[:, 1] = policies[:, 1] * np.exp(-learning_rate*payoff_vec)
/home/zihan/research/MARS/mars/equilibrium_solver/eq_MWUsolver.py:90: RuntimeWarning: invalid value encountered in true_divide
  policies = policies/np.expand_dims(EPS+np.sum(policies, axis=-1), -1)

env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 8461/50000 (16.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 294.1116s / 111945.0215 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.4541
env0_second_0:                 episode reward: 0.1000,                 loss: 29201.5173
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8481/50000 (16.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 293.4928s / 112238.5143 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.4967
env0_second_0:                 episode reward: -0.5500,                 loss: 30493.2820
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 8501/50000 (17.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 292.4506s / 112530.9650 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.4395
env0_second_0:                 episode reward: -0.9500,                 loss: 34077.5438
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 8521/50000 (17.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 293.3956s / 112824.3606 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.4500
env0_second_0:                 episode reward: -0.1000,                 loss: 34408.9138
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 8541/50000 (17.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 291.3745s / 113115.7351 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.4437
env0_second_0:                 episode reward: -0.0500,                 loss: 37358.2441
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Error: Nan Nash value in Nash computation is derived in the udpate function.
Traceback (most recent call last):
  File "general_train.py", line 31, in <module>
    launch_rollout(parser_args.env, parser_args.method, parser_args.save_id)
  File "general_train.py", line 24, in launch_rollout
    rollout(env, model, args, save_id)
  File "/home/zihan/research/MARS/mars/rollout.py", line 21, in rollout
    rollout_normal(env, model, save_id, args)
  File "/home/zihan/research/MARS/mars/rollout.py", line 114, in rollout_normal
    loss = model.update(
  File "/home/zihan/research/MARS/mars/rl/agents/multiagent.py", line 274, in update
    loss = agent.update()
  File "/home/zihan/research/MARS/mars/rl/agents/nash_dqn_factorized.py", line 205, in update
    def DQN_loss(model, target, state, action, reward, next_state, done, multi_step):
  File "<__array_function__ internals>", line 6, in zeros_like
  File "/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/numeric.py", line 138, in zeros_like
    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)
  File "<__array_function__ internals>", line 6, in empty_like
  File "/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/_tensor.py", line 721, in __array__
    return self.numpy()
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
