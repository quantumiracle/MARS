pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 3}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0521/pettingzoo_boxing_v1_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0521/pettingzoo_boxing_v1_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 45.1224s / 45.1224 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0116
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0096
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1633.8512s / 1678.9736 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0206
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0207
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2054.8370s / 3733.8106 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0258
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0263
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2167.5573s / 5901.3679 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0355
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0344
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2181.8329s / 8083.2008 s
env0_first_0:                 episode reward: -11.1000,                 loss: 0.0353
env0_second_0:                 episode reward: 11.1000,                 loss: 0.0347
env1_first_0:                 episode reward: -16.3000,                 loss: nan
env1_second_0:                 episode reward: 16.3000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2196.9292s / 10280.1299 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0561
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0581
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2187.1375s / 12467.2675 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0446
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0473
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2190.3675s / 14657.6350 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0392
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0443
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2183.1061s / 16840.7411 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0244
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0273
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2192.1622s / 19032.9033 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0237
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0238
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2173.5671s / 21206.4705 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0189
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0210
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2195.8712s / 23402.3417 s
env0_first_0:                 episode reward: -8.9500,                 loss: 0.0194
env0_second_0:                 episode reward: 8.9500,                 loss: 0.0203
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2192.4674s / 25594.8091 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0122
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0117
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2191.9969s / 27786.8059 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0166
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0167
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2194.3139s / 29981.1198 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0172
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0172
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2202.3718s / 32183.4916 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0108
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0105
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2184.6383s / 34368.1300 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0122
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0132
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1762.2,                last time consumption/overall running time: 2171.1750s / 36539.3049 s
env0_first_0:                 episode reward: -25.1500,                 loss: 0.0293
env0_second_0:                 episode reward: 25.1500,                 loss: 0.0301
env1_first_0:                 episode reward: -30.0500,                 loss: nan
env1_second_0:                 episode reward: 30.0500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1735.95,                last time consumption/overall running time: 2338.3586s / 38877.6635 s
env0_first_0:                 episode reward: -31.1500,                 loss: 0.0535
env0_second_0:                 episode reward: 31.1500,                 loss: 0.0418
env1_first_0:                 episode reward: -23.0000,                 loss: nan
env1_second_0:                 episode reward: 23.0000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 902.5,                last time consumption/overall running time: 1320.5289s / 40198.1924 s
env0_first_0:                 episode reward: -66.2000,                 loss: 0.0594
env0_second_0:                 episode reward: 66.2000,                 loss: 0.0520
env1_first_0:                 episode reward: -60.0500,                 loss: nan
env1_second_0:                 episode reward: 60.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1237.45,                last time consumption/overall running time: 1793.4249s / 41991.6173 s
env0_first_0:                 episode reward: -45.1500,                 loss: 0.1059
env0_second_0:                 episode reward: 45.1500,                 loss: 0.0976
env1_first_0:                 episode reward: -44.0500,                 loss: nan
env1_second_0:                 episode reward: 44.0500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1360.35,                last time consumption/overall running time: 1983.9108s / 43975.5281 s
env0_first_0:                 episode reward: -44.5500,                 loss: 0.1929
env0_second_0:                 episode reward: 44.5500,                 loss: 0.1742
env1_first_0:                 episode reward: -47.5000,                 loss: nan
env1_second_0:                 episode reward: 47.5000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1106.3,                last time consumption/overall running time: 1623.1824s / 45598.7105 s
env0_first_0:                 episode reward: -65.7500,                 loss: 0.1813
env0_second_0:                 episode reward: 65.7500,                 loss: 0.1718
env1_first_0:                 episode reward: -71.1500,                 loss: nan
env1_second_0:                 episode reward: 71.1500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 452.35,                last time consumption/overall running time: 666.2555s / 46264.9660 s
env0_first_0:                 episode reward: -85.3000,                 loss: 0.1860
env0_second_0:                 episode reward: 85.3000,                 loss: 0.1857
env1_first_0:                 episode reward: -69.3500,                 loss: nan
env1_second_0:                 episode reward: 69.3500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 318.85,                last time consumption/overall running time: 469.6149s / 46734.5809 s
env0_first_0:                 episode reward: -75.0500,                 loss: 0.1876
env0_second_0:                 episode reward: 75.0500,                 loss: 0.1968
env1_first_0:                 episode reward: -84.9500,                 loss: nan
env1_second_0:                 episode reward: 84.9500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 348.1,                last time consumption/overall running time: 513.5769s / 47248.1577 s
env0_first_0:                 episode reward: -70.9500,                 loss: 0.2126
env0_second_0:                 episode reward: 70.9500,                 loss: 0.2236
env1_first_0:                 episode reward: -82.8500,                 loss: nan
env1_second_0:                 episode reward: 82.8500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 281.05,                last time consumption/overall running time: 413.9105s / 47662.0683 s
env0_first_0:                 episode reward: -86.1000,                 loss: 0.2319
env0_second_0:                 episode reward: 86.1000,                 loss: 0.2526
env1_first_0:                 episode reward: -80.9000,                 loss: nan
env1_second_0:                 episode reward: 80.9000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 278.5,                last time consumption/overall running time: 411.1975s / 48073.2657 s
env0_first_0:                 episode reward: -84.8500,                 loss: 0.2338
env0_second_0:                 episode reward: 84.8500,                 loss: 0.2676
env1_first_0:                 episode reward: -86.4500,                 loss: nan
env1_second_0:                 episode reward: 86.4500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 288.25,                last time consumption/overall running time: 426.6118s / 48499.8776 s
env0_first_0:                 episode reward: -83.6500,                 loss: 0.2605
env0_second_0:                 episode reward: 83.6500,                 loss: 0.3076
env1_first_0:                 episode reward: -90.1000,                 loss: nan
env1_second_0:                 episode reward: 90.1000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 267.8,                last time consumption/overall running time: 395.2826s / 48895.1601 s
env0_first_0:                 episode reward: -91.0500,                 loss: 0.2566
env0_second_0:                 episode reward: 91.0500,                 loss: 0.3081
env1_first_0:                 episode reward: -77.6000,                 loss: nan
env1_second_0:                 episode reward: 77.6000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 282.0,                last time consumption/overall running time: 415.1267s / 49310.2868 s
env0_first_0:                 episode reward: -84.3500,                 loss: 0.2908
env0_second_0:                 episode reward: 84.3500,                 loss: 0.3674
env1_first_0:                 episode reward: -84.0000,                 loss: nan
env1_second_0:                 episode reward: 84.0000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 280.2,                last time consumption/overall running time: 415.1480s / 49725.4348 s
env0_first_0:                 episode reward: -85.8000,                 loss: 0.2713
env0_second_0:                 episode reward: 85.8000,                 loss: 0.3346
env1_first_0:                 episode reward: -85.0500,                 loss: nan
env1_second_0:                 episode reward: 85.0500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 280.9,                last time consumption/overall running time: 417.5207s / 50142.9555 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.2864
env0_second_0:                 episode reward: 90.4000,                 loss: 0.3531
env1_first_0:                 episode reward: -82.3500,                 loss: nan
env1_second_0:                 episode reward: 82.3500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 277.35,                last time consumption/overall running time: 408.3458s / 50551.3013 s
env0_first_0:                 episode reward: -78.6000,                 loss: 0.2825
env0_second_0:                 episode reward: 78.6000,                 loss: 0.3750
env1_first_0:                 episode reward: -80.6000,                 loss: nan
env1_second_0:                 episode reward: 80.6000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 278.85,                last time consumption/overall running time: 411.2799s / 50962.5812 s
env0_first_0:                 episode reward: -82.3000,                 loss: 0.2589
env0_second_0:                 episode reward: 82.3000,                 loss: 0.3572
env1_first_0:                 episode reward: -82.1500,                 loss: nan
env1_second_0:                 episode reward: 82.1500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 303.5,                last time consumption/overall running time: 449.7303s / 51412.3115 s
env0_first_0:                 episode reward: -76.9500,                 loss: 0.2772
env0_second_0:                 episode reward: 76.9500,                 loss: 0.3692
env1_first_0:                 episode reward: -74.9000,                 loss: nan
env1_second_0:                 episode reward: 74.9000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 294.7,                last time consumption/overall running time: 433.7775s / 51846.0890 s
env0_first_0:                 episode reward: -86.3500,                 loss: 0.2786
env0_second_0:                 episode reward: 86.3500,                 loss: 0.3464
env1_first_0:                 episode reward: -73.1500,                 loss: nan
env1_second_0:                 episode reward: 73.1500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 266.2,                last time consumption/overall running time: 394.0823s / 52240.1713 s
env0_first_0:                 episode reward: -85.4000,                 loss: 0.2996
env0_second_0:                 episode reward: 85.4000,                 loss: 0.4130
env1_first_0:                 episode reward: -90.1500,                 loss: nan
env1_second_0:                 episode reward: 90.1500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 284.8,                last time consumption/overall running time: 420.9153s / 52661.0866 s
env0_first_0:                 episode reward: -84.6500,                 loss: 0.2997
env0_second_0:                 episode reward: 84.6500,                 loss: 0.3927
env1_first_0:                 episode reward: -88.7500,                 loss: nan
env1_second_0:                 episode reward: 88.7500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 259.55,                last time consumption/overall running time: 381.4509s / 53042.5374 s
env0_first_0:                 episode reward: -86.0500,                 loss: 0.3164
env0_second_0:                 episode reward: 86.0500,                 loss: 0.4323
env1_first_0:                 episode reward: -86.8500,                 loss: nan
env1_second_0:                 episode reward: 86.8500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 249.75,                last time consumption/overall running time: 368.7417s / 53411.2792 s
env0_first_0:                 episode reward: -75.4500,                 loss: 0.3193
env0_second_0:                 episode reward: 75.4500,                 loss: 0.4392
env1_first_0:                 episode reward: -87.6500,                 loss: nan
env1_second_0:                 episode reward: 87.6500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 253.25,                last time consumption/overall running time: 377.8071s / 53789.0863 s
env0_first_0:                 episode reward: -87.6000,                 loss: 0.3521
env0_second_0:                 episode reward: 87.6000,                 loss: 0.4897
env1_first_0:                 episode reward: -87.5000,                 loss: nan
env1_second_0:                 episode reward: 87.5000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 262.75,                last time consumption/overall running time: 390.6683s / 54179.7546 s
env0_first_0:                 episode reward: -81.7000,                 loss: 0.3362
env0_second_0:                 episode reward: 81.7000,                 loss: 0.5050
env1_first_0:                 episode reward: -84.9000,                 loss: nan
env1_second_0:                 episode reward: 84.9000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 246.45,                last time consumption/overall running time: 360.0959s / 54539.8505 s
env0_first_0:                 episode reward: -94.9500,                 loss: 0.3468
env0_second_0:                 episode reward: 94.9500,                 loss: 0.5022
env1_first_0:                 episode reward: -85.8500,                 loss: nan
env1_second_0:                 episode reward: 85.8500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 257.6,                last time consumption/overall running time: 379.2554s / 54919.1058 s
env0_first_0:                 episode reward: -83.7000,                 loss: 0.3797
env0_second_0:                 episode reward: 83.7000,                 loss: 0.5357
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 257.3,                last time consumption/overall running time: 380.2221s / 55299.3280 s
env0_first_0:                 episode reward: -81.6500,                 loss: 0.4177
env0_second_0:                 episode reward: 81.6500,                 loss: 0.5581
env1_first_0:                 episode reward: -87.2000,                 loss: nan
env1_second_0:                 episode reward: 87.2000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 256.4,                last time consumption/overall running time: 378.8462s / 55678.1742 s
env0_first_0:                 episode reward: -77.9000,                 loss: 0.4217
env0_second_0:                 episode reward: 77.9000,                 loss: 0.6269
env1_first_0:                 episode reward: -88.5000,                 loss: nan
env1_second_0:                 episode reward: 88.5000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 261.8,                last time consumption/overall running time: 384.6036s / 56062.7778 s
env0_first_0:                 episode reward: -87.3000,                 loss: 0.4340
env0_second_0:                 episode reward: 87.3000,                 loss: 0.6274
env1_first_0:                 episode reward: -89.0000,                 loss: nan
env1_second_0:                 episode reward: 89.0000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 249.55,                last time consumption/overall running time: 363.0881s / 56425.8659 s
env0_first_0:                 episode reward: -93.5000,                 loss: 0.4649
env0_second_0:                 episode reward: 93.5000,                 loss: 0.6741
env1_first_0:                 episode reward: -87.0000,                 loss: nan
env1_second_0:                 episode reward: 87.0000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 250.55,                last time consumption/overall running time: 369.7197s / 56795.5856 s
env0_first_0:                 episode reward: -85.5000,                 loss: 0.4211
env0_second_0:                 episode reward: 85.5000,                 loss: 0.6225
env1_first_0:                 episode reward: -85.1000,                 loss: nan
env1_second_0:                 episode reward: 85.1000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 250.25,                last time consumption/overall running time: 370.6583s / 57166.2439 s
env0_first_0:                 episode reward: -92.3500,                 loss: 0.4389
env0_second_0:                 episode reward: 92.3500,                 loss: 0.6481
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 253.5,                last time consumption/overall running time: 373.5985s / 57539.8424 s
env0_first_0:                 episode reward: -81.7500,                 loss: 0.4365
env0_second_0:                 episode reward: 81.7500,                 loss: 0.7071
env1_first_0:                 episode reward: -78.4000,                 loss: nan
env1_second_0:                 episode reward: 78.4000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 238.75,                last time consumption/overall running time: 349.4856s / 57889.3280 s
env0_first_0:                 episode reward: -91.4000,                 loss: 0.4492
env0_second_0:                 episode reward: 91.4000,                 loss: 0.6845
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 244.1,                last time consumption/overall running time: 354.3982s / 58243.7261 s
env0_first_0:                 episode reward: -75.2000,                 loss: 0.4549
env0_second_0:                 episode reward: 75.2000,                 loss: 0.6408
env1_first_0:                 episode reward: -92.7500,                 loss: nan
env1_second_0:                 episode reward: 92.7500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 323.35,                last time consumption/overall running time: 473.9573s / 58717.6834 s
env0_first_0:                 episode reward: -77.8000,                 loss: 0.4690
env0_second_0:                 episode reward: 77.8000,                 loss: 0.6774
env1_first_0:                 episode reward: -79.5500,                 loss: nan
env1_second_0:                 episode reward: 79.5500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 239.45,                last time consumption/overall running time: 352.7811s / 59070.4645 s
env0_first_0:                 episode reward: -91.0500,                 loss: 0.4848
env0_second_0:                 episode reward: 91.0500,                 loss: 0.7019
env1_first_0:                 episode reward: -84.2500,                 loss: nan
env1_second_0:                 episode reward: 84.2500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 234.3,                last time consumption/overall running time: 342.9525s / 59413.4170 s
env0_first_0:                 episode reward: -90.6000,                 loss: 0.4773
env0_second_0:                 episode reward: 90.6000,                 loss: 0.6714
env1_first_0:                 episode reward: -92.8500,                 loss: nan
env1_second_0:                 episode reward: 92.8500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 238.9,                last time consumption/overall running time: 347.4462s / 59760.8631 s
env0_first_0:                 episode reward: -85.6500,                 loss: 0.4833
env0_second_0:                 episode reward: 85.6500,                 loss: 0.6467
env1_first_0:                 episode reward: -94.9500,                 loss: nan
env1_second_0:                 episode reward: 94.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 242.8,                last time consumption/overall running time: 359.1210s / 60119.9842 s
env0_first_0:                 episode reward: -90.3000,                 loss: 0.4995
env0_second_0:                 episode reward: 90.3000,                 loss: 0.7065
env1_first_0:                 episode reward: -82.2000,                 loss: nan
env1_second_0:                 episode reward: 82.2000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 240.8,                last time consumption/overall running time: 353.6680s / 60473.6521 s
env0_first_0:                 episode reward: -89.0500,                 loss: 0.5291
env0_second_0:                 episode reward: 89.0500,                 loss: 0.7190
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 241.85,                last time consumption/overall running time: 357.6157s / 60831.2678 s
env0_first_0:                 episode reward: -92.4500,                 loss: 0.5115
env0_second_0:                 episode reward: 92.4500,                 loss: 0.7362
env1_first_0:                 episode reward: -93.6500,                 loss: nan
env1_second_0:                 episode reward: 93.6500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 241.05,                last time consumption/overall running time: 360.8565s / 61192.1243 s
env0_first_0:                 episode reward: -85.3500,                 loss: 0.5375
env0_second_0:                 episode reward: 85.3500,                 loss: 0.7477
env1_first_0:                 episode reward: -95.0000,                 loss: nan
env1_second_0:                 episode reward: 95.0000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 235.35,                last time consumption/overall running time: 350.6295s / 61542.7538 s
env0_first_0:                 episode reward: -88.0000,                 loss: 0.4608
env0_second_0:                 episode reward: 88.0000,                 loss: 0.7581
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 238.8,                last time consumption/overall running time: 351.8655s / 61894.6193 s
env0_first_0:                 episode reward: -91.0500,                 loss: 0.5240
env0_second_0:                 episode reward: 91.0500,                 loss: 0.7378
env1_first_0:                 episode reward: -90.8000,                 loss: nan
env1_second_0:                 episode reward: 90.8000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 241.25,                last time consumption/overall running time: 359.7849s / 62254.4042 s
env0_first_0:                 episode reward: -91.6000,                 loss: 0.5596
env0_second_0:                 episode reward: 91.6000,                 loss: 0.7788
env1_first_0:                 episode reward: -90.7500,                 loss: nan
env1_second_0:                 episode reward: 90.7500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 242.05,                last time consumption/overall running time: 355.8883s / 62610.2925 s
env0_first_0:                 episode reward: -90.8500,                 loss: 0.5535
env0_second_0:                 episode reward: 90.8500,                 loss: 0.7581
env1_first_0:                 episode reward: -91.2000,                 loss: nan
env1_second_0:                 episode reward: 91.2000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 241.15,                last time consumption/overall running time: 352.0807s / 62962.3732 s
env0_first_0:                 episode reward: -90.7500,                 loss: 0.5155
env0_second_0:                 episode reward: 90.7500,                 loss: 0.7092
env1_first_0:                 episode reward: -92.2500,                 loss: nan
env1_second_0:                 episode reward: 92.2500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 240.3,                last time consumption/overall running time: 354.3383s / 63316.7116 s
env0_first_0:                 episode reward: -96.1500,                 loss: 0.5210
env0_second_0:                 episode reward: 96.1500,                 loss: 0.7802
env1_first_0:                 episode reward: -89.6000,                 loss: nan
env1_second_0:                 episode reward: 89.6000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 239.3,                last time consumption/overall running time: 352.3603s / 63669.0719 s
env0_first_0:                 episode reward: -95.9000,                 loss: 0.5525
env0_second_0:                 episode reward: 95.9000,                 loss: 0.7126
env1_first_0:                 episode reward: -92.2500,                 loss: nan
env1_second_0:                 episode reward: 92.2500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 238.85,                last time consumption/overall running time: 354.3228s / 64023.3946 s
env0_first_0:                 episode reward: -90.4500,                 loss: 0.5632
env0_second_0:                 episode reward: 90.4500,                 loss: 0.7621
env1_first_0:                 episode reward: -93.4000,                 loss: nan
env1_second_0:                 episode reward: 93.4000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 240.55,                last time consumption/overall running time: 352.9933s / 64376.3880 s
env0_first_0:                 episode reward: -93.0000,                 loss: 0.5548
env0_second_0:                 episode reward: 93.0000,                 loss: 0.7508
env1_first_0:                 episode reward: -91.1500,                 loss: nan
env1_second_0:                 episode reward: 91.1500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 235.95,                last time consumption/overall running time: 348.2515s / 64724.6395 s
env0_first_0:                 episode reward: -94.9500,                 loss: 0.5529
env0_second_0:                 episode reward: 94.9500,                 loss: 0.7996
env1_first_0:                 episode reward: -84.0500,                 loss: nan
env1_second_0:                 episode reward: 84.0500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 239.45,                last time consumption/overall running time: 352.5305s / 65077.1700 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.5144
env0_second_0:                 episode reward: 92.4000,                 loss: 0.7622
env1_first_0:                 episode reward: -86.5000,                 loss: nan
env1_second_0:                 episode reward: 86.5000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 244.5,                last time consumption/overall running time: 360.5880s / 65437.7581 s
env0_first_0:                 episode reward: -84.3000,                 loss: 0.5534
env0_second_0:                 episode reward: 84.3000,                 loss: 0.8344
env1_first_0:                 episode reward: -91.7000,                 loss: nan
env1_second_0:                 episode reward: 91.7000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 243.95,                last time consumption/overall running time: 359.4474s / 65797.2055 s
env0_first_0:                 episode reward: -89.7000,                 loss: 0.5752
env0_second_0:                 episode reward: 89.7000,                 loss: 0.8296
env1_first_0:                 episode reward: -89.4500,                 loss: nan
env1_second_0:                 episode reward: 89.4500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 230.75,                last time consumption/overall running time: 337.9779s / 66135.1834 s
env0_first_0:                 episode reward: -90.8500,                 loss: 0.5768
env0_second_0:                 episode reward: 90.8500,                 loss: 0.8161
env1_first_0:                 episode reward: -90.3500,                 loss: nan
env1_second_0:                 episode reward: 90.3500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 241.3,                last time consumption/overall running time: 356.1536s / 66491.3369 s
env0_first_0:                 episode reward: -85.8500,                 loss: 0.5759
env0_second_0:                 episode reward: 85.8500,                 loss: 0.8602
env1_first_0:                 episode reward: -92.7000,                 loss: nan
env1_second_0:                 episode reward: 92.7000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 243.1,                last time consumption/overall running time: 356.1448s / 66847.4817 s
env0_first_0:                 episode reward: -83.9000,                 loss: 0.7044
env0_second_0:                 episode reward: 83.9000,                 loss: 0.9539
env1_first_0:                 episode reward: -88.6500,                 loss: nan
env1_second_0:                 episode reward: 88.6500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 240.7,                last time consumption/overall running time: 353.9309s / 67201.4126 s
env0_first_0:                 episode reward: -84.9000,                 loss: 0.6679
env0_second_0:                 episode reward: 84.9000,                 loss: 0.9715
env1_first_0:                 episode reward: -89.2000,                 loss: nan
env1_second_0:                 episode reward: 89.2000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 232.75,                last time consumption/overall running time: 342.7936s / 67544.2063 s
env0_first_0:                 episode reward: -83.9000,                 loss: 0.6621
env0_second_0:                 episode reward: 83.9000,                 loss: 1.0788
env1_first_0:                 episode reward: -87.1500,                 loss: nan
env1_second_0:                 episode reward: 87.1500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 231.4,                last time consumption/overall running time: 339.8027s / 67884.0089 s
env0_first_0:                 episode reward: -74.5500,                 loss: 0.6302
env0_second_0:                 episode reward: 74.5500,                 loss: 0.9501
env1_first_0:                 episode reward: -97.9500,                 loss: nan
env1_second_0:                 episode reward: 97.9500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 226.75,                last time consumption/overall running time: 337.3737s / 68221.3827 s
env0_first_0:                 episode reward: -91.0000,                 loss: 0.6034
env0_second_0:                 episode reward: 91.0000,                 loss: 1.0046
env1_first_0:                 episode reward: -84.7000,                 loss: nan
env1_second_0:                 episode reward: 84.7000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 311.7,                last time consumption/overall running time: 460.3184s / 68681.7011 s
env0_first_0:                 episode reward: -70.7000,                 loss: 0.6590
env0_second_0:                 episode reward: 70.7000,                 loss: 0.9960
env1_first_0:                 episode reward: -92.6000,                 loss: nan
env1_second_0:                 episode reward: 92.6000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 242.25,                last time consumption/overall running time: 354.0120s / 69035.7130 s
env0_first_0:                 episode reward: -84.4000,                 loss: 0.6411
env0_second_0:                 episode reward: 84.4000,                 loss: 0.9716
env1_first_0:                 episode reward: -95.1000,                 loss: nan
env1_second_0:                 episode reward: 95.1000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 235.25,                last time consumption/overall running time: 345.2876s / 69381.0006 s
env0_first_0:                 episode reward: -87.9500,                 loss: 0.6375
env0_second_0:                 episode reward: 87.9500,                 loss: 1.1172
env1_first_0:                 episode reward: -87.6000,                 loss: nan
env1_second_0:                 episode reward: 87.6000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 255.7,                last time consumption/overall running time: 375.1950s / 69756.1955 s
env0_first_0:                 episode reward: -77.6500,                 loss: 0.6738
env0_second_0:                 episode reward: 77.6500,                 loss: 1.1041
env1_first_0:                 episode reward: -93.6500,                 loss: nan
env1_second_0:                 episode reward: 93.6500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 240.6,                last time consumption/overall running time: 353.0886s / 70109.2842 s
env0_first_0:                 episode reward: -88.4000,                 loss: 0.6858
env0_second_0:                 episode reward: 88.4000,                 loss: 1.0904
env1_first_0:                 episode reward: -90.8000,                 loss: nan
env1_second_0:                 episode reward: 90.8000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 238.7,                last time consumption/overall running time: 353.0729s / 70462.3571 s
env0_first_0:                 episode reward: -84.4500,                 loss: 0.6740
env0_second_0:                 episode reward: 84.4500,                 loss: 1.1160
env1_first_0:                 episode reward: -92.5500,                 loss: nan
env1_second_0:                 episode reward: 92.5500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 253.85,                last time consumption/overall running time: 372.5033s / 70834.8604 s
env0_first_0:                 episode reward: -89.7500,                 loss: 0.6512
env0_second_0:                 episode reward: 89.7500,                 loss: 1.0658
env1_first_0:                 episode reward: -81.3500,                 loss: nan
env1_second_0:                 episode reward: 81.3500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 223.8,                last time consumption/overall running time: 330.7399s / 71165.6003 s
env0_first_0:                 episode reward: -89.6500,                 loss: 0.6164
env0_second_0:                 episode reward: 89.6500,                 loss: 1.0861
env1_first_0:                 episode reward: -89.4500,                 loss: nan
env1_second_0:                 episode reward: 89.4500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 247.45,                last time consumption/overall running time: 364.1488s / 71529.7491 s
env0_first_0:                 episode reward: -89.4500,                 loss: 0.6000
env0_second_0:                 episode reward: 89.4500,                 loss: 1.0146
env1_first_0:                 episode reward: -82.7500,                 loss: nan
env1_second_0:                 episode reward: 82.7500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 240.2,                last time consumption/overall running time: 354.2987s / 71884.0479 s
env0_first_0:                 episode reward: -90.9000,                 loss: 0.5787
env0_second_0:                 episode reward: 90.9000,                 loss: 0.9683
env1_first_0:                 episode reward: -87.9000,                 loss: nan
env1_second_0:                 episode reward: 87.9000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 233.85,                last time consumption/overall running time: 343.4485s / 72227.4963 s
env0_first_0:                 episode reward: -80.5000,                 loss: 0.6197
env0_second_0:                 episode reward: 80.5000,                 loss: 0.9915
env1_first_0:                 episode reward: -89.8500,                 loss: nan
env1_second_0:                 episode reward: 89.8500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 239.2,                last time consumption/overall running time: 352.4390s / 72579.9353 s
env0_first_0:                 episode reward: -79.9500,                 loss: 0.5623
env0_second_0:                 episode reward: 79.9500,                 loss: 0.9648
env1_first_0:                 episode reward: -84.1000,                 loss: nan
env1_second_0:                 episode reward: 84.1000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 238.45,                last time consumption/overall running time: 355.0262s / 72934.9615 s
env0_first_0:                 episode reward: -83.7500,                 loss: 0.6138
env0_second_0:                 episode reward: 83.7500,                 loss: 1.0321
env1_first_0:                 episode reward: -86.1000,                 loss: nan
env1_second_0:                 episode reward: 86.1000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 232.0,                last time consumption/overall running time: 339.8829s / 73274.8444 s
env0_first_0:                 episode reward: -87.7500,                 loss: 0.5982
env0_second_0:                 episode reward: 87.7500,                 loss: 0.9896
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 228.1,                last time consumption/overall running time: 336.6195s / 73611.4639 s
env0_first_0:                 episode reward: -89.6000,                 loss: 0.6018
env0_second_0:                 episode reward: 89.6000,                 loss: 1.0021
env1_first_0:                 episode reward: -92.4500,                 loss: nan
env1_second_0:                 episode reward: 92.4500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 237.85,                last time consumption/overall running time: 351.7505s / 73963.2144 s
env0_first_0:                 episode reward: -77.7000,                 loss: 0.5675
env0_second_0:                 episode reward: 77.7000,                 loss: 1.0137
env1_first_0:                 episode reward: -96.2500,                 loss: nan
env1_second_0:                 episode reward: 96.2500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 236.0,                last time consumption/overall running time: 344.8110s / 74308.0254 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.5962
env0_second_0:                 episode reward: 86.4500,                 loss: 1.0491
env1_first_0:                 episode reward: -80.6500,                 loss: nan
env1_second_0:                 episode reward: 80.6500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 240.95,                last time consumption/overall running time: 359.3621s / 74667.3875 s
env0_first_0:                 episode reward: -85.2500,                 loss: 0.5639
env0_second_0:                 episode reward: 85.2500,                 loss: 1.0778
env1_first_0:                 episode reward: -89.6500,                 loss: nan
env1_second_0:                 episode reward: 89.6500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 344.75,                last time consumption/overall running time: 502.9788s / 75170.3663 s
env0_first_0:                 episode reward: -81.7000,                 loss: 0.5815
env0_second_0:                 episode reward: 81.7000,                 loss: 1.1339
env1_first_0:                 episode reward: -61.5000,                 loss: nan
env1_second_0:                 episode reward: 61.5000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 244.45,                last time consumption/overall running time: 358.5550s / 75528.9213 s
env0_first_0:                 episode reward: -79.0000,                 loss: 0.6543
env0_second_0:                 episode reward: 79.0000,                 loss: 1.1111
env1_first_0:                 episode reward: -86.9500,                 loss: nan
env1_second_0:                 episode reward: 86.9500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 313.25,                last time consumption/overall running time: 459.9982s / 75988.9195 s
env0_first_0:                 episode reward: -86.6500,                 loss: 0.6132
env0_second_0:                 episode reward: 86.6500,                 loss: 1.0461
env1_first_0:                 episode reward: -82.3000,                 loss: nan
env1_second_0:                 episode reward: 82.3000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 265.25,                last time consumption/overall running time: 392.3450s / 76381.2645 s
env0_first_0:                 episode reward: -88.6000,                 loss: 0.5968
env0_second_0:                 episode reward: 88.6000,                 loss: 1.0863
env1_first_0:                 episode reward: -77.3000,                 loss: nan
env1_second_0:                 episode reward: 77.3000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 238.05,                last time consumption/overall running time: 352.1208s / 76733.3853 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.6434
env0_second_0:                 episode reward: 86.4500,                 loss: 1.1354
env1_first_0:                 episode reward: -79.0500,                 loss: nan
env1_second_0:                 episode reward: 79.0500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 231.35,                last time consumption/overall running time: 341.8632s / 77075.2485 s
env0_first_0:                 episode reward: -86.7500,                 loss: 0.6127
env0_second_0:                 episode reward: 86.7500,                 loss: 1.0876
env1_first_0:                 episode reward: -88.7000,                 loss: nan
env1_second_0:                 episode reward: 88.7000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 227.4,                last time consumption/overall running time: 336.0814s / 77411.3299 s
env0_first_0:                 episode reward: -94.4500,                 loss: 0.5580
env0_second_0:                 episode reward: 94.4500,                 loss: 1.0350
env1_first_0:                 episode reward: -91.5500,                 loss: nan
env1_second_0:                 episode reward: 91.5500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 241.55,                last time consumption/overall running time: 354.9286s / 77766.2584 s
env0_first_0:                 episode reward: -74.7000,                 loss: 0.6029
env0_second_0:                 episode reward: 74.7000,                 loss: 0.9926
env1_first_0:                 episode reward: -94.6500,                 loss: nan
env1_second_0:                 episode reward: 94.6500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 233.2,                last time consumption/overall running time: 342.8034s / 78109.0618 s
env0_first_0:                 episode reward: -85.4000,                 loss: 0.5952
env0_second_0:                 episode reward: 85.4000,                 loss: 1.0324
env1_first_0:                 episode reward: -84.7000,                 loss: nan
env1_second_0:                 episode reward: 84.7000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 311.5,                last time consumption/overall running time: 456.0394s / 78565.1012 s
env0_first_0:                 episode reward: -73.1000,                 loss: 0.5960
env0_second_0:                 episode reward: 73.1000,                 loss: 0.8461
env1_first_0:                 episode reward: -72.4500,                 loss: nan
env1_second_0:                 episode reward: 72.4500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 299.95,                last time consumption/overall running time: 437.7272s / 79002.8285 s
env0_first_0:                 episode reward: -85.2500,                 loss: 0.5497
env0_second_0:                 episode reward: 85.2500,                 loss: 0.9392
env1_first_0:                 episode reward: -76.0500,                 loss: nan
env1_second_0:                 episode reward: 76.0500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 222.5,                last time consumption/overall running time: 329.5241s / 79332.3525 s
env0_first_0:                 episode reward: -92.2500,                 loss: 0.5081
env0_second_0:                 episode reward: 92.2500,                 loss: 0.8346
env1_first_0:                 episode reward: -79.7500,                 loss: nan
env1_second_0:                 episode reward: 79.7500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 227.55,                last time consumption/overall running time: 340.5848s / 79672.9374 s
env0_first_0:                 episode reward: -85.7000,                 loss: 0.5393
env0_second_0:                 episode reward: 85.7000,                 loss: 0.8971
env1_first_0:                 episode reward: -92.6500,                 loss: nan
env1_second_0:                 episode reward: 92.6500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 228.2,                last time consumption/overall running time: 336.4912s / 80009.4286 s
env0_first_0:                 episode reward: -85.9500,                 loss: 0.5217
env0_second_0:                 episode reward: 85.9500,                 loss: 0.9035
env1_first_0:                 episode reward: -94.5000,                 loss: nan
env1_second_0:                 episode reward: 94.5000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 230.8,                last time consumption/overall running time: 337.9975s / 80347.4261 s
env0_first_0:                 episode reward: -85.1000,                 loss: 0.5345
env0_second_0:                 episode reward: 85.1000,                 loss: 0.8442
env1_first_0:                 episode reward: -93.5000,                 loss: nan
env1_second_0:                 episode reward: 93.5000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 331.45,                last time consumption/overall running time: 487.9884s / 80835.4145 s
env0_first_0:                 episode reward: -81.8500,                 loss: 0.5033
env0_second_0:                 episode reward: 81.8500,                 loss: 0.8158
env1_first_0:                 episode reward: -68.1000,                 loss: nan
env1_second_0:                 episode reward: 68.1000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 241.65,                last time consumption/overall running time: 357.6856s / 81193.1001 s
env0_first_0:                 episode reward: -69.9500,                 loss: 0.4510
env0_second_0:                 episode reward: 69.9500,                 loss: 0.7791
env1_first_0:                 episode reward: -82.0000,                 loss: nan
env1_second_0:                 episode reward: 82.0000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 381.45,                last time consumption/overall running time: 560.6294s / 81753.7296 s
env0_first_0:                 episode reward: -74.8500,                 loss: 0.5374
env0_second_0:                 episode reward: 74.8500,                 loss: 0.8246
env1_first_0:                 episode reward: -75.8000,                 loss: nan
env1_second_0:                 episode reward: 75.8000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 567.75,                last time consumption/overall running time: 835.5764s / 82589.3060 s
env0_first_0:                 episode reward: -63.0000,                 loss: 0.4307
env0_second_0:                 episode reward: 63.0000,                 loss: 0.7595
env1_first_0:                 episode reward: -66.5000,                 loss: nan
env1_second_0:                 episode reward: 66.5000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 626.45,                last time consumption/overall running time: 918.2883s / 83507.5943 s
env0_first_0:                 episode reward: -71.7500,                 loss: 0.3981
env0_second_0:                 episode reward: 71.7500,                 loss: 0.6697
env1_first_0:                 episode reward: -69.8000,                 loss: nan
env1_second_0:                 episode reward: 69.8000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 429.8,                last time consumption/overall running time: 636.1268s / 84143.7211 s
env0_first_0:                 episode reward: -69.0500,                 loss: 0.3211
env0_second_0:                 episode reward: 69.0500,                 loss: 0.6183
env1_first_0:                 episode reward: -81.0000,                 loss: nan
env1_second_0:                 episode reward: 81.0000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 309.6,                last time consumption/overall running time: 453.3637s / 84597.0848 s
env0_first_0:                 episode reward: -88.7500,                 loss: 0.3369
env0_second_0:                 episode reward: 88.7500,                 loss: 0.5455
env1_first_0:                 episode reward: -85.6000,                 loss: nan
env1_second_0:                 episode reward: 85.6000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 393.45,                last time consumption/overall running time: 578.6700s / 85175.7548 s
env0_first_0:                 episode reward: -80.6000,                 loss: 0.3043
env0_second_0:                 episode reward: 80.6000,                 loss: 0.4834
env1_first_0:                 episode reward: -84.2000,                 loss: nan
env1_second_0:                 episode reward: 84.2000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 622.15,                last time consumption/overall running time: 910.8106s / 86086.5654 s
env0_first_0:                 episode reward: -70.9500,                 loss: 0.2983
env0_second_0:                 episode reward: 70.9500,                 loss: 0.4457
env1_first_0:                 episode reward: -65.9500,                 loss: nan
env1_second_0:                 episode reward: 65.9500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 299.35,                last time consumption/overall running time: 442.6846s / 86529.2500 s
env0_first_0:                 episode reward: -83.5000,                 loss: 0.3136
env0_second_0:                 episode reward: 83.5000,                 loss: 0.4493
env1_first_0:                 episode reward: -79.0500,                 loss: nan
env1_second_0:                 episode reward: 79.0500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 391.9,                last time consumption/overall running time: 578.1155s / 87107.3655 s
env0_first_0:                 episode reward: -77.9000,                 loss: 0.3430
env0_second_0:                 episode reward: 77.9000,                 loss: 0.4937
env1_first_0:                 episode reward: -84.2500,                 loss: nan
env1_second_0:                 episode reward: 84.2500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 404.8,                last time consumption/overall running time: 596.4312s / 87703.7967 s
env0_first_0:                 episode reward: -75.1500,                 loss: 0.3587
env0_second_0:                 episode reward: 75.1500,                 loss: 0.5268
env1_first_0:                 episode reward: -80.0500,                 loss: nan
env1_second_0:                 episode reward: 80.0500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 309.0,                last time consumption/overall running time: 455.2518s / 88159.0485 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.3832
env0_second_0:                 episode reward: 87.7000,                 loss: 0.5805
env1_first_0:                 episode reward: -78.7000,                 loss: nan
env1_second_0:                 episode reward: 78.7000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 316.4,                last time consumption/overall running time: 466.6329s / 88625.6814 s
env0_first_0:                 episode reward: -92.0500,                 loss: 0.3602
env0_second_0:                 episode reward: 92.0500,                 loss: 0.5227
env1_first_0:                 episode reward: -94.5000,                 loss: nan
env1_second_0:                 episode reward: 94.5000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 249.9,                last time consumption/overall running time: 367.8787s / 88993.5601 s
env0_first_0:                 episode reward: -92.2000,                 loss: 0.3722
env0_second_0:                 episode reward: 92.2000,                 loss: 0.5910
env1_first_0:                 episode reward: -88.2000,                 loss: nan
env1_second_0:                 episode reward: 88.2000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 544.15,                last time consumption/overall running time: 797.0444s / 89790.6045 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.4233
env0_second_0:                 episode reward: 87.7000,                 loss: 0.6191
env1_first_0:                 episode reward: -76.5000,                 loss: nan
env1_second_0:                 episode reward: 76.5000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 624.95,                last time consumption/overall running time: 917.7593s / 90708.3638 s
env0_first_0:                 episode reward: -84.4000,                 loss: 0.3820
env0_second_0:                 episode reward: 84.4000,                 loss: 0.5625
env1_first_0:                 episode reward: -83.2000,                 loss: nan
env1_second_0:                 episode reward: 83.2000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 478.9,                last time consumption/overall running time: 702.7248s / 91411.0886 s
env0_first_0:                 episode reward: -92.4500,                 loss: 0.3580
env0_second_0:                 episode reward: 92.4500,                 loss: 0.5069
env1_first_0:                 episode reward: -79.3000,                 loss: nan
env1_second_0:                 episode reward: 79.3000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 229.75,                last time consumption/overall running time: 341.8611s / 91752.9497 s
env0_first_0:                 episode reward: -81.0500,                 loss: 0.3535
env0_second_0:                 episode reward: 81.0500,                 loss: 0.4903
env1_first_0:                 episode reward: -91.7500,                 loss: nan
env1_second_0:                 episode reward: 91.7500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 244.15,                last time consumption/overall running time: 362.1455s / 92115.0952 s
env0_first_0:                 episode reward: -78.2500,                 loss: 0.3916
env0_second_0:                 episode reward: 78.2500,                 loss: 0.5364
env1_first_0:                 episode reward: -91.4500,                 loss: nan
env1_second_0:                 episode reward: 91.4500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 240.9,                last time consumption/overall running time: 361.1589s / 92476.2540 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.3939
env0_second_0:                 episode reward: 91.6500,                 loss: 0.5380
env1_first_0:                 episode reward: -83.5500,                 loss: nan
env1_second_0:                 episode reward: 83.5500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 231.75,                last time consumption/overall running time: 343.5314s / 92819.7854 s
env0_first_0:                 episode reward: -93.1500,                 loss: 0.4300
env0_second_0:                 episode reward: 93.1500,                 loss: 0.5895
env1_first_0:                 episode reward: -84.3500,                 loss: nan
env1_second_0:                 episode reward: 84.3500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 319.5,                last time consumption/overall running time: 474.8581s / 93294.6435 s
env0_first_0:                 episode reward: -88.0000,                 loss: 0.3899
env0_second_0:                 episode reward: 88.0000,                 loss: 0.5659
env1_first_0:                 episode reward: -89.9000,                 loss: nan
env1_second_0:                 episode reward: 89.9000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 233.7,                last time consumption/overall running time: 345.9890s / 93640.6326 s
env0_first_0:                 episode reward: -88.0500,                 loss: 0.4697
env0_second_0:                 episode reward: 88.0500,                 loss: 0.6611
env1_first_0:                 episode reward: -82.3500,                 loss: nan
env1_second_0:                 episode reward: 82.3500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 230.4,                last time consumption/overall running time: 341.8172s / 93982.4498 s
env0_first_0:                 episode reward: -90.1000,                 loss: 0.5729
env0_second_0:                 episode reward: 90.1000,                 loss: 0.7310
env1_first_0:                 episode reward: -71.7500,                 loss: nan
env1_second_0:                 episode reward: 71.7500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 319.4,                last time consumption/overall running time: 471.6389s / 94454.0887 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.5693
env0_second_0:                 episode reward: 90.4000,                 loss: 0.8062
env1_first_0:                 episode reward: -80.3000,                 loss: nan
env1_second_0:                 episode reward: 80.3000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 313.65,                last time consumption/overall running time: 461.4882s / 94915.5769 s
env0_first_0:                 episode reward: -86.5000,                 loss: 0.5954
env0_second_0:                 episode reward: 86.5000,                 loss: 0.8616
env1_first_0:                 episode reward: -88.8000,                 loss: nan
env1_second_0:                 episode reward: 88.8000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 230.4,                last time consumption/overall running time: 344.1334s / 95259.7103 s
env0_first_0:                 episode reward: -86.6500,                 loss: 0.6103
env0_second_0:                 episode reward: 86.6500,                 loss: 0.8486
env1_first_0:                 episode reward: -87.5000,                 loss: nan
env1_second_0:                 episode reward: 87.5000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 236.4,                last time consumption/overall running time: 348.1017s / 95607.8119 s
env0_first_0:                 episode reward: -85.3500,                 loss: 0.6286
env0_second_0:                 episode reward: 85.3500,                 loss: 0.9042
env1_first_0:                 episode reward: -83.9500,                 loss: nan
env1_second_0:                 episode reward: 83.9500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 221.85,                last time consumption/overall running time: 325.4557s / 95933.2677 s
env0_first_0:                 episode reward: -92.9000,                 loss: 0.5721
env0_second_0:                 episode reward: 92.9000,                 loss: 0.9112
env1_first_0:                 episode reward: -76.4500,                 loss: nan
env1_second_0:                 episode reward: 76.4500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 220.3,                last time consumption/overall running time: 324.7490s / 96258.0167 s
env0_first_0:                 episode reward: -91.1000,                 loss: 0.5696
env0_second_0:                 episode reward: 91.1000,                 loss: 0.8905
env1_first_0:                 episode reward: -85.0500,                 loss: nan
env1_second_0:                 episode reward: 85.0500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 259.6,                last time consumption/overall running time: 379.6639s / 96637.6806 s
env0_first_0:                 episode reward: -73.9000,                 loss: 0.5806
env0_second_0:                 episode reward: 73.9000,                 loss: 0.8424
env1_first_0:                 episode reward: -91.0000,                 loss: nan
env1_second_0:                 episode reward: 91.0000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 221.75,                last time consumption/overall running time: 326.0239s / 96963.7045 s
env0_first_0:                 episode reward: -84.4000,                 loss: 0.5669
env0_second_0:                 episode reward: 84.4000,                 loss: 0.8542
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 381.0,                last time consumption/overall running time: 560.0361s / 97523.7406 s
env0_first_0:                 episode reward: -82.1500,                 loss: 0.5565
env0_second_0:                 episode reward: 82.1500,                 loss: 0.8529
env1_first_0:                 episode reward: -88.3000,                 loss: nan
env1_second_0:                 episode reward: 88.3000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 300.4,                last time consumption/overall running time: 440.0593s / 97963.7999 s
env0_first_0:                 episode reward: -82.2000,                 loss: 0.5215
env0_second_0:                 episode reward: 82.2000,                 loss: 0.7866
env1_first_0:                 episode reward: -85.2000,                 loss: nan
env1_second_0:                 episode reward: 85.2000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 330.1,                last time consumption/overall running time: 474.4513s / 98438.2512 s
env0_first_0:                 episode reward: -77.6500,                 loss: 0.5489
env0_second_0:                 episode reward: 77.6500,                 loss: 0.7860
env1_first_0:                 episode reward: -86.0000,                 loss: nan
env1_second_0:                 episode reward: 86.0000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 239.85,                last time consumption/overall running time: 346.0897s / 98784.3409 s
env0_first_0:                 episode reward: -87.5000,                 loss: 0.5205
env0_second_0:                 episode reward: 87.5000,                 loss: 0.7920
env1_first_0:                 episode reward: -86.6500,                 loss: nan
env1_second_0:                 episode reward: 86.6500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 307.45,                last time consumption/overall running time: 444.5569s / 99228.8977 s
env0_first_0:                 episode reward: -87.0500,                 loss: 0.5533
env0_second_0:                 episode reward: 87.0500,                 loss: 0.8138
env1_first_0:                 episode reward: -90.3000,                 loss: nan
env1_second_0:                 episode reward: 90.3000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 469.0,                last time consumption/overall running time: 676.2857s / 99905.1835 s
env0_first_0:                 episode reward: -88.9000,                 loss: 0.4506
env0_second_0:                 episode reward: 88.9000,                 loss: 0.6731
env1_first_0:                 episode reward: -81.4500,                 loss: nan
env1_second_0:                 episode reward: 81.4500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 404.35,                last time consumption/overall running time: 582.3153s / 100487.4988 s
env0_first_0:                 episode reward: -89.0000,                 loss: 0.4324
env0_second_0:                 episode reward: 89.0000,                 loss: 0.6290
env1_first_0:                 episode reward: -92.8000,                 loss: nan
env1_second_0:                 episode reward: 92.8000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 890.7,                last time consumption/overall running time: 1285.4077s / 101772.9064 s
env0_first_0:                 episode reward: -78.3000,                 loss: 0.3528
env0_second_0:                 episode reward: 78.3000,                 loss: 0.5157
env1_first_0:                 episode reward: -81.3000,                 loss: nan
env1_second_0:                 episode reward: 81.3000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 416.1,                last time consumption/overall running time: 599.8016s / 102372.7080 s
env0_first_0:                 episode reward: -88.8000,                 loss: 0.3219
env0_second_0:                 episode reward: 88.8000,                 loss: 0.4445
env1_first_0:                 episode reward: -91.1500,                 loss: nan
env1_second_0:                 episode reward: 91.1500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 712.6,                last time consumption/overall running time: 1033.3561s / 103406.0642 s
env0_first_0:                 episode reward: -72.3000,                 loss: 0.3300
env0_second_0:                 episode reward: 72.3000,                 loss: 0.4282
env1_first_0:                 episode reward: -74.1000,                 loss: nan
env1_second_0:                 episode reward: 74.1000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 406.3,                last time consumption/overall running time: 591.2028s / 103997.2669 s
env0_first_0:                 episode reward: -89.0000,                 loss: 0.3261
env0_second_0:                 episode reward: 89.0000,                 loss: 0.4760
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 479.45,                last time consumption/overall running time: 689.2362s / 104686.5031 s
env0_first_0:                 episode reward: -95.1500,                 loss: 0.3370
env0_second_0:                 episode reward: 95.1500,                 loss: 0.4817
env1_first_0:                 episode reward: -78.6000,                 loss: nan
env1_second_0:                 episode reward: 78.6000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 616.1,                last time consumption/overall running time: 892.7006s / 105579.2038 s
env0_first_0:                 episode reward: -86.4000,                 loss: 0.3512
env0_second_0:                 episode reward: 86.4000,                 loss: 0.4864
env1_first_0:                 episode reward: -85.8500,                 loss: nan
env1_second_0:                 episode reward: 85.8500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 852.75,                last time consumption/overall running time: 1227.2167s / 106806.4205 s
env0_first_0:                 episode reward: -91.0500,                 loss: 0.3316
env0_second_0:                 episode reward: 91.0500,                 loss: 0.4416
env1_first_0:                 episode reward: -90.7000,                 loss: nan
env1_second_0:                 episode reward: 90.7000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 860.65,                last time consumption/overall running time: 1236.3987s / 108042.8191 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.2569
env0_second_0:                 episode reward: 88.5500,                 loss: 0.4126
env1_first_0:                 episode reward: -88.5000,                 loss: nan
env1_second_0:                 episode reward: 88.5000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 523.15,                last time consumption/overall running time: 762.0371s / 108804.8562 s
env0_first_0:                 episode reward: -80.9500,                 loss: 0.3037
env0_second_0:                 episode reward: 80.9500,                 loss: 0.4032
env1_first_0:                 episode reward: -84.2000,                 loss: nan
env1_second_0:                 episode reward: 84.2000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 523.6,                last time consumption/overall running time: 755.9716s / 109560.8278 s
env0_first_0:                 episode reward: -85.7000,                 loss: 0.3368
env0_second_0:                 episode reward: 85.7000,                 loss: 0.4897
env1_first_0:                 episode reward: -86.9500,                 loss: nan
env1_second_0:                 episode reward: 86.9500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 513.2,                last time consumption/overall running time: 742.0582s / 110302.8860 s
env0_first_0:                 episode reward: -91.6000,                 loss: 0.3637
env0_second_0:                 episode reward: 91.6000,                 loss: 0.5061
env1_first_0:                 episode reward: -84.4500,                 loss: nan
env1_second_0:                 episode reward: 84.4500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 393.85,                last time consumption/overall running time: 569.4137s / 110872.2997 s
env0_first_0:                 episode reward: -74.8000,                 loss: 0.4007
env0_second_0:                 episode reward: 74.8000,                 loss: 0.5365
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1094.35,                last time consumption/overall running time: 1574.9837s / 112447.2833 s
env0_first_0:                 episode reward: -87.0000,                 loss: 0.3816
env0_second_0:                 episode reward: 87.0000,                 loss: 0.5423
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1477.85,                last time consumption/overall running time: 2128.8593s / 114576.1426 s
env0_first_0:                 episode reward: -85.7000,                 loss: 0.2492
env0_second_0:                 episode reward: 85.7000,                 loss: 0.3412
env1_first_0:                 episode reward: -86.2000,                 loss: nan
env1_second_0:                 episode reward: 86.2000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1101.3,                last time consumption/overall running time: 1584.6317s / 116160.7743 s
env0_first_0:                 episode reward: -85.9500,                 loss: 0.1518
env0_second_0:                 episode reward: 85.9500,                 loss: 0.2135
env1_first_0:                 episode reward: -92.5000,                 loss: nan
env1_second_0:                 episode reward: 92.5000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 902.35,                last time consumption/overall running time: 1303.6345s / 117464.4088 s
env0_first_0:                 episode reward: -77.9500,                 loss: 0.1865
env0_second_0:                 episode reward: 77.9500,                 loss: 0.2767
env1_first_0:                 episode reward: -69.7000,                 loss: nan
env1_second_0:                 episode reward: 69.7000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 443.25,                last time consumption/overall running time: 643.2921s / 118107.7009 s
env0_first_0:                 episode reward: -75.4000,                 loss: 0.2166
env0_second_0:                 episode reward: 75.4000,                 loss: 0.3568
env1_first_0:                 episode reward: -79.7500,                 loss: nan
env1_second_0:                 episode reward: 79.7500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 587.3,                last time consumption/overall running time: 844.0927s / 118951.7936 s
env0_first_0:                 episode reward: -75.3500,                 loss: 0.2684
env0_second_0:                 episode reward: 75.3500,                 loss: 0.4413
env1_first_0:                 episode reward: -83.6500,                 loss: nan
env1_second_0:                 episode reward: 83.6500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1298.55,                last time consumption/overall running time: 1875.0798s / 120826.8733 s
env0_first_0:                 episode reward: -83.8000,                 loss: 0.2704
env0_second_0:                 episode reward: 83.8000,                 loss: 0.4524
env1_first_0:                 episode reward: -90.1000,                 loss: nan
env1_second_0:                 episode reward: 90.1000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 910.6,                last time consumption/overall running time: 1305.8355s / 122132.7089 s
env0_first_0:                 episode reward: -93.2000,                 loss: 0.2929
env0_second_0:                 episode reward: 93.2000,                 loss: 0.5170
env1_first_0:                 episode reward: -88.3500,                 loss: nan
env1_second_0:                 episode reward: 88.3500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1359.65,                last time consumption/overall running time: 1958.5364s / 124091.2452 s
env0_first_0:                 episode reward: -92.5000,                 loss: 0.2569
env0_second_0:                 episode reward: 92.5000,                 loss: 0.4366
env1_first_0:                 episode reward: -91.1000,                 loss: nan
env1_second_0:                 episode reward: 91.1000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1065.7,                last time consumption/overall running time: 1529.8063s / 125621.0515 s
env0_first_0:                 episode reward: -86.8500,                 loss: 0.2204
env0_second_0:                 episode reward: 86.8500,                 loss: 0.3831
env1_first_0:                 episode reward: -84.8000,                 loss: nan
env1_second_0:                 episode reward: 84.8000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 447.35,                last time consumption/overall running time: 637.2161s / 126258.2676 s
env0_first_0:                 episode reward: -86.4000,                 loss: 0.2361
env0_second_0:                 episode reward: 86.4000,                 loss: 0.3936
env1_first_0:                 episode reward: -69.0500,                 loss: nan
env1_second_0:                 episode reward: 69.0500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 398.4,                last time consumption/overall running time: 576.9282s / 126835.1958 s
env0_first_0:                 episode reward: -80.8500,                 loss: 0.3022
env0_second_0:                 episode reward: 80.8500,                 loss: 0.4914
env1_first_0:                 episode reward: -84.9000,                 loss: nan
env1_second_0:                 episode reward: 84.9000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 432.1,                last time consumption/overall running time: 625.4984s / 127460.6942 s
env0_first_0:                 episode reward: -93.3000,                 loss: 0.3488
env0_second_0:                 episode reward: 93.3000,                 loss: 0.5809
env1_first_0:                 episode reward: -74.0500,                 loss: nan
env1_second_0:                 episode reward: 74.0500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 647.55,                last time consumption/overall running time: 924.6497s / 128385.3439 s
env0_first_0:                 episode reward: -82.9500,                 loss: 0.3916
env0_second_0:                 episode reward: 82.9500,                 loss: 0.6211
env1_first_0:                 episode reward: -93.1000,                 loss: nan
env1_second_0:                 episode reward: 93.1000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 934.2,                last time consumption/overall running time: 1346.4174s / 129731.7613 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.4146
env0_second_0:                 episode reward: 91.6500,                 loss: 0.6353
env1_first_0:                 episode reward: -82.8500,                 loss: nan
env1_second_0:                 episode reward: 82.8500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1369.1,                last time consumption/overall running time: 1968.3582s / 131700.1195 s
env0_first_0:                 episode reward: -83.7000,                 loss: 0.3136
env0_second_0:                 episode reward: 83.7000,                 loss: 0.4780
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1471.4,                last time consumption/overall running time: 2106.5766s / 133806.6961 s
env0_first_0:                 episode reward: -92.1500,                 loss: 0.1911
env0_second_0:                 episode reward: 92.1500,                 loss: 0.3073
env1_first_0:                 episode reward: -93.9500,                 loss: nan
env1_second_0:                 episode reward: 93.9500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 491.9,                last time consumption/overall running time: 702.8819s / 134509.5780 s
env0_first_0:                 episode reward: -85.7500,                 loss: 0.1967
env0_second_0:                 episode reward: 85.7500,                 loss: 0.3429
env1_first_0:                 episode reward: -78.3500,                 loss: nan
env1_second_0:                 episode reward: 78.3500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 824.75,                last time consumption/overall running time: 1192.2886s / 135701.8666 s
env0_first_0:                 episode reward: -70.1000,                 loss: 0.2822
env0_second_0:                 episode reward: 70.1000,                 loss: 0.4400
env1_first_0:                 episode reward: -65.7000,                 loss: nan
env1_second_0:                 episode reward: 65.7000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1549.75,                last time consumption/overall running time: 2224.4082s / 137926.2748 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.2592
env0_second_0:                 episode reward: 6.8500,                 loss: 0.4353
env1_first_0:                 episode reward: -16.8000,                 loss: nan
env1_second_0:                 episode reward: 16.8000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 549.7,                last time consumption/overall running time: 792.3122s / 138718.5870 s
env0_first_0:                 episode reward: -70.6000,                 loss: 0.2009
env0_second_0:                 episode reward: 70.6000,                 loss: 0.3517
env1_first_0:                 episode reward: -77.3500,                 loss: nan
env1_second_0:                 episode reward: 77.3500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 277.85,                last time consumption/overall running time: 396.5876s / 139115.1746 s
env0_first_0:                 episode reward: -78.2500,                 loss: 0.2243
env0_second_0:                 episode reward: 78.2500,                 loss: 0.3638
env1_first_0:                 episode reward: -77.2000,                 loss: nan
env1_second_0:                 episode reward: 77.2000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 404.2,                last time consumption/overall running time: 577.2231s / 139692.3978 s
env0_first_0:                 episode reward: -78.0500,                 loss: 0.2984
env0_second_0:                 episode reward: 78.0500,                 loss: 0.5051
env1_first_0:                 episode reward: -86.4500,                 loss: nan
env1_second_0:                 episode reward: 86.4500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 565.8,                last time consumption/overall running time: 812.7343s / 140505.1321 s
env0_first_0:                 episode reward: -93.3500,                 loss: 0.3492
env0_second_0:                 episode reward: 93.3500,                 loss: 0.6380
env1_first_0:                 episode reward: -78.6500,                 loss: nan
env1_second_0:                 episode reward: 78.6500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 292.25,                last time consumption/overall running time: 421.5154s / 140926.6475 s
env0_first_0:                 episode reward: -75.1000,                 loss: 0.3977
env0_second_0:                 episode reward: 75.1000,                 loss: 0.5907
env1_first_0:                 episode reward: -93.2500,                 loss: nan
env1_second_0:                 episode reward: 93.2500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 256.95,                last time consumption/overall running time: 375.1530s / 141301.8006 s
env0_first_0:                 episode reward: -92.9500,                 loss: 0.4638
env0_second_0:                 episode reward: 92.9500,                 loss: 0.7265
env1_first_0:                 episode reward: -83.7000,                 loss: nan
env1_second_0:                 episode reward: 83.7000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 287.6,                last time consumption/overall running time: 417.2913s / 141719.0919 s
env0_first_0:                 episode reward: -80.3500,                 loss: 0.4950
env0_second_0:                 episode reward: 80.3500,                 loss: 0.7261
env1_first_0:                 episode reward: -79.2500,                 loss: nan
env1_second_0:                 episode reward: 79.2500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 226.7,                last time consumption/overall running time: 326.9609s / 142046.0528 s
env0_first_0:                 episode reward: -94.4500,                 loss: 0.5119
env0_second_0:                 episode reward: 94.4500,                 loss: 0.7353
env1_first_0:                 episode reward: -85.8500,                 loss: nan
env1_second_0:                 episode reward: 85.8500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 315.65,                last time consumption/overall running time: 457.1908s / 142503.2436 s
env0_first_0:                 episode reward: -89.7500,                 loss: 0.5551
env0_second_0:                 episode reward: 89.7500,                 loss: 0.7763
env1_first_0:                 episode reward: -83.3500,                 loss: nan
env1_second_0:                 episode reward: 83.3500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 475.55,                last time consumption/overall running time: 685.8576s / 143189.1012 s
env0_first_0:                 episode reward: -80.0000,                 loss: 0.5188
env0_second_0:                 episode reward: 80.0000,                 loss: 0.7398
env1_first_0:                 episode reward: -62.7000,                 loss: nan
env1_second_0:                 episode reward: 62.7000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 291.85,                last time consumption/overall running time: 421.2277s / 143610.3290 s
env0_first_0:                 episode reward: -87.5500,                 loss: 0.5543
env0_second_0:                 episode reward: 87.5500,                 loss: 0.7615
env1_first_0:                 episode reward: -88.0500,                 loss: nan
env1_second_0:                 episode reward: 88.0500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 370.3,                last time consumption/overall running time: 533.1681s / 144143.4971 s
env0_first_0:                 episode reward: -83.8500,                 loss: 0.5415
env0_second_0:                 episode reward: 83.8500,                 loss: 0.7866
env1_first_0:                 episode reward: -89.3500,                 loss: nan
env1_second_0:                 episode reward: 89.3500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 257.95,                last time consumption/overall running time: 370.4731s / 144513.9702 s
env0_first_0:                 episode reward: -76.7500,                 loss: 0.6133
env0_second_0:                 episode reward: 76.7500,                 loss: 0.8121
env1_first_0:                 episode reward: -84.0500,                 loss: nan
env1_second_0:                 episode reward: 84.0500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 552.8,                last time consumption/overall running time: 791.1122s / 145305.0824 s
env0_first_0:                 episode reward: -87.3500,                 loss: 0.5748
env0_second_0:                 episode reward: 87.3500,                 loss: 0.7832
env1_first_0:                 episode reward: -83.2000,                 loss: nan
env1_second_0:                 episode reward: 83.2000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 457.4,                last time consumption/overall running time: 646.5118s / 145951.5942 s
env0_first_0:                 episode reward: -86.1000,                 loss: 0.5003
env0_second_0:                 episode reward: 86.1000,                 loss: 0.6635
env1_first_0:                 episode reward: -85.8000,                 loss: nan
env1_second_0:                 episode reward: 85.8000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 669.45,                last time consumption/overall running time: 948.9863s / 146900.5805 s
env0_first_0:                 episode reward: -87.9000,                 loss: 0.4821
env0_second_0:                 episode reward: 87.9000,                 loss: 0.6411
env1_first_0:                 episode reward: -78.3000,                 loss: nan
env1_second_0:                 episode reward: 78.3000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1108.3,                last time consumption/overall running time: 1545.7305s / 148446.3110 s
env0_first_0:                 episode reward: -84.2500,                 loss: 0.3481
env0_second_0:                 episode reward: 84.2500,                 loss: 0.4730
env1_first_0:                 episode reward: -86.8000,                 loss: nan
env1_second_0:                 episode reward: 86.8000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 418.25,                last time consumption/overall running time: 586.6870s / 149032.9979 s
env0_first_0:                 episode reward: -78.2500,                 loss: 0.2778
env0_second_0:                 episode reward: 78.2500,                 loss: 0.3145
env1_first_0:                 episode reward: -84.0000,                 loss: nan
env1_second_0:                 episode reward: 84.0000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1160.4,                last time consumption/overall running time: 1628.1095s / 150661.1074 s
env0_first_0:                 episode reward: -81.5500,                 loss: 0.2578
env0_second_0:                 episode reward: 81.5500,                 loss: 0.3246
env1_first_0:                 episode reward: -90.3500,                 loss: nan
env1_second_0:                 episode reward: 90.3500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1332.25,                last time consumption/overall running time: 1860.4351s / 152521.5425 s
env0_first_0:                 episode reward: -85.8500,                 loss: 0.2701
env0_second_0:                 episode reward: 85.8500,                 loss: 0.3293
env1_first_0:                 episode reward: -84.5000,                 loss: nan
env1_second_0:                 episode reward: 84.5000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1097.15,                last time consumption/overall running time: 1535.6446s / 154057.1872 s
env0_first_0:                 episode reward: -82.1500,                 loss: 0.2756
env0_second_0:                 episode reward: 82.1500,                 loss: 0.3530
env1_first_0:                 episode reward: -79.0500,                 loss: nan
env1_second_0:                 episode reward: 79.0500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 712.2,                last time consumption/overall running time: 985.9301s / 155043.1172 s
env0_first_0:                 episode reward: -86.6000,                 loss: 0.3379
env0_second_0:                 episode reward: 86.6000,                 loss: 0.4129
env1_first_0:                 episode reward: -88.5000,                 loss: nan
env1_second_0:                 episode reward: 88.5000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 393.05,                last time consumption/overall running time: 548.7508s / 155591.8680 s
env0_first_0:                 episode reward: -86.0500,                 loss: 0.3863
env0_second_0:                 episode reward: 86.0500,                 loss: 0.4907
env1_first_0:                 episode reward: -83.1000,                 loss: nan
env1_second_0:                 episode reward: 83.1000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 571.45,                last time consumption/overall running time: 793.9745s / 156385.8425 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.3986
env0_second_0:                 episode reward: 86.4500,                 loss: 0.5353
env1_first_0:                 episode reward: -95.1000,                 loss: nan
env1_second_0:                 episode reward: 95.1000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 767.3,                last time consumption/overall running time: 1064.4358s / 157450.2783 s
env0_first_0:                 episode reward: -81.0500,                 loss: 0.3799
env0_second_0:                 episode reward: 81.0500,                 loss: 0.5162
env1_first_0:                 episode reward: -82.6000,                 loss: nan
env1_second_0:                 episode reward: 82.6000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 392.15,                last time consumption/overall running time: 544.3716s / 157994.6499 s
env0_first_0:                 episode reward: -93.5500,                 loss: 0.3438
env0_second_0:                 episode reward: 93.5500,                 loss: 0.4709
env1_first_0:                 episode reward: -75.8000,                 loss: nan
env1_second_0:                 episode reward: 75.8000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 442.15,                last time consumption/overall running time: 618.8453s / 158613.4951 s
env0_first_0:                 episode reward: -73.2000,                 loss: 0.3555
env0_second_0:                 episode reward: 73.2000,                 loss: 0.5557
env1_first_0:                 episode reward: -82.7000,                 loss: nan
env1_second_0:                 episode reward: 82.7000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 391.45,                last time consumption/overall running time: 545.3046s / 159158.7998 s
env0_first_0:                 episode reward: -91.1500,                 loss: 0.3857
env0_second_0:                 episode reward: 91.1500,                 loss: 0.5889
env1_first_0:                 episode reward: -79.3500,                 loss: nan
env1_second_0:                 episode reward: 79.3500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 417.35,                last time consumption/overall running time: 583.7024s / 159742.5022 s
env0_first_0:                 episode reward: -89.1000,                 loss: 0.4199
env0_second_0:                 episode reward: 89.1000,                 loss: 0.5811
env1_first_0:                 episode reward: -86.5500,                 loss: nan
env1_second_0:                 episode reward: 86.5500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 384.0,                last time consumption/overall running time: 541.6262s / 160284.1284 s
env0_first_0:                 episode reward: -85.7000,                 loss: 0.4692
env0_second_0:                 episode reward: 85.7000,                 loss: 0.6553
env1_first_0:                 episode reward: -75.5500,                 loss: nan
env1_second_0:                 episode reward: 75.5500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 551.15,                last time consumption/overall running time: 767.6058s / 161051.7342 s
env0_first_0:                 episode reward: -78.5000,                 loss: 0.4761
env0_second_0:                 episode reward: 78.5000,                 loss: 0.6679
env1_first_0:                 episode reward: -84.8500,                 loss: nan
env1_second_0:                 episode reward: 84.8500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1018.6,                last time consumption/overall running time: 1405.5150s / 162457.2492 s
env0_first_0:                 episode reward: -87.3500,                 loss: 0.4646
env0_second_0:                 episode reward: 87.3500,                 loss: 0.6454
env1_first_0:                 episode reward: -86.4000,                 loss: nan
env1_second_0:                 episode reward: 86.4000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 814.55,                last time consumption/overall running time: 1127.2559s / 163584.5051 s
env0_first_0:                 episode reward: -79.0000,                 loss: 0.3748
env0_second_0:                 episode reward: 79.0000,                 loss: 0.5183
env1_first_0:                 episode reward: -71.6000,                 loss: nan
env1_second_0:                 episode reward: 71.6000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 772.85,                last time consumption/overall running time: 1062.9591s / 164647.4642 s
env0_first_0:                 episode reward: -72.7000,                 loss: 0.3539
env0_second_0:                 episode reward: 72.7000,                 loss: 0.4811
env1_first_0:                 episode reward: -83.2500,                 loss: nan
env1_second_0:                 episode reward: 83.2500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 917.7,                last time consumption/overall running time: 1265.3032s / 165912.7674 s
env0_first_0:                 episode reward: -86.7500,                 loss: 0.3579
env0_second_0:                 episode reward: 86.7500,                 loss: 0.4544
env1_first_0:                 episode reward: -80.2500,                 loss: nan
env1_second_0:                 episode reward: 80.2500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 471.55,                last time consumption/overall running time: 649.1137s / 166561.8811 s
env0_first_0:                 episode reward: -76.9500,                 loss: 0.4129
env0_second_0:                 episode reward: 76.9500,                 loss: 0.5203
env1_first_0:                 episode reward: -89.7000,                 loss: nan
env1_second_0:                 episode reward: 89.7000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1313.45,                last time consumption/overall running time: 1813.2147s / 168375.0958 s
env0_first_0:                 episode reward: -75.8000,                 loss: 0.3588
env0_second_0:                 episode reward: 75.8000,                 loss: 0.4398
env1_first_0:                 episode reward: -84.2000,                 loss: nan
env1_second_0:                 episode reward: 84.2000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1073.0,                last time consumption/overall running time: 1478.5576s / 169853.6534 s
env0_first_0:                 episode reward: -89.9000,                 loss: 0.3576
env0_second_0:                 episode reward: 89.9000,                 loss: 0.4454
env1_first_0:                 episode reward: -78.7000,                 loss: nan
env1_second_0:                 episode reward: 78.7000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1099.1,                last time consumption/overall running time: 1500.6273s / 171354.2807 s
env0_first_0:                 episode reward: -74.8500,                 loss: 0.3849
env0_second_0:                 episode reward: 74.8500,                 loss: 0.4666
env1_first_0:                 episode reward: -61.4000,                 loss: nan
env1_second_0:                 episode reward: 61.4000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 741.9,                last time consumption/overall running time: 1019.2589s / 172373.5396 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.3955
env0_second_0:                 episode reward: 90.4000,                 loss: 0.4677
env1_first_0:                 episode reward: -85.6500,                 loss: nan
env1_second_0:                 episode reward: 85.6500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 890.85,                last time consumption/overall running time: 1218.9244s / 173592.4640 s
env0_first_0:                 episode reward: -81.0000,                 loss: 0.3846
env0_second_0:                 episode reward: 81.0000,                 loss: 0.4773
env1_first_0:                 episode reward: -81.9000,                 loss: nan
env1_second_0:                 episode reward: 81.9000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 880.75,                last time consumption/overall running time: 1208.8713s / 174801.3354 s
env0_first_0:                 episode reward: -93.0500,                 loss: 0.3717
env0_second_0:                 episode reward: 93.0500,                 loss: 0.4656
env1_first_0:                 episode reward: -85.9000,                 loss: nan
env1_second_0:                 episode reward: 85.9000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 539.7,                last time consumption/overall running time: 740.1603s / 175541.4956 s
env0_first_0:                 episode reward: -89.2000,                 loss: 0.4122
env0_second_0:                 episode reward: 89.2000,                 loss: 0.4950
env1_first_0:                 episode reward: -89.1000,                 loss: nan
env1_second_0:                 episode reward: 89.1000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 349.7,                last time consumption/overall running time: 479.6268s / 176021.1224 s
env0_first_0:                 episode reward: -82.9000,                 loss: 0.4190
env0_second_0:                 episode reward: 82.9000,                 loss: 0.4618
env1_first_0:                 episode reward: -76.8000,                 loss: nan
env1_second_0:                 episode reward: 76.8000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 331.05,                last time consumption/overall running time: 456.4139s / 176477.5363 s
env0_first_0:                 episode reward: -94.7000,                 loss: 0.4630
env0_second_0:                 episode reward: 94.7000,                 loss: 0.4485
env1_first_0:                 episode reward: -78.0000,                 loss: nan
env1_second_0:                 episode reward: 78.0000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 315.75,                last time consumption/overall running time: 437.4139s / 176914.9502 s
env0_first_0:                 episode reward: -74.7500,                 loss: 0.4769
env0_second_0:                 episode reward: 74.7500,                 loss: 0.5178
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 569.95,                last time consumption/overall running time: 780.3866s / 177695.3368 s
env0_first_0:                 episode reward: -90.1500,                 loss: 0.4973
env0_second_0:                 episode reward: 90.1500,                 loss: 0.5152
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 341.65,                last time consumption/overall running time: 467.9382s / 178163.2749 s
env0_first_0:                 episode reward: -86.2000,                 loss: 0.4517
env0_second_0:                 episode reward: 86.2000,                 loss: 0.5175
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 455.4,                last time consumption/overall running time: 615.3050s / 178778.5800 s
env0_first_0:                 episode reward: -86.9000,                 loss: 0.4596
env0_second_0:                 episode reward: 86.9000,                 loss: 0.5449
env1_first_0:                 episode reward: -92.5500,                 loss: nan
env1_second_0:                 episode reward: 92.5500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 474.05,                last time consumption/overall running time: 648.2400s / 179426.8200 s
env0_first_0:                 episode reward: -92.9000,                 loss: 0.4473
env0_second_0:                 episode reward: 92.9000,                 loss: 0.5674
env1_first_0:                 episode reward: -90.9000,                 loss: nan
env1_second_0:                 episode reward: 90.9000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1110.2,                last time consumption/overall running time: 1509.5154s / 180936.3354 s
env0_first_0:                 episode reward: -88.4500,                 loss: 0.3584
env0_second_0:                 episode reward: 88.4500,                 loss: 0.4441
env1_first_0:                 episode reward: -83.6500,                 loss: nan
env1_second_0:                 episode reward: 83.6500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 878.9,                last time consumption/overall running time: 1191.6361s / 182127.9715 s
env0_first_0:                 episode reward: -87.4500,                 loss: 0.2632
env0_second_0:                 episode reward: 87.4500,                 loss: 0.3613
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 871.7,                last time consumption/overall running time: 1184.9979s / 183312.9695 s
env0_first_0:                 episode reward: -87.6500,                 loss: 0.2119
env0_second_0:                 episode reward: 87.6500,                 loss: 0.2918
env1_first_0:                 episode reward: -83.0500,                 loss: nan
env1_second_0:                 episode reward: 83.0500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 808.05,                last time consumption/overall running time: 1097.8585s / 184410.8279 s
env0_first_0:                 episode reward: -88.3500,                 loss: 0.2139
env0_second_0:                 episode reward: 88.3500,                 loss: 0.2719
env1_first_0:                 episode reward: -82.0500,                 loss: nan
env1_second_0:                 episode reward: 82.0500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 749.5,                last time consumption/overall running time: 1020.0184s / 185430.8464 s
env0_first_0:                 episode reward: -88.8000,                 loss: 0.2356
env0_second_0:                 episode reward: 88.8000,                 loss: 0.3360
env1_first_0:                 episode reward: -73.3000,                 loss: nan
env1_second_0:                 episode reward: 73.3000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 299.9,                last time consumption/overall running time: 407.5184s / 185838.3647 s
env0_first_0:                 episode reward: -78.2000,                 loss: 0.2681
env0_second_0:                 episode reward: 78.2000,                 loss: 0.3349
env1_first_0:                 episode reward: -89.3500,                 loss: nan
env1_second_0:                 episode reward: 89.3500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 438.15,                last time consumption/overall running time: 598.3245s / 186436.6892 s
env0_first_0:                 episode reward: -87.2500,                 loss: 0.2837
env0_second_0:                 episode reward: 87.2500,                 loss: 0.3528
env1_first_0:                 episode reward: -86.2500,                 loss: nan
env1_second_0:                 episode reward: 86.2500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 549.05,                last time consumption/overall running time: 740.2032s / 187176.8924 s
env0_first_0:                 episode reward: -84.0500,                 loss: 0.3515
env0_second_0:                 episode reward: 84.0500,                 loss: 0.4310
env1_first_0:                 episode reward: -82.0500,                 loss: nan
env1_second_0:                 episode reward: 82.0500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 368.0,                last time consumption/overall running time: 494.1432s / 187671.0355 s
env0_first_0:                 episode reward: -92.8500,                 loss: 0.3506
env0_second_0:                 episode reward: 92.8500,                 loss: 0.4017
env1_first_0:                 episode reward: -89.0000,                 loss: nan
env1_second_0:                 episode reward: 89.0000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 243.7,                last time consumption/overall running time: 330.3546s / 188001.3902 s
env0_first_0:                 episode reward: -90.5500,                 loss: 0.3428
env0_second_0:                 episode reward: 90.5500,                 loss: 0.4411
env1_first_0:                 episode reward: -79.6000,                 loss: nan
env1_second_0:                 episode reward: 79.6000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 544.7,                last time consumption/overall running time: 731.6299s / 188733.0200 s
env0_first_0:                 episode reward: -91.3500,                 loss: 0.3265
env0_second_0:                 episode reward: 91.3500,                 loss: 0.4229
env1_first_0:                 episode reward: -85.6000,                 loss: nan
env1_second_0:                 episode reward: 85.6000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1446.3,                last time consumption/overall running time: 1955.2901s / 190688.3101 s
env0_first_0:                 episode reward: -84.9500,                 loss: 0.3049
env0_second_0:                 episode reward: 84.9500,                 loss: 0.3449
env1_first_0:                 episode reward: -86.2500,                 loss: nan
env1_second_0:                 episode reward: 86.2500,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1069.55,                last time consumption/overall running time: 1428.9177s / 192117.2277 s
env0_first_0:                 episode reward: -86.1000,                 loss: 0.1937
env0_second_0:                 episode reward: 86.1000,                 loss: 0.2375
env1_first_0:                 episode reward: -79.1500,                 loss: nan
env1_second_0:                 episode reward: 79.1500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 715.3,                last time consumption/overall running time: 942.5814s / 193059.8092 s
env0_first_0:                 episode reward: -88.2000,                 loss: 0.2023
env0_second_0:                 episode reward: 88.2000,                 loss: 0.2793
env1_first_0:                 episode reward: -83.8500,                 loss: nan
env1_second_0:                 episode reward: 83.8500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 601.75,                last time consumption/overall running time: 796.1428s / 193855.9520 s
env0_first_0:                 episode reward: -85.5000,                 loss: 0.2219
env0_second_0:                 episode reward: 85.5000,                 loss: 0.2836
env1_first_0:                 episode reward: -84.1500,                 loss: nan
env1_second_0:                 episode reward: 84.1500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 449.1,                last time consumption/overall running time: 595.2956s / 194451.2476 s
env0_first_0:                 episode reward: -78.1000,                 loss: 0.2902
env0_second_0:                 episode reward: 78.1000,                 loss: 0.3535
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 306.55,                last time consumption/overall running time: 405.8672s / 194857.1148 s
env0_first_0:                 episode reward: -89.7500,                 loss: 0.3165
env0_second_0:                 episode reward: 89.7500,                 loss: 0.4176
env1_first_0:                 episode reward: -85.1500,                 loss: nan
env1_second_0:                 episode reward: 85.1500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 269.1,                last time consumption/overall running time: 356.9325s / 195214.0473 s
env0_first_0:                 episode reward: -91.1000,                 loss: 0.3379
env0_second_0:                 episode reward: 91.1000,                 loss: 0.4606
env1_first_0:                 episode reward: -76.7000,                 loss: nan
env1_second_0:                 episode reward: 76.7000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 394.65,                last time consumption/overall running time: 515.4175s / 195729.4648 s
env0_first_0:                 episode reward: -94.2000,                 loss: 0.3556
env0_second_0:                 episode reward: 94.2000,                 loss: 0.4810
env1_first_0:                 episode reward: -85.4000,                 loss: nan
env1_second_0:                 episode reward: 85.4000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 649.05,                last time consumption/overall running time: 849.3725s / 196578.8372 s
env0_first_0:                 episode reward: -92.7500,                 loss: 0.3139
env0_second_0:                 episode reward: 92.7500,                 loss: 0.4628
env1_first_0:                 episode reward: -92.9000,                 loss: nan
env1_second_0:                 episode reward: 92.9000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 959.25,                last time consumption/overall running time: 1265.4763s / 197844.3135 s
env0_first_0:                 episode reward: -90.0000,                 loss: 0.3284
env0_second_0:                 episode reward: 90.0000,                 loss: 0.4417
env1_first_0:                 episode reward: -78.7000,                 loss: nan
env1_second_0:                 episode reward: 78.7000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 805.45,                last time consumption/overall running time: 1051.7948s / 198896.1082 s
env0_first_0:                 episode reward: -84.8000,                 loss: 0.2941
env0_second_0:                 episode reward: 84.8000,                 loss: 0.4323
env1_first_0:                 episode reward: -86.4000,                 loss: nan
env1_second_0:                 episode reward: 86.4000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 731.55,                last time consumption/overall running time: 950.9682s / 199847.0764 s
env0_first_0:                 episode reward: -84.7500,                 loss: 0.3037
env0_second_0:                 episode reward: 84.7500,                 loss: 0.4051
env1_first_0:                 episode reward: -87.2000,                 loss: nan
env1_second_0:                 episode reward: 87.2000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 486.25,                last time consumption/overall running time: 617.5858s / 200464.6623 s
env0_first_0:                 episode reward: -87.3000,                 loss: 0.3192
env0_second_0:                 episode reward: 87.3000,                 loss: 0.4426
env1_first_0:                 episode reward: -88.3500,                 loss: nan
env1_second_0:                 episode reward: 88.3500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 578.85,                last time consumption/overall running time: 733.7679s / 201198.4302 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.3148
env0_second_0:                 episode reward: 91.6500,                 loss: 0.4618
env1_first_0:                 episode reward: -87.4000,                 loss: nan
env1_second_0:                 episode reward: 87.4000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 665.45,                last time consumption/overall running time: 848.3805s / 202046.8107 s
env0_first_0:                 episode reward: -84.4000,                 loss: 0.3068
env0_second_0:                 episode reward: 84.4000,                 loss: 0.3999
env1_first_0:                 episode reward: -89.5500,                 loss: nan
env1_second_0:                 episode reward: 89.5500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 877.25,                last time consumption/overall running time: 1107.9105s / 203154.7212 s
env0_first_0:                 episode reward: -78.5000,                 loss: 0.2981
env0_second_0:                 episode reward: 78.5000,                 loss: 0.4481
env1_first_0:                 episode reward: -81.8500,                 loss: nan
env1_second_0:                 episode reward: 81.8500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 712.0,                last time consumption/overall running time: 915.2929s / 204070.0142 s
env0_first_0:                 episode reward: -62.8500,                 loss: 0.2679
env0_second_0:                 episode reward: 62.8500,                 loss: 0.4416
env1_first_0:                 episode reward: -73.6500,                 loss: nan
env1_second_0:                 episode reward: 73.6500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 300.95,                last time consumption/overall running time: 386.0811s / 204456.0953 s
env0_first_0:                 episode reward: -81.0000,                 loss: 0.3234
env0_second_0:                 episode reward: 81.0000,                 loss: 0.4283
env1_first_0:                 episode reward: -78.3000,                 loss: nan
env1_second_0:                 episode reward: 78.3000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 311.75,                last time consumption/overall running time: 395.7730s / 204851.8683 s
env0_first_0:                 episode reward: -74.1000,                 loss: 0.3403
env0_second_0:                 episode reward: 74.1000,                 loss: 0.4990
env1_first_0:                 episode reward: -80.9500,                 loss: nan
env1_second_0:                 episode reward: 80.9500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 304.75,                last time consumption/overall running time: 389.8619s / 205241.7301 s
env0_first_0:                 episode reward: -91.8500,                 loss: 0.3649
env0_second_0:                 episode reward: 91.8500,                 loss: 0.5493
env1_first_0:                 episode reward: -79.2500,                 loss: nan
env1_second_0:                 episode reward: 79.2500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 514.2,                last time consumption/overall running time: 655.6511s / 205897.3812 s
env0_first_0:                 episode reward: -79.5000,                 loss: 0.3693
env0_second_0:                 episode reward: 79.5000,                 loss: 0.6154
env1_first_0:                 episode reward: -76.1000,                 loss: nan
env1_second_0:                 episode reward: 76.1000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 467.25,                last time consumption/overall running time: 594.1818s / 206491.5630 s
env0_first_0:                 episode reward: -82.1000,                 loss: 0.4276
env0_second_0:                 episode reward: 82.1000,                 loss: 0.5979
env1_first_0:                 episode reward: -90.2500,                 loss: nan
env1_second_0:                 episode reward: 90.2500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 449.55,                last time consumption/overall running time: 569.8868s / 207061.4498 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.5060
env0_second_0:                 episode reward: 92.4000,                 loss: 0.5698
env1_first_0:                 episode reward: -81.7500,                 loss: nan
env1_second_0:                 episode reward: 81.7500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 231.0,                last time consumption/overall running time: 295.2725s / 207356.7223 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.4316
env0_second_0:                 episode reward: 90.4000,                 loss: 0.5830
env1_first_0:                 episode reward: -96.8500,                 loss: nan
env1_second_0:                 episode reward: 96.8500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 378.25,                last time consumption/overall running time: 485.3654s / 207842.0877 s
env0_first_0:                 episode reward: -79.6000,                 loss: 0.4316
env0_second_0:                 episode reward: 79.6000,                 loss: 0.5264
env1_first_0:                 episode reward: -93.4000,                 loss: nan
env1_second_0:                 episode reward: 93.4000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 780.4,                last time consumption/overall running time: 993.2789s / 208835.3666 s
env0_first_0:                 episode reward: -90.6500,                 loss: 0.4061
env0_second_0:                 episode reward: 90.6500,                 loss: 0.4636
env1_first_0:                 episode reward: -88.2000,                 loss: nan
env1_second_0:                 episode reward: 88.2000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 447.4,                last time consumption/overall running time: 572.4250s / 209407.7916 s
env0_first_0:                 episode reward: -94.9000,                 loss: 0.3407
env0_second_0:                 episode reward: 94.9000,                 loss: 0.4013
env1_first_0:                 episode reward: -77.2500,                 loss: nan
env1_second_0:                 episode reward: 77.2500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 227.45,                last time consumption/overall running time: 292.6916s / 209700.4832 s
env0_first_0:                 episode reward: -82.7000,                 loss: 0.3312
env0_second_0:                 episode reward: 82.7000,                 loss: 0.3928
env1_first_0:                 episode reward: -92.1500,                 loss: nan
env1_second_0:                 episode reward: 92.1500,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 281.85,                last time consumption/overall running time: 359.5915s / 210060.0747 s
env0_first_0:                 episode reward: -79.3500,                 loss: 0.3120
env0_second_0:                 episode reward: 79.3500,                 loss: 0.4351
env1_first_0:                 episode reward: -71.8000,                 loss: nan
env1_second_0:                 episode reward: 71.8000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 520.05,                last time consumption/overall running time: 657.5563s / 210717.6311 s
env0_first_0:                 episode reward: -82.4500,                 loss: 0.3209
env0_second_0:                 episode reward: 82.4500,                 loss: 0.4157
env1_first_0:                 episode reward: -79.0000,                 loss: nan
env1_second_0:                 episode reward: 79.0000,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 362.05,                last time consumption/overall running time: 458.0788s / 211175.7099 s
env0_first_0:                 episode reward: -69.4500,                 loss: 0.3135
env0_second_0:                 episode reward: 69.4500,                 loss: 0.3997
env1_first_0:                 episode reward: -96.6500,                 loss: nan
env1_second_0:                 episode reward: 96.6500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 390.8,                last time consumption/overall running time: 499.4721s / 211675.1820 s
env0_first_0:                 episode reward: -92.4500,                 loss: 0.3632
env0_second_0:                 episode reward: 92.4500,                 loss: 0.4661
env1_first_0:                 episode reward: -93.8500,                 loss: nan
env1_second_0:                 episode reward: 93.8500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 254.75,                last time consumption/overall running time: 325.3800s / 212000.5619 s
env0_first_0:                 episode reward: -90.9500,                 loss: 0.3203
env0_second_0:                 episode reward: 90.9500,                 loss: 0.4783
env1_first_0:                 episode reward: -92.8500,                 loss: nan
env1_second_0:                 episode reward: 92.8500,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 320.9,                last time consumption/overall running time: 407.7897s / 212408.3516 s
env0_first_0:                 episode reward: -86.8500,                 loss: 0.3799
env0_second_0:                 episode reward: 86.8500,                 loss: 0.4642
env1_first_0:                 episode reward: -94.0000,                 loss: nan
env1_second_0:                 episode reward: 94.0000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 406.5,                last time consumption/overall running time: 519.1717s / 212927.5233 s
env0_first_0:                 episode reward: -78.4000,                 loss: 0.3242
env0_second_0:                 episode reward: 78.4000,                 loss: 0.4121
env1_first_0:                 episode reward: -97.3500,                 loss: nan
env1_second_0:                 episode reward: 97.3500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 261.05,                last time consumption/overall running time: 335.3441s / 213262.8675 s
env0_first_0:                 episode reward: -87.5500,                 loss: 0.3190
env0_second_0:                 episode reward: 87.5500,                 loss: 0.3874
env1_first_0:                 episode reward: -79.4000,                 loss: nan
env1_second_0:                 episode reward: 79.4000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 301.15,                last time consumption/overall running time: 383.5179s / 213646.3854 s
env0_first_0:                 episode reward: -95.7000,                 loss: 0.3048
env0_second_0:                 episode reward: 95.7000,                 loss: 0.3902
env1_first_0:                 episode reward: -86.3500,                 loss: nan
env1_second_0:                 episode reward: 86.3500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 362.4,                last time consumption/overall running time: 463.9545s / 214110.3399 s
env0_first_0:                 episode reward: -80.6500,                 loss: 0.3254
env0_second_0:                 episode reward: 80.6500,                 loss: 0.4093
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 546.15,                last time consumption/overall running time: 699.7850s / 214810.1249 s
env0_first_0:                 episode reward: -85.0000,                 loss: 0.3262
env0_second_0:                 episode reward: 85.0000,                 loss: 0.4114
env1_first_0:                 episode reward: -87.2500,                 loss: nan
env1_second_0:                 episode reward: 87.2500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 529.65,                last time consumption/overall running time: 675.8653s / 215485.9902 s
env0_first_0:                 episode reward: -90.5000,                 loss: 0.3491
env0_second_0:                 episode reward: 90.5000,                 loss: 0.4441
env1_first_0:                 episode reward: -87.5500,                 loss: nan
env1_second_0:                 episode reward: 87.5500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 944.2,                last time consumption/overall running time: 1194.5976s / 216680.5878 s
env0_first_0:                 episode reward: -92.7000,                 loss: 0.3256
env0_second_0:                 episode reward: 92.7000,                 loss: 0.4108
env1_first_0:                 episode reward: -89.9500,                 loss: nan
env1_second_0:                 episode reward: 89.9500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1538.95,                last time consumption/overall running time: 1911.4250s / 218592.0127 s
env0_first_0:                 episode reward: -93.9500,                 loss: 0.2229
env0_second_0:                 episode reward: 93.9500,                 loss: 0.2848
env1_first_0:                 episode reward: -90.4000,                 loss: nan
env1_second_0:                 episode reward: 90.4000,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 1108.45,                last time consumption/overall running time: 1374.2205s / 219966.2332 s
env0_first_0:                 episode reward: -90.4500,                 loss: 0.1499
env0_second_0:                 episode reward: 90.4500,                 loss: 0.1897
env1_first_0:                 episode reward: -87.1500,                 loss: nan
env1_second_0:                 episode reward: 87.1500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 624.15,                last time consumption/overall running time: 775.3537s / 220741.5869 s
env0_first_0:                 episode reward: -85.2500,                 loss: 0.1996
env0_second_0:                 episode reward: 85.2500,                 loss: 0.2651
env1_first_0:                 episode reward: -90.1500,                 loss: nan
env1_second_0:                 episode reward: 90.1500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 512.75,                last time consumption/overall running time: 642.2524s / 221383.8393 s
env0_first_0:                 episode reward: -89.5000,                 loss: 0.2674
env0_second_0:                 episode reward: 89.5000,                 loss: 0.3007
env1_first_0:                 episode reward: -95.6000,                 loss: nan
env1_second_0:                 episode reward: 95.6000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 365.4,                last time consumption/overall running time: 455.0165s / 221838.8558 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.2832
env0_second_0:                 episode reward: 86.4500,                 loss: 0.3866
env1_first_0:                 episode reward: -98.5000,                 loss: nan
env1_second_0:                 episode reward: 98.5000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 370.4,                last time consumption/overall running time: 463.2336s / 222302.0895 s
env0_first_0:                 episode reward: -89.2500,                 loss: 0.3251
env0_second_0:                 episode reward: 89.2500,                 loss: 0.4162
env1_first_0:                 episode reward: -93.1500,                 loss: nan
env1_second_0:                 episode reward: 93.1500,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 241.45,                last time consumption/overall running time: 303.4341s / 222605.5236 s
env0_first_0:                 episode reward: -93.1500,                 loss: 0.3259
env0_second_0:                 episode reward: 93.1500,                 loss: 0.4023
env1_first_0:                 episode reward: -87.6000,                 loss: nan
env1_second_0:                 episode reward: 87.6000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 240.85,                last time consumption/overall running time: 301.3329s / 222906.8564 s
env0_first_0:                 episode reward: -86.8000,                 loss: 0.3694
env0_second_0:                 episode reward: 86.8000,                 loss: 0.4625
env1_first_0:                 episode reward: -79.8500,                 loss: nan
env1_second_0:                 episode reward: 79.8500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 383.4,                last time consumption/overall running time: 479.3184s / 223386.1749 s
env0_first_0:                 episode reward: -90.3000,                 loss: 0.3361
env0_second_0:                 episode reward: 90.3000,                 loss: 0.4673
env1_first_0:                 episode reward: -87.4000,                 loss: nan
env1_second_0:                 episode reward: 87.4000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 877.3,                last time consumption/overall running time: 1081.2627s / 224467.4375 s
env0_first_0:                 episode reward: -85.9500,                 loss: 0.3009
env0_second_0:                 episode reward: 85.9500,                 loss: 0.3832
env1_first_0:                 episode reward: -81.1000,                 loss: nan
env1_second_0:                 episode reward: 81.1000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 715.8,                last time consumption/overall running time: 886.0748s / 225353.5123 s
env0_first_0:                 episode reward: -88.8000,                 loss: 0.2860
env0_second_0:                 episode reward: 88.8000,                 loss: 0.3813
env1_first_0:                 episode reward: -89.9000,                 loss: nan
env1_second_0:                 episode reward: 89.9000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 327.35,                last time consumption/overall running time: 407.7287s / 225761.2410 s
env0_first_0:                 episode reward: -87.8500,                 loss: 0.2896
env0_second_0:                 episode reward: 87.8500,                 loss: 0.4420
env1_first_0:                 episode reward: -97.0500,                 loss: nan
env1_second_0:                 episode reward: 97.0500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 481.0,                last time consumption/overall running time: 598.5690s / 226359.8100 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.2761
env0_second_0:                 episode reward: 89.3000,                 loss: 0.4108
env1_first_0:                 episode reward: -89.2500,                 loss: nan
env1_second_0:                 episode reward: 89.2500,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 421.4,                last time consumption/overall running time: 527.1622s / 226886.9722 s
env0_first_0:                 episode reward: -92.0000,                 loss: 0.2937
env0_second_0:                 episode reward: 92.0000,                 loss: 0.3775
env1_first_0:                 episode reward: -89.7000,                 loss: nan
env1_second_0:                 episode reward: 89.7000,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 301.3,                last time consumption/overall running time: 376.9519s / 227263.9241 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.3318
env0_second_0:                 episode reward: 87.7000,                 loss: 0.4008
env1_first_0:                 episode reward: -83.3000,                 loss: nan
env1_second_0:                 episode reward: 83.3000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 267.4,                last time consumption/overall running time: 329.8706s / 227593.7947 s
env0_first_0:                 episode reward: -84.1000,                 loss: 0.3317
env0_second_0:                 episode reward: 84.1000,                 loss: 0.4793
env1_first_0:                 episode reward: -90.6500,                 loss: nan
env1_second_0:                 episode reward: 90.6500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 468.25,                last time consumption/overall running time: 575.2756s / 228169.0703 s
env0_first_0:                 episode reward: -93.1000,                 loss: 0.3422
env0_second_0:                 episode reward: 93.1000,                 loss: 0.4916
env1_first_0:                 episode reward: -71.7000,                 loss: nan
env1_second_0:                 episode reward: 71.7000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 526.7,                last time consumption/overall running time: 651.8948s / 228820.9651 s
env0_first_0:                 episode reward: -77.0500,                 loss: 0.2961
env0_second_0:                 episode reward: 77.0500,                 loss: 0.3812
env1_first_0:                 episode reward: -90.9000,                 loss: nan
env1_second_0:                 episode reward: 90.9000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 368.9,                last time consumption/overall running time: 448.1704s / 229269.1355 s
env0_first_0:                 episode reward: -86.6000,                 loss: 0.3178
env0_second_0:                 episode reward: 86.6000,                 loss: 0.3800
env1_first_0:                 episode reward: -96.6000,                 loss: nan
env1_second_0:                 episode reward: 96.6000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 242.95,                last time consumption/overall running time: 297.8647s / 229567.0002 s
env0_first_0:                 episode reward: -82.7000,                 loss: 0.3399
env0_second_0:                 episode reward: 82.7000,                 loss: 0.3854
env1_first_0:                 episode reward: -94.3500,                 loss: nan
env1_second_0:                 episode reward: 94.3500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 347.75,                last time consumption/overall running time: 423.2878s / 229990.2880 s
env0_first_0:                 episode reward: -97.6000,                 loss: 0.3421
env0_second_0:                 episode reward: 97.6000,                 loss: 0.4023
env1_first_0:                 episode reward: -85.8000,                 loss: nan
env1_second_0:                 episode reward: 85.8000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 367.4,                last time consumption/overall running time: 448.9709s / 230439.2589 s
env0_first_0:                 episode reward: -91.5500,                 loss: 0.3223
env0_second_0:                 episode reward: 91.5500,                 loss: 0.4054
env1_first_0:                 episode reward: -89.2500,                 loss: nan
env1_second_0:                 episode reward: 89.2500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 693.2,                last time consumption/overall running time: 849.2067s / 231288.4657 s
env0_first_0:                 episode reward: -94.9000,                 loss: 0.2632
env0_second_0:                 episode reward: 94.9000,                 loss: 0.3438
env1_first_0:                 episode reward: -83.3000,                 loss: nan
env1_second_0:                 episode reward: 83.3000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 487.45,                last time consumption/overall running time: 597.8044s / 231886.2701 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.2870
env0_second_0:                 episode reward: 86.4500,                 loss: 0.3434
env1_first_0:                 episode reward: -87.3000,                 loss: nan
env1_second_0:                 episode reward: 87.3000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 529.85,                last time consumption/overall running time: 647.2382s / 232533.5083 s
env0_first_0:                 episode reward: -88.8000,                 loss: 0.2819
env0_second_0:                 episode reward: 88.8000,                 loss: 0.3331
env1_first_0:                 episode reward: -88.0000,                 loss: nan
env1_second_0:                 episode reward: 88.0000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 411.25,                last time consumption/overall running time: 499.3521s / 233032.8604 s
env0_first_0:                 episode reward: -80.7500,                 loss: 0.3114
env0_second_0:                 episode reward: 80.7500,                 loss: 0.3699
env1_first_0:                 episode reward: -79.1500,                 loss: nan
env1_second_0:                 episode reward: 79.1500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 656.7,                last time consumption/overall running time: 801.4328s / 233834.2932 s
env0_first_0:                 episode reward: -80.2000,                 loss: 0.3413
env0_second_0:                 episode reward: 80.2000,                 loss: 0.3842
env1_first_0:                 episode reward: -86.6000,                 loss: nan
env1_second_0:                 episode reward: 86.6000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 666.9,                last time consumption/overall running time: 813.4807s / 234647.7739 s
env0_first_0:                 episode reward: -86.1500,                 loss: 0.3723
env0_second_0:                 episode reward: 86.1500,                 loss: 0.4175
env1_first_0:                 episode reward: -81.9000,                 loss: nan
env1_second_0:                 episode reward: 81.9000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 272.3,                last time consumption/overall running time: 332.9561s / 234980.7300 s
env0_first_0:                 episode reward: -94.4500,                 loss: 0.3662
env0_second_0:                 episode reward: 94.4500,                 loss: 0.4237
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 360.6,                last time consumption/overall running time: 442.2753s / 235423.0053 s
env0_first_0:                 episode reward: -95.8500,                 loss: 0.3763
env0_second_0:                 episode reward: 95.8500,                 loss: 0.4286
env1_first_0:                 episode reward: -90.6000,                 loss: nan
env1_second_0:                 episode reward: 90.6000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 412.8,                last time consumption/overall running time: 503.2683s / 235926.2735 s
env0_first_0:                 episode reward: -84.8500,                 loss: 0.3949
env0_second_0:                 episode reward: 84.8500,                 loss: 0.4286
env1_first_0:                 episode reward: -86.4000,                 loss: nan
env1_second_0:                 episode reward: 86.4000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 324.45,                last time consumption/overall running time: 399.5129s / 236325.7864 s
env0_first_0:                 episode reward: -85.6000,                 loss: 0.3515
env0_second_0:                 episode reward: 85.6000,                 loss: 0.3987
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 371.3,                last time consumption/overall running time: 455.3221s / 236781.1085 s
env0_first_0:                 episode reward: -78.9500,                 loss: 0.3827
env0_second_0:                 episode reward: 78.9500,                 loss: 0.4055
env1_first_0:                 episode reward: -85.4000,                 loss: nan
env1_second_0:                 episode reward: 85.4000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 296.7,                last time consumption/overall running time: 361.9714s / 237143.0799 s
env0_first_0:                 episode reward: -82.4500,                 loss: 0.4305
env0_second_0:                 episode reward: 82.4500,                 loss: 0.4870
env1_first_0:                 episode reward: -86.8000,                 loss: nan
env1_second_0:                 episode reward: 86.8000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 281.25,                last time consumption/overall running time: 342.8236s / 237485.9035 s
env0_first_0:                 episode reward: -80.5500,                 loss: 0.4172
env0_second_0:                 episode reward: 80.5500,                 loss: 0.4708
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 359.9,                last time consumption/overall running time: 440.2281s / 237926.1315 s
env0_first_0:                 episode reward: -75.0000,                 loss: 0.4249
env0_second_0:                 episode reward: 75.0000,                 loss: 0.5015
env1_first_0:                 episode reward: -96.3000,                 loss: nan
env1_second_0:                 episode reward: 96.3000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 407.6,                last time consumption/overall running time: 494.9117s / 238421.0432 s
env0_first_0:                 episode reward: -94.7000,                 loss: 0.4523
env0_second_0:                 episode reward: 94.7000,                 loss: 0.4957
env1_first_0:                 episode reward: -82.4500,                 loss: nan
env1_second_0:                 episode reward: 82.4500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 312.35,                last time consumption/overall running time: 382.2074s / 238803.2506 s
env0_first_0:                 episode reward: -77.9500,                 loss: 0.4653
env0_second_0:                 episode reward: 77.9500,                 loss: 0.5089
env1_first_0:                 episode reward: -81.4000,                 loss: nan
env1_second_0:                 episode reward: 81.4000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 284.2,                last time consumption/overall running time: 349.0425s / 239152.2932 s
env0_first_0:                 episode reward: -90.1000,                 loss: 0.4726
env0_second_0:                 episode reward: 90.1000,                 loss: 0.5412
env1_first_0:                 episode reward: -80.8500,                 loss: nan
env1_second_0:                 episode reward: 80.8500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 237.15,                last time consumption/overall running time: 293.8833s / 239446.1765 s
env0_first_0:                 episode reward: -89.9000,                 loss: 0.4541
env0_second_0:                 episode reward: 89.9000,                 loss: 0.5623
env1_first_0:                 episode reward: -83.1000,                 loss: nan
env1_second_0:                 episode reward: 83.1000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 290.5,                last time consumption/overall running time: 358.3038s / 239804.4803 s
env0_first_0:                 episode reward: -84.2500,                 loss: 0.4588
env0_second_0:                 episode reward: 84.2500,                 loss: 0.5173
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 253.3,                last time consumption/overall running time: 310.9245s / 240115.4048 s
env0_first_0:                 episode reward: -83.2500,                 loss: 0.4300
env0_second_0:                 episode reward: 83.2500,                 loss: 0.5071
env1_first_0:                 episode reward: -77.0000,                 loss: nan
env1_second_0:                 episode reward: 77.0000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 244.0,                last time consumption/overall running time: 296.2390s / 240411.6438 s
env0_first_0:                 episode reward: -93.2000,                 loss: 0.3977
env0_second_0:                 episode reward: 93.2000,                 loss: 0.4448
env1_first_0:                 episode reward: -81.4500,                 loss: nan
env1_second_0:                 episode reward: 81.4500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 261.1,                last time consumption/overall running time: 320.5967s / 240732.2405 s
env0_first_0:                 episode reward: -93.6000,                 loss: 0.3763
env0_second_0:                 episode reward: 93.6000,                 loss: 0.4609
env1_first_0:                 episode reward: -95.1000,                 loss: nan
env1_second_0:                 episode reward: 95.1000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 264.35,                last time consumption/overall running time: 325.0792s / 241057.3197 s
env0_first_0:                 episode reward: -92.8500,                 loss: 0.3571
env0_second_0:                 episode reward: 92.8500,                 loss: 0.4392
env1_first_0:                 episode reward: -89.4000,                 loss: nan
env1_second_0:                 episode reward: 89.4000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 289.15,                last time consumption/overall running time: 355.1743s / 241412.4940 s
env0_first_0:                 episode reward: -96.9500,                 loss: 0.3639
env0_second_0:                 episode reward: 96.9500,                 loss: 0.4558
env1_first_0:                 episode reward: -77.0500,                 loss: nan
env1_second_0:                 episode reward: 77.0500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 299.25,                last time consumption/overall running time: 367.5945s / 241780.0885 s
env0_first_0:                 episode reward: -87.4000,                 loss: 0.3722
env0_second_0:                 episode reward: 87.4000,                 loss: 0.4259
env1_first_0:                 episode reward: -88.9000,                 loss: nan
env1_second_0:                 episode reward: 88.9000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 296.95,                last time consumption/overall running time: 363.1359s / 242143.2244 s
env0_first_0:                 episode reward: -84.8000,                 loss: 0.3984
env0_second_0:                 episode reward: 84.8000,                 loss: 0.4298
env1_first_0:                 episode reward: -88.7000,                 loss: nan
env1_second_0:                 episode reward: 88.7000,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 304.75,                last time consumption/overall running time: 373.7887s / 242517.0131 s
env0_first_0:                 episode reward: -80.1000,                 loss: 0.3525
env0_second_0:                 episode reward: 80.1000,                 loss: 0.4212
env1_first_0:                 episode reward: -86.7000,                 loss: nan
env1_second_0:                 episode reward: 86.7000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 236.2,                last time consumption/overall running time: 289.2063s / 242806.2194 s
env0_first_0:                 episode reward: -96.7000,                 loss: 0.3577
env0_second_0:                 episode reward: 96.7000,                 loss: 0.4252
env1_first_0:                 episode reward: -81.8500,                 loss: nan
env1_second_0:                 episode reward: 81.8500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 235.55,                last time consumption/overall running time: 287.7541s / 243093.9735 s
env0_first_0:                 episode reward: -92.9500,                 loss: 0.3331
env0_second_0:                 episode reward: 92.9500,                 loss: 0.4103
env1_first_0:                 episode reward: -91.0000,                 loss: nan
env1_second_0:                 episode reward: 91.0000,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 277.25,                last time consumption/overall running time: 335.9397s / 243429.9132 s
env0_first_0:                 episode reward: -92.9000,                 loss: 0.3525
env0_second_0:                 episode reward: 92.9000,                 loss: 0.3962
env1_first_0:                 episode reward: -89.4500,                 loss: nan
env1_second_0:                 episode reward: 89.4500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 295.65,                last time consumption/overall running time: 370.2987s / 243800.2119 s
env0_first_0:                 episode reward: -87.1000,                 loss: 0.3184
env0_second_0:                 episode reward: 87.1000,                 loss: 0.3863
env1_first_0:                 episode reward: -92.7500,                 loss: nan
env1_second_0:                 episode reward: 92.7500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 280.85,                last time consumption/overall running time: 364.1414s / 244164.3532 s
env0_first_0:                 episode reward: -96.3000,                 loss: 0.3135
env0_second_0:                 episode reward: 96.3000,                 loss: 0.3828
env1_first_0:                 episode reward: -88.9000,                 loss: nan
env1_second_0:                 episode reward: 88.9000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 294.7,                last time consumption/overall running time: 380.7067s / 244545.0600 s
env0_first_0:                 episode reward: -94.4000,                 loss: 0.2838
env0_second_0:                 episode reward: 94.4000,                 loss: 0.3268
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 323.15,                last time consumption/overall running time: 415.4882s / 244960.5481 s
env0_first_0:                 episode reward: -96.9500,                 loss: 0.2821
env0_second_0:                 episode reward: 96.9500,                 loss: 0.3445
env1_first_0:                 episode reward: -91.8000,                 loss: nan
env1_second_0:                 episode reward: 91.8000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 267.2,                last time consumption/overall running time: 340.1488s / 245300.6970 s
env0_first_0:                 episode reward: -87.4000,                 loss: 0.3004
env0_second_0:                 episode reward: 87.4000,                 loss: 0.3334
env1_first_0:                 episode reward: -98.4500,                 loss: nan
env1_second_0:                 episode reward: 98.4500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 283.15,                last time consumption/overall running time: 361.4902s / 245662.1872 s
env0_first_0:                 episode reward: -90.4500,                 loss: 0.2981
env0_second_0:                 episode reward: 90.4500,                 loss: 0.3312
env1_first_0:                 episode reward: -92.4000,                 loss: nan
env1_second_0:                 episode reward: 92.4000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 279.7,                last time consumption/overall running time: 355.7070s / 246017.8942 s
env0_first_0:                 episode reward: -82.0000,                 loss: 0.3219
env0_second_0:                 episode reward: 82.0000,                 loss: 0.3898
env1_first_0:                 episode reward: -89.2500,                 loss: nan
env1_second_0:                 episode reward: 89.2500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 383.2,                last time consumption/overall running time: 491.1636s / 246509.0579 s
env0_first_0:                 episode reward: -85.3000,                 loss: 0.3459
env0_second_0:                 episode reward: 85.3000,                 loss: 0.3737
env1_first_0:                 episode reward: -83.6000,                 loss: nan
env1_second_0:                 episode reward: 83.6000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 262.65,                last time consumption/overall running time: 339.7880s / 246848.8459 s
env0_first_0:                 episode reward: -88.7500,                 loss: 0.3659
env0_second_0:                 episode reward: 88.7500,                 loss: 0.4014
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 256.1,                last time consumption/overall running time: 330.6241s / 247179.4700 s
env0_first_0:                 episode reward: -88.6500,                 loss: 0.3683
env0_second_0:                 episode reward: 88.6500,                 loss: 0.4582
env1_first_0:                 episode reward: -89.9500,                 loss: nan
env1_second_0:                 episode reward: 89.9500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 222.3,                last time consumption/overall running time: 283.5644s / 247463.0344 s
env0_first_0:                 episode reward: -92.1500,                 loss: 0.3954
env0_second_0:                 episode reward: 92.1500,                 loss: 0.4485
env1_first_0:                 episode reward: -78.1000,                 loss: nan
env1_second_0:                 episode reward: 78.1000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 380.3,                last time consumption/overall running time: 486.3946s / 247949.4290 s
env0_first_0:                 episode reward: -83.7000,                 loss: 0.4533
env0_second_0:                 episode reward: 83.7000,                 loss: 0.4913
env1_first_0:                 episode reward: -66.4000,                 loss: nan
env1_second_0:                 episode reward: 66.4000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 464.0,                last time consumption/overall running time: 592.8067s / 248542.2357 s
env0_first_0:                 episode reward: -73.3000,                 loss: 0.4434
env0_second_0:                 episode reward: 73.3000,                 loss: 0.4952
env1_first_0:                 episode reward: -87.9500,                 loss: nan
env1_second_0:                 episode reward: 87.9500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 397.4,                last time consumption/overall running time: 509.8657s / 249052.1014 s
env0_first_0:                 episode reward: -87.0500,                 loss: 0.4222
env0_second_0:                 episode reward: 87.0500,                 loss: 0.4372
env1_first_0:                 episode reward: -71.9000,                 loss: nan
env1_second_0:                 episode reward: 71.9000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 307.55,                last time consumption/overall running time: 400.0162s / 249452.1176 s
env0_first_0:                 episode reward: -89.1500,                 loss: 0.3925
env0_second_0:                 episode reward: 89.1500,                 loss: 0.4323
env1_first_0:                 episode reward: -88.4000,                 loss: nan
env1_second_0:                 episode reward: 88.4000,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 431.5,                last time consumption/overall running time: 552.6983s / 250004.8159 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.3725
env0_second_0:                 episode reward: 91.6500,                 loss: 0.3873
env1_first_0:                 episode reward: -90.5500,                 loss: nan
env1_second_0:                 episode reward: 90.5500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 371.75,                last time consumption/overall running time: 477.7475s / 250482.5634 s
env0_first_0:                 episode reward: -87.3500,                 loss: 0.3407
env0_second_0:                 episode reward: 87.3500,                 loss: 0.3713
env1_first_0:                 episode reward: -95.9500,                 loss: nan
env1_second_0:                 episode reward: 95.9500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 415.25,                last time consumption/overall running time: 530.8814s / 251013.4448 s
env0_first_0:                 episode reward: -84.1500,                 loss: 0.3506
env0_second_0:                 episode reward: 84.1500,                 loss: 0.3250
env1_first_0:                 episode reward: -89.8000,                 loss: nan
env1_second_0:                 episode reward: 89.8000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 444.95,                last time consumption/overall running time: 572.0700s / 251585.5148 s
env0_first_0:                 episode reward: -88.3000,                 loss: 0.3030
env0_second_0:                 episode reward: 88.3000,                 loss: 0.3014
env1_first_0:                 episode reward: -91.1000,                 loss: nan
env1_second_0:                 episode reward: 91.1000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 378.9,                last time consumption/overall running time: 482.0620s / 252067.5768 s
env0_first_0:                 episode reward: -96.5500,                 loss: 0.3023
env0_second_0:                 episode reward: 96.5500,                 loss: 0.2738
env1_first_0:                 episode reward: -91.0500,                 loss: nan
env1_second_0:                 episode reward: 91.0500,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 270.05,                last time consumption/overall running time: 343.3177s / 252410.8945 s
env0_first_0:                 episode reward: -81.3500,                 loss: 0.2779
env0_second_0:                 episode reward: 81.3500,                 loss: 0.2693
env1_first_0:                 episode reward: -89.3500,                 loss: nan
env1_second_0:                 episode reward: 89.3500,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 285.85,                last time consumption/overall running time: 360.1057s / 252771.0002 s
env0_first_0:                 episode reward: -85.6500,                 loss: 0.3123
env0_second_0:                 episode reward: 85.6500,                 loss: 0.2957
env1_first_0:                 episode reward: -92.5000,                 loss: nan
env1_second_0:                 episode reward: 92.5000,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 219.8,                last time consumption/overall running time: 278.3950s / 253049.3952 s
env0_first_0:                 episode reward: -84.9000,                 loss: 0.3359
env0_second_0:                 episode reward: 84.9000,                 loss: 0.2680
env1_first_0:                 episode reward: -82.3500,                 loss: nan
env1_second_0:                 episode reward: 82.3500,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 240.2,                last time consumption/overall running time: 305.5446s / 253354.9398 s
env0_first_0:                 episode reward: -94.3500,                 loss: 0.3596
env0_second_0:                 episode reward: 94.3500,                 loss: 0.3197
env1_first_0:                 episode reward: -82.5000,                 loss: nan
env1_second_0:                 episode reward: 82.5000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 243.4,                last time consumption/overall running time: 311.1081s / 253666.0479 s
env0_first_0:                 episode reward: -89.5000,                 loss: 0.3236
env0_second_0:                 episode reward: 89.5000,                 loss: 0.3279
env1_first_0:                 episode reward: -93.1000,                 loss: nan
env1_second_0:                 episode reward: 93.1000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 312.6,                last time consumption/overall running time: 399.5881s / 254065.6360 s
env0_first_0:                 episode reward: -87.4500,                 loss: 0.3270
env0_second_0:                 episode reward: 87.4500,                 loss: 0.3384
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 536.15,                last time consumption/overall running time: 678.5698s / 254744.2058 s
env0_first_0:                 episode reward: -91.2000,                 loss: 0.3710
env0_second_0:                 episode reward: 91.2000,                 loss: 0.3790
env1_first_0:                 episode reward: -75.0000,                 loss: nan
env1_second_0:                 episode reward: 75.0000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 311.3,                last time consumption/overall running time: 394.4040s / 255138.6099 s
env0_first_0:                 episode reward: -86.9000,                 loss: 0.4158
env0_second_0:                 episode reward: 86.9000,                 loss: 0.3712
env1_first_0:                 episode reward: -84.3000,                 loss: nan
env1_second_0:                 episode reward: 84.3000,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 389.9,                last time consumption/overall running time: 495.5400s / 255634.1499 s
env0_first_0:                 episode reward: -90.7000,                 loss: 0.4103
env0_second_0:                 episode reward: 90.7000,                 loss: 0.3768
env1_first_0:                 episode reward: -93.2000,                 loss: nan
env1_second_0:                 episode reward: 93.2000,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 331.7,                last time consumption/overall running time: 420.7450s / 256054.8950 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.3885
env0_second_0:                 episode reward: 90.4000,                 loss: 0.3576
env1_first_0:                 episode reward: -81.4500,                 loss: nan
env1_second_0:                 episode reward: 81.4500,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 434.8,                last time consumption/overall running time: 546.3907s / 256601.2857 s
env0_first_0:                 episode reward: -84.6500,                 loss: 0.3714
env0_second_0:                 episode reward: 84.6500,                 loss: 0.3441
env1_first_0:                 episode reward: -82.5500,                 loss: nan
env1_second_0:                 episode reward: 82.5500,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 249.2,                last time consumption/overall running time: 316.7014s / 256917.9871 s
env0_first_0:                 episode reward: -94.8000,                 loss: 0.3621
env0_second_0:                 episode reward: 94.8000,                 loss: 0.3716
env1_first_0:                 episode reward: -82.6000,                 loss: nan
env1_second_0:                 episode reward: 82.6000,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 249.2,                last time consumption/overall running time: 312.6163s / 257230.6033 s
env0_first_0:                 episode reward: -75.5500,                 loss: 0.3818
env0_second_0:                 episode reward: 75.5500,                 loss: 0.4154
env1_first_0:                 episode reward: -81.9000,                 loss: nan
env1_second_0:                 episode reward: 81.9000,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 233.45,                last time consumption/overall running time: 294.5094s / 257525.1128 s
env0_first_0:                 episode reward: -93.1500,                 loss: 0.3820
env0_second_0:                 episode reward: 93.1500,                 loss: 0.4113
env1_first_0:                 episode reward: -88.8000,                 loss: nan
env1_second_0:                 episode reward: 88.8000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 267.3,                last time consumption/overall running time: 337.4018s / 257862.5146 s
env0_first_0:                 episode reward: -88.7500,                 loss: 0.4106
env0_second_0:                 episode reward: 88.7500,                 loss: 0.3923
env1_first_0:                 episode reward: -74.5000,                 loss: nan
env1_second_0:                 episode reward: 74.5000,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 216.3,                last time consumption/overall running time: 273.4305s / 258135.9451 s
env0_first_0:                 episode reward: -95.1500,                 loss: 0.4003
env0_second_0:                 episode reward: 95.1500,                 loss: 0.3731
env1_first_0:                 episode reward: -92.7500,                 loss: nan
env1_second_0:                 episode reward: 92.7500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 231.8,                last time consumption/overall running time: 293.8390s / 258429.7841 s
env0_first_0:                 episode reward: -87.8000,                 loss: 0.4099
env0_second_0:                 episode reward: 87.8000,                 loss: 0.3815
env1_first_0:                 episode reward: -86.3000,                 loss: nan
env1_second_0:                 episode reward: 86.3000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 266.25,                last time consumption/overall running time: 336.6280s / 258766.4121 s
env0_first_0:                 episode reward: -90.5000,                 loss: 0.4622
env0_second_0:                 episode reward: 90.5000,                 loss: 0.3507
env1_first_0:                 episode reward: -76.1500,                 loss: nan
env1_second_0:                 episode reward: 76.1500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 280.1,                last time consumption/overall running time: 352.5190s / 259118.9311 s
env0_first_0:                 episode reward: -81.3500,                 loss: 0.4505
env0_second_0:                 episode reward: 81.3500,                 loss: 0.4038
env1_first_0:                 episode reward: -81.0500,                 loss: nan
env1_second_0:                 episode reward: 81.0500,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 297.05,                last time consumption/overall running time: 373.6627s / 259492.5938 s
env0_first_0:                 episode reward: -93.5000,                 loss: 0.4721
env0_second_0:                 episode reward: 93.5000,                 loss: 0.4425
env1_first_0:                 episode reward: -73.4500,                 loss: nan
env1_second_0:                 episode reward: 73.4500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 447.1,                last time consumption/overall running time: 556.1756s / 260048.7694 s
env0_first_0:                 episode reward: -85.8500,                 loss: 0.5195
env0_second_0:                 episode reward: 85.8500,                 loss: 0.5030
env1_first_0:                 episode reward: -72.6500,                 loss: nan
env1_second_0:                 episode reward: 72.6500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 281.85,                last time consumption/overall running time: 348.5834s / 260397.3528 s
env0_first_0:                 episode reward: -86.7000,                 loss: 0.5317
env0_second_0:                 episode reward: 86.7000,                 loss: 0.5482
env1_first_0:                 episode reward: -83.9500,                 loss: nan
env1_second_0:                 episode reward: 83.9500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 242.6,                last time consumption/overall running time: 304.2765s / 260701.6292 s
env0_first_0:                 episode reward: -77.7500,                 loss: 0.5596
env0_second_0:                 episode reward: 77.7500,                 loss: 0.5314
env1_first_0:                 episode reward: -82.8000,                 loss: nan
env1_second_0:                 episode reward: 82.8000,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 231.25,                last time consumption/overall running time: 283.9491s / 260985.5784 s
env0_first_0:                 episode reward: -94.5000,                 loss: 0.5553
env0_second_0:                 episode reward: 94.5000,                 loss: 0.5538
env1_first_0:                 episode reward: -84.8500,                 loss: nan
env1_second_0:                 episode reward: 84.8500,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 236.4,                last time consumption/overall running time: 285.3676s / 261270.9460 s
env0_first_0:                 episode reward: -94.8500,                 loss: 0.5988
env0_second_0:                 episode reward: 94.8500,                 loss: 0.5527
env1_first_0:                 episode reward: -85.1500,                 loss: nan
env1_second_0:                 episode reward: 85.1500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 258.15,                last time consumption/overall running time: 308.6590s / 261579.6051 s
env0_first_0:                 episode reward: -90.1000,                 loss: 0.5642
env0_second_0:                 episode reward: 90.1000,                 loss: 0.5148
env1_first_0:                 episode reward: -94.3500,                 loss: nan
env1_second_0:                 episode reward: 94.3500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 308.75,                last time consumption/overall running time: 369.4353s / 261949.0403 s
env0_first_0:                 episode reward: -94.1000,                 loss: 0.4496
env0_second_0:                 episode reward: 94.1000,                 loss: 0.5064
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 511.15,                last time consumption/overall running time: 603.5230s / 262552.5634 s
env0_first_0:                 episode reward: -94.1500,                 loss: 0.4777
env0_second_0:                 episode reward: 94.1500,                 loss: 0.4550
env1_first_0:                 episode reward: -94.7000,                 loss: nan
env1_second_0:                 episode reward: 94.7000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 525.45,                last time consumption/overall running time: 621.1391s / 263173.7025 s
env0_first_0:                 episode reward: -93.8500,                 loss: 0.3541
env0_second_0:                 episode reward: 93.8500,                 loss: 0.3845
env1_first_0:                 episode reward: -90.0000,                 loss: nan
env1_second_0:                 episode reward: 90.0000,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 354.2,                last time consumption/overall running time: 432.9927s / 263606.6952 s
env0_first_0:                 episode reward: -98.1000,                 loss: 0.3156
env0_second_0:                 episode reward: 98.1000,                 loss: 0.3202
env1_first_0:                 episode reward: -86.4500,                 loss: nan
env1_second_0:                 episode reward: 86.4500,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 580.2,                last time consumption/overall running time: 693.8728s / 264300.5679 s
env0_first_0:                 episode reward: -91.8500,                 loss: 0.2568
env0_second_0:                 episode reward: 91.8500,                 loss: 0.2777
env1_first_0:                 episode reward: -86.5000,                 loss: nan
env1_second_0:                 episode reward: 86.5000,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 423.85,                last time consumption/overall running time: 513.0806s / 264813.6486 s
env0_first_0:                 episode reward: -94.3000,                 loss: 0.2591
env0_second_0:                 episode reward: 94.3000,                 loss: 0.2786
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 299.95,                last time consumption/overall running time: 360.5720s / 265174.2206 s
env0_first_0:                 episode reward: -97.5000,                 loss: 0.2684
env0_second_0:                 episode reward: 97.5000,                 loss: 0.2817
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 250.7,                last time consumption/overall running time: 310.6871s / 265484.9076 s
env0_first_0:                 episode reward: -79.6000,                 loss: 0.2693
env0_second_0:                 episode reward: 79.6000,                 loss: 0.3151
env1_first_0:                 episode reward: -89.9500,                 loss: nan
env1_second_0:                 episode reward: 89.9500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 243.3,                last time consumption/overall running time: 293.0057s / 265777.9133 s
env0_first_0:                 episode reward: -91.8000,                 loss: 0.3416
env0_second_0:                 episode reward: 91.8000,                 loss: 0.3231
env1_first_0:                 episode reward: -80.8000,                 loss: nan
env1_second_0:                 episode reward: 80.8000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 223.45,                last time consumption/overall running time: 269.7505s / 266047.6638 s
env0_first_0:                 episode reward: -80.7500,                 loss: 0.2884
env0_second_0:                 episode reward: 80.7500,                 loss: 0.3396
env1_first_0:                 episode reward: -87.7500,                 loss: nan
env1_second_0:                 episode reward: 87.7500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 227.4,                last time consumption/overall running time: 277.4577s / 266325.1215 s
env0_first_0:                 episode reward: -92.0000,                 loss: 0.3226
env0_second_0:                 episode reward: 92.0000,                 loss: 0.3557
env1_first_0:                 episode reward: -91.7500,                 loss: nan
env1_second_0:                 episode reward: 91.7500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 294.5,                last time consumption/overall running time: 356.6119s / 266681.7334 s
env0_first_0:                 episode reward: -93.8500,                 loss: 0.3375
env0_second_0:                 episode reward: 93.8500,                 loss: 0.3578
env1_first_0:                 episode reward: -96.1500,                 loss: nan
env1_second_0:                 episode reward: 96.1500,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 503.0,                last time consumption/overall running time: 608.1229s / 267289.8563 s
env0_first_0:                 episode reward: -96.8000,                 loss: 0.3141
env0_second_0:                 episode reward: 96.8000,                 loss: 0.3835
env1_first_0:                 episode reward: -93.1000,                 loss: nan
env1_second_0:                 episode reward: 93.1000,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 364.2,                last time consumption/overall running time: 431.6919s / 267721.5482 s
env0_first_0:                 episode reward: -94.4500,                 loss: 0.3045
env0_second_0:                 episode reward: 94.4500,                 loss: 0.3379
env1_first_0:                 episode reward: -94.0000,                 loss: nan
env1_second_0:                 episode reward: 94.0000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 250.7,                last time consumption/overall running time: 300.7621s / 268022.3103 s
env0_first_0:                 episode reward: -94.8500,                 loss: 0.2932
env0_second_0:                 episode reward: 94.8500,                 loss: 0.3310
env1_first_0:                 episode reward: -88.0000,                 loss: nan
env1_second_0:                 episode reward: 88.0000,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 246.5,                last time consumption/overall running time: 292.6330s / 268314.9434 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.2923
env0_second_0:                 episode reward: 89.3000,                 loss: 0.3156
env1_first_0:                 episode reward: -86.7000,                 loss: nan
env1_second_0:                 episode reward: 86.7000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 239.65,                last time consumption/overall running time: 288.8397s / 268603.7830 s
env0_first_0:                 episode reward: -98.0000,                 loss: 0.2678
env0_second_0:                 episode reward: 98.0000,                 loss: 0.2736
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 399.6,                last time consumption/overall running time: 477.1993s / 269080.9823 s
env0_first_0:                 episode reward: -92.5500,                 loss: 0.2679
env0_second_0:                 episode reward: 92.5500,                 loss: 0.2930
env1_first_0:                 episode reward: -91.3500,                 loss: nan
env1_second_0:                 episode reward: 91.3500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 376.65,                last time consumption/overall running time: 454.2730s / 269535.2553 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.2874
env0_second_0:                 episode reward: 92.4000,                 loss: 0.2871
env1_first_0:                 episode reward: -82.8500,                 loss: nan
env1_second_0:                 episode reward: 82.8500,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 440.45,                last time consumption/overall running time: 530.8005s / 270066.0558 s
env0_first_0:                 episode reward: -85.6000,                 loss: 0.2971
env0_second_0:                 episode reward: 85.6000,                 loss: 0.3311
env1_first_0:                 episode reward: -86.0500,                 loss: nan
env1_second_0:                 episode reward: 86.0500,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 506.05,                last time consumption/overall running time: 610.2887s / 270676.3445 s
env0_first_0:                 episode reward: -89.0500,                 loss: 0.3250
env0_second_0:                 episode reward: 89.0500,                 loss: 0.3361
env1_first_0:                 episode reward: -95.4000,                 loss: nan
env1_second_0:                 episode reward: 95.4000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 255.65,                last time consumption/overall running time: 307.5584s / 270983.9029 s
env0_first_0:                 episode reward: -87.5500,                 loss: 0.2900
env0_second_0:                 episode reward: 87.5500,                 loss: 0.3210
env1_first_0:                 episode reward: -88.7500,                 loss: nan
env1_second_0:                 episode reward: 88.7500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 360.95,                last time consumption/overall running time: 437.4875s / 271421.3904 s
env0_first_0:                 episode reward: -87.7500,                 loss: 0.3125
env0_second_0:                 episode reward: 87.7500,                 loss: 0.3384
env1_first_0:                 episode reward: -93.0500,                 loss: nan
env1_second_0:                 episode reward: 93.0500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 317.3,                last time consumption/overall running time: 379.9636s / 271801.3540 s
env0_first_0:                 episode reward: -98.0000,                 loss: 0.2947
env0_second_0:                 episode reward: 98.0000,                 loss: 0.3313
env1_first_0:                 episode reward: -82.0000,                 loss: nan
env1_second_0:                 episode reward: 82.0000,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 307.5,                last time consumption/overall running time: 369.9961s / 272171.3502 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.3181
env0_second_0:                 episode reward: 87.7000,                 loss: 0.3696
env1_first_0:                 episode reward: -93.7500,                 loss: nan
env1_second_0:                 episode reward: 93.7500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 534.45,                last time consumption/overall running time: 638.0392s / 272809.3893 s
env0_first_0:                 episode reward: -91.6000,                 loss: 0.3063
env0_second_0:                 episode reward: 91.6000,                 loss: 0.3009
env1_first_0:                 episode reward: -89.1500,                 loss: nan
env1_second_0:                 episode reward: 89.1500,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 236.0,                last time consumption/overall running time: 281.5902s / 273090.9795 s
env0_first_0:                 episode reward: -86.1000,                 loss: 0.2763
env0_second_0:                 episode reward: 86.1000,                 loss: 0.2869
env1_first_0:                 episode reward: -95.3000,                 loss: nan
env1_second_0:                 episode reward: 95.3000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 226.25,                last time consumption/overall running time: 273.8955s / 273364.8750 s
env0_first_0:                 episode reward: -86.2000,                 loss: 0.3300
env0_second_0:                 episode reward: 86.2000,                 loss: 0.3060
env1_first_0:                 episode reward: -95.0500,                 loss: nan
env1_second_0:                 episode reward: 95.0500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 531.7,                last time consumption/overall running time: 641.1267s / 274006.0018 s
env0_first_0:                 episode reward: -95.2500,                 loss: 0.2810
env0_second_0:                 episode reward: 95.2500,                 loss: 0.3134
env1_first_0:                 episode reward: -82.8000,                 loss: nan
env1_second_0:                 episode reward: 82.8000,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 519.6,                last time consumption/overall running time: 624.5917s / 274630.5935 s
env0_first_0:                 episode reward: -75.9500,                 loss: 0.2917
env0_second_0:                 episode reward: 75.9500,                 loss: 0.3497
env1_first_0:                 episode reward: -82.2000,                 loss: nan
env1_second_0:                 episode reward: 82.2000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 445.3,                last time consumption/overall running time: 533.4914s / 275164.0849 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.3162
env0_second_0:                 episode reward: 89.3000,                 loss: 0.3447
env1_first_0:                 episode reward: -93.3000,                 loss: nan
env1_second_0:                 episode reward: 93.3000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 337.65,                last time consumption/overall running time: 411.5555s / 275575.6404 s
env0_first_0:                 episode reward: -91.3000,                 loss: 0.3284
env0_second_0:                 episode reward: 91.3000,                 loss: 0.3453
env1_first_0:                 episode reward: -87.5500,                 loss: nan
env1_second_0:                 episode reward: 87.5500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 363.4,                last time consumption/overall running time: 439.3650s / 276015.0055 s
env0_first_0:                 episode reward: -95.3000,                 loss: 0.3036
env0_second_0:                 episode reward: 95.3000,                 loss: 0.3609
env1_first_0:                 episode reward: -94.0500,                 loss: nan
env1_second_0:                 episode reward: 94.0500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 498.8,                last time consumption/overall running time: 607.4763s / 276622.4817 s
env0_first_0:                 episode reward: -96.9500,                 loss: 0.3351
env0_second_0:                 episode reward: 96.9500,                 loss: 0.3855
env1_first_0:                 episode reward: -83.9500,                 loss: nan
env1_second_0:                 episode reward: 83.9500,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 355.15,                last time consumption/overall running time: 427.6680s / 277050.1497 s
env0_first_0:                 episode reward: -84.0500,                 loss: 0.3334
env0_second_0:                 episode reward: 84.0500,                 loss: 0.3508
env1_first_0:                 episode reward: -89.3000,                 loss: nan
env1_second_0:                 episode reward: 89.3000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 362.1,                last time consumption/overall running time: 434.8064s / 277484.9561 s
env0_first_0:                 episode reward: -93.1000,                 loss: 0.3357
env0_second_0:                 episode reward: 93.1000,                 loss: 0.3749
env1_first_0:                 episode reward: -94.4000,                 loss: nan
env1_second_0:                 episode reward: 94.4000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 257.65,                last time consumption/overall running time: 311.4010s / 277796.3571 s
env0_first_0:                 episode reward: -97.7000,                 loss: 0.3286
env0_second_0:                 episode reward: 97.7000,                 loss: 0.3533
env1_first_0:                 episode reward: -93.6000,                 loss: nan
env1_second_0:                 episode reward: 93.6000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 299.45,                last time consumption/overall running time: 361.4197s / 278157.7768 s
env0_first_0:                 episode reward: -80.3000,                 loss: 0.2832
env0_second_0:                 episode reward: 80.3000,                 loss: 0.3005
env1_first_0:                 episode reward: -94.3500,                 loss: nan
env1_second_0:                 episode reward: 94.3500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 508.05,                last time consumption/overall running time: 609.0635s / 278766.8403 s
env0_first_0:                 episode reward: -81.7500,                 loss: 0.2888
env0_second_0:                 episode reward: 81.7500,                 loss: 0.3357
env1_first_0:                 episode reward: -90.5500,                 loss: nan
env1_second_0:                 episode reward: 90.5500,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 253.65,                last time consumption/overall running time: 307.4459s / 279074.2863 s
env0_first_0:                 episode reward: -91.0000,                 loss: 0.2838
env0_second_0:                 episode reward: 91.0000,                 loss: 0.3708
env1_first_0:                 episode reward: -75.9000,                 loss: nan
env1_second_0:                 episode reward: 75.9000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 411.25,                last time consumption/overall running time: 489.5456s / 279563.8319 s
env0_first_0:                 episode reward: -89.7500,                 loss: 0.2896
env0_second_0:                 episode reward: 89.7500,                 loss: 0.3496
env1_first_0:                 episode reward: -84.3000,                 loss: nan
env1_second_0:                 episode reward: 84.3000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 376.2,                last time consumption/overall running time: 447.8419s / 280011.6738 s
env0_first_0:                 episode reward: -85.4500,                 loss: 0.2911
env0_second_0:                 episode reward: 85.4500,                 loss: 0.3393
env1_first_0:                 episode reward: -87.9500,                 loss: nan
env1_second_0:                 episode reward: 87.9500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 938.95,                last time consumption/overall running time: 1114.4417s / 281126.1155 s
env0_first_0:                 episode reward: -91.4000,                 loss: 0.2609
env0_second_0:                 episode reward: 91.4000,                 loss: 0.3286
env1_first_0:                 episode reward: -92.4000,                 loss: nan
env1_second_0:                 episode reward: 92.4000,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 294.05,                last time consumption/overall running time: 353.5130s / 281479.6285 s
env0_first_0:                 episode reward: -93.7000,                 loss: 0.2220
env0_second_0:                 episode reward: 93.7000,                 loss: 0.2803
env1_first_0:                 episode reward: -90.3000,                 loss: nan
env1_second_0:                 episode reward: 90.3000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 297.75,                last time consumption/overall running time: 357.2891s / 281836.9176 s
env0_first_0:                 episode reward: -91.2500,                 loss: 0.2156
env0_second_0:                 episode reward: 91.2500,                 loss: 0.2456
env1_first_0:                 episode reward: -90.8500,                 loss: nan
env1_second_0:                 episode reward: 90.8500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 294.65,                last time consumption/overall running time: 352.7173s / 282189.6349 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.1859
env0_second_0:                 episode reward: 88.5500,                 loss: 0.2584
env1_first_0:                 episode reward: -93.6000,                 loss: nan
env1_second_0:                 episode reward: 93.6000,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 306.0,                last time consumption/overall running time: 362.9746s / 282552.6095 s
env0_first_0:                 episode reward: -94.0500,                 loss: 0.2069
env0_second_0:                 episode reward: 94.0500,                 loss: 0.2747
env1_first_0:                 episode reward: -80.7500,                 loss: nan
env1_second_0:                 episode reward: 80.7500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 255.5,                last time consumption/overall running time: 307.9049s / 282860.5144 s
env0_first_0:                 episode reward: -90.1500,                 loss: 0.2034
env0_second_0:                 episode reward: 90.1500,                 loss: 0.2948
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 381.95,                last time consumption/overall running time: 453.4956s / 283314.0100 s
env0_first_0:                 episode reward: -82.5000,                 loss: 0.2222
env0_second_0:                 episode reward: 82.5000,                 loss: 0.2819
env1_first_0:                 episode reward: -90.4500,                 loss: nan
env1_second_0:                 episode reward: 90.4500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 434.0,                last time consumption/overall running time: 510.7604s / 283824.7704 s
env0_first_0:                 episode reward: -81.7000,                 loss: 0.2645
env0_second_0:                 episode reward: 81.7000,                 loss: 0.3088
env1_first_0:                 episode reward: -92.2500,                 loss: nan
env1_second_0:                 episode reward: 92.2500,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 629.1,                last time consumption/overall running time: 748.9716s / 284573.7420 s
env0_first_0:                 episode reward: -86.7500,                 loss: 0.3014
env0_second_0:                 episode reward: 86.7500,                 loss: 0.3820
env1_first_0:                 episode reward: -90.0000,                 loss: nan
env1_second_0:                 episode reward: 90.0000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 240.6,                last time consumption/overall running time: 289.4614s / 284863.2034 s
env0_first_0:                 episode reward: -86.2000,                 loss: 0.3104
env0_second_0:                 episode reward: 86.2000,                 loss: 0.3755
env1_first_0:                 episode reward: -85.1500,                 loss: nan
env1_second_0:                 episode reward: 85.1500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 239.35,                last time consumption/overall running time: 283.6740s / 285146.8774 s
env0_first_0:                 episode reward: -88.1000,                 loss: 0.2898
env0_second_0:                 episode reward: 88.1000,                 loss: 0.3661
env1_first_0:                 episode reward: -96.0500,                 loss: nan
env1_second_0:                 episode reward: 96.0500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 403.1,                last time consumption/overall running time: 478.5181s / 285625.3955 s
env0_first_0:                 episode reward: -80.2000,                 loss: 0.2974
env0_second_0:                 episode reward: 80.2000,                 loss: 0.3650
env1_first_0:                 episode reward: -85.2500,                 loss: nan
env1_second_0:                 episode reward: 85.2500,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 267.35,                last time consumption/overall running time: 319.5126s / 285944.9081 s
env0_first_0:                 episode reward: -84.3000,                 loss: 0.3219
env0_second_0:                 episode reward: 84.3000,                 loss: 0.3591
env1_first_0:                 episode reward: -89.3500,                 loss: nan
env1_second_0:                 episode reward: 89.3500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 327.95,                last time consumption/overall running time: 389.4351s / 286334.3432 s
env0_first_0:                 episode reward: -91.3000,                 loss: 0.3078
env0_second_0:                 episode reward: 91.3000,                 loss: 0.3615
env1_first_0:                 episode reward: -88.5500,                 loss: nan
env1_second_0:                 episode reward: 88.5500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 275.2,                last time consumption/overall running time: 327.1489s / 286661.4920 s
env0_first_0:                 episode reward: -95.3000,                 loss: 0.2854
env0_second_0:                 episode reward: 95.3000,                 loss: 0.3619
env1_first_0:                 episode reward: -91.1000,                 loss: nan
env1_second_0:                 episode reward: 91.1000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 369.25,                last time consumption/overall running time: 441.1987s / 287102.6907 s
env0_first_0:                 episode reward: -96.3500,                 loss: 0.2976
env0_second_0:                 episode reward: 96.3500,                 loss: 0.3279
env1_first_0:                 episode reward: -92.8000,                 loss: nan
env1_second_0:                 episode reward: 92.8000,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 303.5,                last time consumption/overall running time: 360.5267s / 287463.2174 s
env0_first_0:                 episode reward: -80.7000,                 loss: 0.3286
env0_second_0:                 episode reward: 80.7000,                 loss: 0.3598
env1_first_0:                 episode reward: -80.8000,                 loss: nan
env1_second_0:                 episode reward: 80.8000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 346.55,                last time consumption/overall running time: 411.2595s / 287874.4770 s
env0_first_0:                 episode reward: -83.2500,                 loss: 0.3892
env0_second_0:                 episode reward: 83.2500,                 loss: 0.4133
env1_first_0:                 episode reward: -83.7000,                 loss: nan
env1_second_0:                 episode reward: 83.7000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 323.85,                last time consumption/overall running time: 381.8652s / 288256.3422 s
env0_first_0:                 episode reward: -84.8500,                 loss: 0.4030
env0_second_0:                 episode reward: 84.8500,                 loss: 0.4186
env1_first_0:                 episode reward: -96.6000,                 loss: nan
env1_second_0:                 episode reward: 96.6000,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 288.7,                last time consumption/overall running time: 339.6817s / 288596.0239 s
env0_first_0:                 episode reward: -88.2500,                 loss: 0.3734
env0_second_0:                 episode reward: 88.2500,                 loss: 0.3497
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 295.2,                last time consumption/overall running time: 350.1900s / 288946.2139 s
env0_first_0:                 episode reward: -94.0000,                 loss: 0.3678
env0_second_0:                 episode reward: 94.0000,                 loss: 0.3878
env1_first_0:                 episode reward: -94.1000,                 loss: nan
env1_second_0:                 episode reward: 94.1000,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 237.5,                last time consumption/overall running time: 280.1962s / 289226.4101 s
env0_first_0:                 episode reward: -93.6000,                 loss: 0.3392
env0_second_0:                 episode reward: 93.6000,                 loss: 0.3671
env1_first_0:                 episode reward: -89.4500,                 loss: nan
env1_second_0:                 episode reward: 89.4500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 251.25,                last time consumption/overall running time: 295.8722s / 289522.2822 s
env0_first_0:                 episode reward: -91.5000,                 loss: 0.2904
env0_second_0:                 episode reward: 91.5000,                 loss: 0.4031
env1_first_0:                 episode reward: -85.9000,                 loss: nan
env1_second_0:                 episode reward: 85.9000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 460.55,                last time consumption/overall running time: 549.0765s / 290071.3587 s
env0_first_0:                 episode reward: -92.9000,                 loss: 0.3337
env0_second_0:                 episode reward: 92.9000,                 loss: 0.3821
env1_first_0:                 episode reward: -90.7500,                 loss: nan
env1_second_0:                 episode reward: 90.7500,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 296.3,                last time consumption/overall running time: 354.0297s / 290425.3884 s
env0_first_0:                 episode reward: -94.8000,                 loss: 0.3036
env0_second_0:                 episode reward: 94.8000,                 loss: 0.3672
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 253.2,                last time consumption/overall running time: 295.7835s / 290721.1719 s
env0_first_0:                 episode reward: -91.2000,                 loss: 0.2707
env0_second_0:                 episode reward: 91.2000,                 loss: 0.3257
env1_first_0:                 episode reward: -94.6000,                 loss: nan
env1_second_0:                 episode reward: 94.6000,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 233.95,                last time consumption/overall running time: 273.1841s / 290994.3560 s
env0_first_0:                 episode reward: -91.4500,                 loss: 0.2890
env0_second_0:                 episode reward: 91.4500,                 loss: 0.2939
env1_first_0:                 episode reward: -88.7000,                 loss: nan
env1_second_0:                 episode reward: 88.7000,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 255.25,                last time consumption/overall running time: 298.1736s / 291292.5296 s
env0_first_0:                 episode reward: -85.8500,                 loss: 0.2755
env0_second_0:                 episode reward: 85.8500,                 loss: 0.3176
env1_first_0:                 episode reward: -93.1500,                 loss: nan
env1_second_0:                 episode reward: 93.1500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 253.0,                last time consumption/overall running time: 298.6225s / 291591.1520 s
env0_first_0:                 episode reward: -93.3000,                 loss: 0.3038
env0_second_0:                 episode reward: 93.3000,                 loss: 0.3233
env1_first_0:                 episode reward: -85.2000,                 loss: nan
env1_second_0:                 episode reward: 85.2000,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 359.25,                last time consumption/overall running time: 422.8265s / 292013.9786 s
env0_first_0:                 episode reward: -96.1500,                 loss: 0.2794
env0_second_0:                 episode reward: 96.1500,                 loss: 0.3538
env1_first_0:                 episode reward: -76.7000,                 loss: nan
env1_second_0:                 episode reward: 76.7000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 400.55,                last time consumption/overall running time: 464.9825s / 292478.9611 s
env0_first_0:                 episode reward: -90.9000,                 loss: 0.2812
env0_second_0:                 episode reward: 90.9000,                 loss: 0.3577
env1_first_0:                 episode reward: -92.8000,                 loss: nan
env1_second_0:                 episode reward: 92.8000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 410.55,                last time consumption/overall running time: 479.4993s / 292958.4604 s
env0_first_0:                 episode reward: -93.4000,                 loss: 0.3273
env0_second_0:                 episode reward: 93.4000,                 loss: 0.3607
env1_first_0:                 episode reward: -80.9000,                 loss: nan
env1_second_0:                 episode reward: 80.9000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 221.0,                last time consumption/overall running time: 253.9863s / 293212.4467 s
env0_first_0:                 episode reward: -94.0000,                 loss: 0.3318
env0_second_0:                 episode reward: 94.0000,                 loss: 0.3521
env1_first_0:                 episode reward: -94.9500,                 loss: nan
env1_second_0:                 episode reward: 94.9500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 415.35,                last time consumption/overall running time: 474.5936s / 293687.0403 s
env0_first_0:                 episode reward: -87.2500,                 loss: 0.3292
env0_second_0:                 episode reward: 87.2500,                 loss: 0.3501
env1_first_0:                 episode reward: -92.4500,                 loss: nan
env1_second_0:                 episode reward: 92.4500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 325.15,                last time consumption/overall running time: 374.5461s / 294061.5864 s
env0_first_0:                 episode reward: -93.0500,                 loss: 0.2725
env0_second_0:                 episode reward: 93.0500,                 loss: 0.3613
env1_first_0:                 episode reward: -90.9000,                 loss: nan
env1_second_0:                 episode reward: 90.9000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 280.35,                last time consumption/overall running time: 320.4105s / 294381.9968 s
env0_first_0:                 episode reward: -83.0500,                 loss: 0.2872
env0_second_0:                 episode reward: 83.0500,                 loss: 0.3470
env1_first_0:                 episode reward: -94.5500,                 loss: nan
env1_second_0:                 episode reward: 94.5500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 279.75,                last time consumption/overall running time: 319.8943s / 294701.8911 s
env0_first_0:                 episode reward: -85.3000,                 loss: 0.2868
env0_second_0:                 episode reward: 85.3000,                 loss: 0.3459
env1_first_0:                 episode reward: -87.4000,                 loss: nan
env1_second_0:                 episode reward: 87.4000,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 338.4,                last time consumption/overall running time: 394.3440s / 295096.2351 s
env0_first_0:                 episode reward: -87.8500,                 loss: 0.2925
env0_second_0:                 episode reward: 87.8500,                 loss: 0.3410
env1_first_0:                 episode reward: -88.0500,                 loss: nan
env1_second_0:                 episode reward: 88.0500,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 381.2,                last time consumption/overall running time: 445.7959s / 295542.0310 s
env0_first_0:                 episode reward: -83.0000,                 loss: 0.3383
env0_second_0:                 episode reward: 83.0000,                 loss: 0.3772
env1_first_0:                 episode reward: -91.6500,                 loss: nan
env1_second_0:                 episode reward: 91.6500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 384.6,                last time consumption/overall running time: 446.7966s / 295988.8276 s
env0_first_0:                 episode reward: -94.7500,                 loss: 0.3146
env0_second_0:                 episode reward: 94.7500,                 loss: 0.3634
env1_first_0:                 episode reward: -93.1500,                 loss: nan
env1_second_0:                 episode reward: 93.1500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 330.0,                last time consumption/overall running time: 378.4710s / 296367.2986 s
env0_first_0:                 episode reward: -87.5000,                 loss: 0.3603
env0_second_0:                 episode reward: 87.5000,                 loss: 0.3545
env1_first_0:                 episode reward: -88.9000,                 loss: nan
env1_second_0:                 episode reward: 88.9000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 335.25,                last time consumption/overall running time: 382.8531s / 296750.1517 s
env0_first_0:                 episode reward: -88.7500,                 loss: 0.3099
env0_second_0:                 episode reward: 88.7500,                 loss: 0.3663
env1_first_0:                 episode reward: -96.1000,                 loss: nan
env1_second_0:                 episode reward: 96.1000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 337.9,                last time consumption/overall running time: 392.8050s / 297142.9567 s
env0_first_0:                 episode reward: -84.1500,                 loss: 0.3200
env0_second_0:                 episode reward: 84.1500,                 loss: 0.3916
env1_first_0:                 episode reward: -97.6000,                 loss: nan
env1_second_0:                 episode reward: 97.6000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 272.35,                last time consumption/overall running time: 314.0272s / 297456.9840 s
env0_first_0:                 episode reward: -95.8000,                 loss: 0.3186
env0_second_0:                 episode reward: 95.8000,                 loss: 0.3728
env1_first_0:                 episode reward: -88.3500,                 loss: nan
env1_second_0:                 episode reward: 88.3500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 301.65,                last time consumption/overall running time: 347.8634s / 297804.8474 s
env0_first_0:                 episode reward: -97.1500,                 loss: 0.2975
env0_second_0:                 episode reward: 97.1500,                 loss: 0.3461
env1_first_0:                 episode reward: -91.3500,                 loss: nan
env1_second_0:                 episode reward: 91.3500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 289.65,                last time consumption/overall running time: 334.5141s / 298139.3615 s
env0_first_0:                 episode reward: -75.0000,                 loss: 0.2881
env0_second_0:                 episode reward: 75.0000,                 loss: 0.3471
env1_first_0:                 episode reward: -85.1000,                 loss: nan
env1_second_0:                 episode reward: 85.1000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 260.9,                last time consumption/overall running time: 299.7907s / 298439.1522 s
env0_first_0:                 episode reward: -85.3000,                 loss: 0.2749
env0_second_0:                 episode reward: 85.3000,                 loss: 0.3486
env1_first_0:                 episode reward: -86.9500,                 loss: nan
env1_second_0:                 episode reward: 86.9500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 338.65,                last time consumption/overall running time: 386.1038s / 298825.2560 s
env0_first_0:                 episode reward: -83.6500,                 loss: 0.3048
env0_second_0:                 episode reward: 83.6500,                 loss: 0.3568
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 512.8,                last time consumption/overall running time: 580.2759s / 299405.5319 s
env0_first_0:                 episode reward: -87.3500,                 loss: 0.3205
env0_second_0:                 episode reward: 87.3500,                 loss: 0.4069
env1_first_0:                 episode reward: -83.2500,                 loss: nan
env1_second_0:                 episode reward: 83.2500,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 373.6,                last time consumption/overall running time: 429.1449s / 299834.6768 s
env0_first_0:                 episode reward: -81.4500,                 loss: 0.3547
env0_second_0:                 episode reward: 81.4500,                 loss: 0.4218
env1_first_0:                 episode reward: -81.8500,                 loss: nan
env1_second_0:                 episode reward: 81.8500,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 259.7,                last time consumption/overall running time: 293.4998s / 300128.1766 s
env0_first_0:                 episode reward: -96.9500,                 loss: 0.3571
env0_second_0:                 episode reward: 96.9500,                 loss: 0.4274
env1_first_0:                 episode reward: -80.6500,                 loss: nan
env1_second_0:                 episode reward: 80.6500,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 257.9,                last time consumption/overall running time: 293.0724s / 300421.2490 s
env0_first_0:                 episode reward: -84.4000,                 loss: 0.3359
env0_second_0:                 episode reward: 84.4000,                 loss: 0.4532
env1_first_0:                 episode reward: -87.3000,                 loss: nan
env1_second_0:                 episode reward: 87.3000,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 374.1,                last time consumption/overall running time: 427.7348s / 300848.9838 s
env0_first_0:                 episode reward: -84.6500,                 loss: 0.3657
env0_second_0:                 episode reward: 84.6500,                 loss: 0.4374
env1_first_0:                 episode reward: -92.8500,                 loss: nan
env1_second_0:                 episode reward: 92.8500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 551.75,                last time consumption/overall running time: 625.0993s / 301474.0831 s
env0_first_0:                 episode reward: -89.0000,                 loss: 0.4107
env0_second_0:                 episode reward: 89.0000,                 loss: 0.4163
env1_first_0:                 episode reward: -89.9500,                 loss: nan
env1_second_0:                 episode reward: 89.9500,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 544.1,                last time consumption/overall running time: 618.9166s / 302092.9997 s
env0_first_0:                 episode reward: -84.7000,                 loss: 0.3725
env0_second_0:                 episode reward: 84.7000,                 loss: 0.3964
env1_first_0:                 episode reward: -89.4000,                 loss: nan
env1_second_0:                 episode reward: 89.4000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 346.65,                last time consumption/overall running time: 396.2624s / 302489.2621 s
env0_first_0:                 episode reward: -80.1500,                 loss: 0.3034
env0_second_0:                 episode reward: 80.1500,                 loss: 0.3864
env1_first_0:                 episode reward: -93.7000,                 loss: nan
env1_second_0:                 episode reward: 93.7000,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 300.75,                last time consumption/overall running time: 346.0681s / 302835.3302 s
env0_first_0:                 episode reward: -92.0000,                 loss: 0.2955
env0_second_0:                 episode reward: 92.0000,                 loss: 0.3769
env1_first_0:                 episode reward: -90.8000,                 loss: nan
env1_second_0:                 episode reward: 90.8000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 292.75,                last time consumption/overall running time: 329.3690s / 303164.6992 s
env0_first_0:                 episode reward: -84.8000,                 loss: 0.3167
env0_second_0:                 episode reward: 84.8000,                 loss: 0.3645
env1_first_0:                 episode reward: -91.1000,                 loss: nan
env1_second_0:                 episode reward: 91.1000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 253.75,                last time consumption/overall running time: 291.1968s / 303455.8960 s
env0_first_0:                 episode reward: -83.9500,                 loss: 0.3105
env0_second_0:                 episode reward: 83.9500,                 loss: 0.3829
env1_first_0:                 episode reward: -80.8000,                 loss: nan
env1_second_0:                 episode reward: 80.8000,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 357.85,                last time consumption/overall running time: 406.0500s / 303861.9460 sLoad boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rl/agents/nash_dqn_exploiter.py:174: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  action = torch.LongTensor(action).to(self.device)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_first_0:                 episode reward: -81.8500,                 loss: 0.3358
env0_second_0:                 episode reward: 81.8500,                 loss: 0.3584
env1_first_0:                 episode reward: -91.7500,                 loss: nan
env1_second_0:                 episode reward: 91.7500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 241.4,                last time consumption/overall running time: 278.9657s / 304140.9117 s
env0_first_0:                 episode reward: -89.3500,                 loss: 0.3216
env0_second_0:                 episode reward: 89.3500,                 loss: 0.3787
env1_first_0:                 episode reward: -84.3000,                 loss: nan
env1_second_0:                 episode reward: 84.3000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 247.95,                last time consumption/overall running time: 280.3896s / 304421.3013 s
env0_first_0:                 episode reward: -97.8500,                 loss: 0.3412
env0_second_0:                 episode reward: 97.8500,                 loss: 0.3426
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 272.65,                last time consumption/overall running time: 308.2727s / 304729.5740 s
env0_first_0:                 episode reward: -93.6000,                 loss: 0.3783
env0_second_0:                 episode reward: 93.6000,                 loss: 0.3623
env1_first_0:                 episode reward: -89.5000,                 loss: nan
env1_second_0:                 episode reward: 89.5000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 240.9,                last time consumption/overall running time: 277.9133s / 305007.4873 s
env0_first_0:                 episode reward: -97.7500,                 loss: 0.3156
env0_second_0:                 episode reward: 97.7500,                 loss: 0.3884
env1_first_0:                 episode reward: -91.9500,                 loss: nan
env1_second_0:                 episode reward: 91.9500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 268.75,                last time consumption/overall running time: 304.6716s / 305312.1589 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.3271
env0_second_0:                 episode reward: 91.6500,                 loss: 0.3594
env1_first_0:                 episode reward: -95.2000,                 loss: nan
env1_second_0:                 episode reward: 95.2000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 302.25,                last time consumption/overall running time: 342.8299s / 305654.9888 s
env0_first_0:                 episode reward: -94.0000,                 loss: 0.3107
env0_second_0:                 episode reward: 94.0000,                 loss: 0.3315
env1_first_0:                 episode reward: -89.3500,                 loss: nan
env1_second_0:                 episode reward: 89.3500,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 288.55,                last time consumption/overall running time: 329.6130s / 305984.6018 s
env0_first_0:                 episode reward: -86.3000,                 loss: 0.3097
env0_second_0:                 episode reward: 86.3000,                 loss: 0.3271
env1_first_0:                 episode reward: -90.4000,                 loss: nan
env1_second_0:                 episode reward: 90.4000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 320.0,                last time consumption/overall running time: 365.6439s / 306350.2458 s
env0_first_0:                 episode reward: -89.1000,                 loss: 0.3129
env0_second_0:                 episode reward: 89.1000,                 loss: 0.3330
env1_first_0:                 episode reward: -96.1500,                 loss: nan
env1_second_0:                 episode reward: 96.1500,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 401.9,                last time consumption/overall running time: 454.9707s / 306805.2164 s
env0_first_0:                 episode reward: -86.6500,                 loss: 0.3227
env0_second_0:                 episode reward: 86.6500,                 loss: 0.3173
env1_first_0:                 episode reward: -81.9500,                 loss: nan
env1_second_0:                 episode reward: 81.9500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 437.4,                last time consumption/overall running time: 492.8301s / 307298.0466 s
env0_first_0:                 episode reward: -89.8500,                 loss: 0.3061
env0_second_0:                 episode reward: 89.8500,                 loss: 0.3411
env1_first_0:                 episode reward: -88.2000,                 loss: nan
env1_second_0:                 episode reward: 88.2000,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 275.25,                last time consumption/overall running time: 307.5617s / 307605.6083 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.3176
env0_second_0:                 episode reward: 87.7000,                 loss: 0.3386
env1_first_0:                 episode reward: -88.4500,                 loss: nan
env1_second_0:                 episode reward: 88.4500,                 loss: nan
